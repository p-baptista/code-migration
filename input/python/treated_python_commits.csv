repo,commit,add_lib,rmv_lib,before,after,type,before_flat,after_flat,file
18F/cg-compliance,ed026433089496e8bb6480f4c340451c00e52b28,requests,urllib,"from behave import given, then
from urllib import request

import datetime


@given('the github link - {url}')
def step_impl(context, url):
    context.url = url


@then('the policy has been updated within the last {days} days')
def step_impl(context, days):
    res = request.urlopen(context.url)
    last_modified = datetime.datetime.strptime(
        res.getheader(name='Last-Modified'), ""%a, %d %b %Y %H:%M:%S %Z"")
    x_days_ago = datetime.datetime.now() - datetime.timedelta(days=int(days))
    assert last_modified > x_days_ago","from behave import given, then
import requests

import datetime


@given('the github link - {url}')
def step_impl(context, url):
    context.url = url


@then('the policy has been updated within the last {days} days')
def step_impl(context, days):
    res = requests.get(context.url)
    last_modified = datetime.datetime.strptime(
        res.headers.get('Last-Modified'), ""%a, %d %b %Y %H:%M:%S %Z"")
    x_days_ago = datetime.datetime.now() - datetime.timedelta(days=int(days))
    assert last_modified > x_days_ago",update,"frombehaveimportgiven,thenfromurllibimportrequestimportdatetime@given('thegithublink-{url}')defstep_impl(context,url):context.url=url@then('thepolicyhasbeenupdatedwithinthelast{days}days')defstep_impl(context,days):res=request.urlopen(context.url)last_modified=datetime.datetime.strptime(res.getheader(name='Last-Modified'),""%a,%d%b%Y%H:%M:%S%Z"")x_days_ago=datetime.datetime.now()-datetime.timedelta(days=int(days))assertlast_modified>x_days_ago","frombehaveimportgiven,thenimportrequestsimportdatetime@given('thegithublink-{url}')defstep_impl(context,url):context.url=url@then('thepolicyhasbeenupdatedwithinthelast{days}days')defstep_impl(context,days):res=requests.get(context.url)last_modified=datetime.datetime.strptime(res.headers.get('Last-Modified'),""%a,%d%b%Y%H:%M:%S%Z"")x_days_ago=datetime.datetime.now()-datetime.timedelta(days=int(days))assertlast_modified>x_days_ago",BDD/steps/policies.py
5agado/conversation-analyzer,8989fba9ea32a8b7afc9a28d7814d876db05ebd9,requests,urllib,"
        req = urllib.request.Request(url, requestData, headers)
        with urllib.request.urlopen(req) as response:
            with gzip.GzipFile(fileobj=response) as uncompressed:
                decompressedFile = uncompressed.read()
        end = time.time()
        logging.info(""Retrieved in {0:.2f}s"".format(end-start))

        #Remove additional leading characters
        msgsData = decompressedFile.decode(""utf-8"")[9:]
        return  msgsData","        response = requests.post(url, data=requestData, headers=headers)



        end = time.time()
        logging.info(""Retrieved in {0:.2f}s"".format(end-start))

        #Remove additional leading characters
        msgsData = response.text[9:]
        return  msgsData",update,"req=urllib.request.Request(url,requestData,headers)withurllib.request.urlopen(req)asresponse:withgzip.GzipFile(fileobj=response)asuncompressed:decompressedFile=uncompressed.read()end=time.time()logging.info(""Retrievedin{0:.2f}s"".format(end-start))#RemoveadditionalleadingcharactersmsgsData=decompressedFile.decode(""utf-8"")[9:]returnmsgsData","response=requests.post(url,data=requestData,headers=headers)end=time.time()logging.info(""Retrievedin{0:.2f}s"".format(end-start))#RemoveadditionalleadingcharactersmsgsData=response.text[9:]returnmsgsData",src/util/conversationScraper.py
6uhrmittag/taskbutler,853f9b986c73e8e8eb48e79e675f58c21b2c0ca4,requests,urllib,"request = urllib.request.Request(config.update_url)
try:
    urllib.request.urlopen(request)

    update_releaseinforaw = urllib.request.urlopen(config.update_url).read()
    json = json.loads(update_releaseinforaw.decode('utf-8'))

    if not config.version == json[0]['tag_name']:
        print(""\n#########\n"")
        print(""Your version is not up-to-date!"")
        print(""Your version  :"", config.version)
        print(""Latest version: "", json[0]['tag_name'])
        print(""See latest version at: "", json[0]['html_url'])
        print(""\n#########"")

except urllib.error.URLError as e:
    print(""Error while checking for updates: "", e.reason)
except urllib.error.HTTPError as e:
    print(""Error while checking for updates: "", e.code)","try:
    r = requests.get(config.update_url)
    r.raise_for_status()
    release_info_json = r.json()


    if not config.version == release_info_json[0]['tag_name']:
        print(""\n#########\n"")
        print(""Your version is not up-to-date!"")
        print(""Your version  :"", config.version)
        print(""Latest version: "", release_info_json[0]['tag_name'])
        print(""See latest version at: "", release_info_json[0]['html_url'])
        print(""\n#########"")

except requests.exceptions.ConnectionError as e:
    print(""Error while checking for updates (Connection error): "", e)
except requests.exceptions.HTTPError as e:
    print(""Error while checking for updates (HTTP error): "", e)
except requests.exceptions.RequestException  as e:
    print(""Error while checking for updates: "", e)",update,"request=urllib.request.Request(config.update_url)try:urllib.request.urlopen(request)update_releaseinforaw=urllib.request.urlopen(config.update_url).read()json=json.loads(update_releaseinforaw.decode('utf-8'))ifnotconfig.version==json[0]['tag_name']:print(""\n#########\n"")print(""Yourversionisnotup-to-date!"")print(""Yourversion:"",config.version)print(""Latestversion:"",json[0]['tag_name'])print(""Seelatestversionat:"",json[0]['html_url'])print(""\n#########"")excepturllib.error.URLErrorase:print(""Errorwhilecheckingforupdates:"",e.reason)excepturllib.error.HTTPErrorase:print(""Errorwhilecheckingforupdates:"",e.code)","try:r=requests.get(config.update_url)r.raise_for_status()release_info_json=r.json()ifnotconfig.version==release_info_json[0]['tag_name']:print(""\n#########\n"")print(""Yourversionisnotup-to-date!"")print(""Yourversion:"",config.version)print(""Latestversion:"",release_info_json[0]['tag_name'])print(""Seelatestversionat:"",release_info_json[0]['html_url'])print(""\n#########"")exceptrequests.exceptions.ConnectionErrorase:print(""Errorwhilecheckingforupdates(Connectionerror):"",e)exceptrequests.exceptions.HTTPErrorase:print(""Errorwhilecheckingforupdates(HTTPerror):"",e)exceptrequests.exceptions.RequestExceptionase:print(""Errorwhilecheckingforupdates:"",e)",todoist-progressbar.py
AlienVault-Labs/OTX-Python-SDK,89cb7875c8ae0aff22ffe6b2b354ca4db64235e8,requests,urllib,"def __init__(self, api_key, proxy=None, server=""https://otx.alienvault.com"", project=""SDK""):
        self.key = api_key
        self.server = server
        self.proxy = proxy
        self.sdk = 'OTX Python {}/1.1'.format(project)","def __init__(self, api_key, proxy=None, server=""https://otx.alienvault.com"", project=""SDK"", user_agent=None):
        self.key = api_key
        self.server = server
        self.proxies = {'http': proxy} if proxy else {}
        self.request_session = None
        self.headers = {
            'X-OTX-API-KEY': self.key,
            'User-Agent': user_agent or 'OTX Python {}/1.1'.format(project),
            'Content-Type': 'application/json'
        }

    def session(self):
        if self.request_session is None:
            self.request_session = requests.Session()

        return self.request_session

    @classmethod
    def handle_response_errors(cls, response):
        if response.status_code == 403:
            raise InvalidAPIKey()
        elif response.status_code == 400:
            raise BadRequest(response.json())
        elif str(response.status_code)[0] != ""2"":
            raise Exception(""Unexpected http code: %r"", response.code)

        return response",update,"def__init__(self,api_key,proxy=None,server=""https://otx.alienvault.com"",project=""SDK""):self.key=api_keyself.server=serverself.proxy=proxyself.sdk='OTXPython{}/1.1'.format(project)","def__init__(self,api_key,proxy=None,server=""https://otx.alienvault.com"",project=""SDK"",user_agent=None):self.key=api_keyself.server=serverself.proxies={'http':proxy}ifproxyelse{}self.request_session=Noneself.headers={'X-OTX-API-KEY':self.key,'User-Agent':user_agentor'OTXPython{}/1.1'.format(project),'Content-Type':'application/json'}defsession(self):ifself.request_sessionisNone:self.request_session=requests.Session()returnself.request_session@classmethoddefhandle_response_errors(cls,response):ifresponse.status_code==403:raiseInvalidAPIKey()elifresponse.status_code==400:raiseBadRequest(response.json())elifstr(response.status_code)[0]!=""2"":raiseException(""Unexpectedhttpcode:%r"",response.code)returnresponse",OTXv2.py
AlienVault-Labs/OTX-Python-SDK,89cb7875c8ae0aff22ffe6b2b354ca4db64235e8,requests,urllib,"def get(self, url):
        """"""
        Internal API for GET request on a OTX URL
        :param url: URL to retrieve
        :return: response in JSON object form
        """"""
        if self.proxy:
            proxy = ProxyHandler({'http': self.proxy})
            request = build_opener(proxy)
        else:
            request = build_opener()
        request.addheaders = [
            ('X-OTX-API-KEY', self.key),
            ('User-Agent', self.sdk)
        ]
        response = None
        try:
            response = request.open(url)
        except URLError as e:
            if isinstance(e, HTTPError):
                if e.code == 403:
                    raise InvalidAPIKey(""Invalid API Key"")
                elif e.code == 400:
                    raise BadRequest(""Bad Request"")
            else:
                raise e
        data = response.read().decode('utf-8')
        json_data = json.loads(data)
        return json_data","    def get(self, url):
        """"""
        Internal API for GET request on a OTX URL
        :param url: URL to retrieve
        :return: response in JSON object form
        """"""

        response = self.session().get(url, headers=self.headers, proxies=self.proxies)
        return self.handle_response_errors(response).json()


















",update,"defget(self,url):""""""InternalAPIforGETrequestonaOTXURL:paramurl:URLtoretrieve:return:responseinJSONobjectform""""""ifself.proxy:proxy=ProxyHandler({'http':self.proxy})request=build_opener(proxy)else:request=build_opener()request.addheaders=[('X-OTX-API-KEY',self.key),('User-Agent',self.sdk)]response=Nonetry:response=request.open(url)exceptURLErrorase:ifisinstance(e,HTTPError):ife.code==403:raiseInvalidAPIKey(""InvalidAPIKey"")elife.code==400:raiseBadRequest(""BadRequest"")else:raiseedata=response.read().decode('utf-8')json_data=json.loads(data)returnjson_data","defget(self,url):""""""InternalAPIforGETrequestonaOTXURL:paramurl:URLtoretrieve:return:responseinJSONobjectform""""""response=self.session().get(url,headers=self.headers,proxies=self.proxies)returnself.handle_response_errors(response).json()",OTXv2.py
AlienVault-Labs/OTX-Python-SDK,89cb7875c8ae0aff22ffe6b2b354ca4db64235e8,requests,urllib," def patch(self, url, body):
        """"""
        Internal API for POST request on a OTX URL
        :param url: URL to retrieve
        :param body: HTTP Body to send in request
        :return: response as dict
        """"""
        request = Request(url)
        request.add_header('X-OTX-API-KEY', self.key)
        request.add_header('User-Agent', self.sdk)
        request.add_header(""Content-Type"", ""application/json"")
        method = ""PATCH""
        request.get_method = lambda: method
        if body:
            try:  # python2
                request.add_data(json.dumps(body))
            except AttributeError as ae:  # python3
                request.data = json.dumps(body).encode('utf-8')
        try:
            response = urlopen(request)
            data = response.read().decode('utf-8')
            json_data = json.loads(data)
            return json_data
        except URLError as e:
            if isinstance(e, HTTPError):
                if e.code == 403:
                    raise InvalidAPIKey(""Invalid API Key"")
                elif e.code == 400:
                    encoded_error = e.read()
                    decoded_error = encoded_error.decode('utf-8')
                    json.loads(decoded_error)
                    raise BadRequest(decoded_error)
        return {}

    def post(self, url, body):
        """"""","    def patch(self, url, body):
        """"""
        Internal API for POST request on a OTX URL
        :param url: URL to retrieve
        :param body: HTTP Body to send in request
        :return: response as dict
        """"""

        response = self.session().patch(url, data=json.dumps(body), headers=self.headers, proxies=self.proxies)
        return self.handle_response_errors(response).json()
























    def post(self, url, body):",update,"defpatch(self,url,body):""""""InternalAPIforPOSTrequestonaOTXURL:paramurl:URLtoretrieve:parambody:HTTPBodytosendinrequest:return:responseasdict""""""request=Request(url)request.add_header('X-OTX-API-KEY',self.key)request.add_header('User-Agent',self.sdk)request.add_header(""Content-Type"",""application/json"")method=""PATCH""request.get_method=lambda:methodifbody:try:#python2request.add_data(json.dumps(body))exceptAttributeErrorasae:#python3request.data=json.dumps(body).encode('utf-8')try:response=urlopen(request)data=response.read().decode('utf-8')json_data=json.loads(data)returnjson_dataexceptURLErrorase:ifisinstance(e,HTTPError):ife.code==403:raiseInvalidAPIKey(""InvalidAPIKey"")elife.code==400:encoded_error=e.read()decoded_error=encoded_error.decode('utf-8')json.loads(decoded_error)raiseBadRequest(decoded_error)return{}defpost(self,url,body):""""""","defpatch(self,url,body):""""""InternalAPIforPOSTrequestonaOTXURL:paramurl:URLtoretrieve:parambody:HTTPBodytosendinrequest:return:responseasdict""""""response=self.session().patch(url,data=json.dumps(body),headers=self.headers,proxies=self.proxies)returnself.handle_response_errors(response).json()defpost(self,url,body):",OTXv2.py
AlienVault-Labs/OTX-Python-SDK,89cb7875c8ae0aff22ffe6b2b354ca4db64235e8,requests,urllib,"    def post(self, url, body):
        """"""
        Internal API for POST request on a OTX URL
        :param url: URL to retrieve
        :param body: HTTP Body to send in request
        :return: response as dict
        """"""
        request = Request(url)
        request.add_header('X-OTX-API-KEY', self.key)
        request.add_header('User-Agent', self.sdk)
        request.add_header(""Content-Type"", ""application/json"")
        method = ""POST""
        request.get_method = lambda: method
        if body:
            try:  # python2
                request.add_data(json.dumps(body))
            except AttributeError as ae:  # python3
                request.data = json.dumps(body).encode('utf-8')
        try:
            response = urlopen(request)
            data = response.read().decode('utf-8')
            json_data = json.loads(data)
            return json_data
        except URLError as e:
            if isinstance(e, HTTPError):
                if e.code == 403:
                    raise InvalidAPIKey(""Invalid API Key"")
                elif e.code == 400:
                    encoded_error = e.read()
                    decoded_error = encoded_error.decode('utf-8')
                    json.loads(decoded_error)
                    raise BadRequest(decoded_error)
        return {}","    def post(self, url, body):
        """"""
        Internal API for POST request on a OTX URL
        :param url: URL to retrieve
        :param body: HTTP Body to send in request
        :return: response as dict
        """"""

        response = self.session().post(url, data=json.dumps(body), headers=self.headers, proxies=self.proxies)
        return self.handle_response_errors(response).json()





















",update,"defpost(self,url,body):""""""InternalAPIforPOSTrequestonaOTXURL:paramurl:URLtoretrieve:parambody:HTTPBodytosendinrequest:return:responseasdict""""""request=Request(url)request.add_header('X-OTX-API-KEY',self.key)request.add_header('User-Agent',self.sdk)request.add_header(""Content-Type"",""application/json"")method=""POST""request.get_method=lambda:methodifbody:try:#python2request.add_data(json.dumps(body))exceptAttributeErrorasae:#python3request.data=json.dumps(body).encode('utf-8')try:response=urlopen(request)data=response.read().decode('utf-8')json_data=json.loads(data)returnjson_dataexceptURLErrorase:ifisinstance(e,HTTPError):ife.code==403:raiseInvalidAPIKey(""InvalidAPIKey"")elife.code==400:encoded_error=e.read()decoded_error=encoded_error.decode('utf-8')json.loads(decoded_error)raiseBadRequest(decoded_error)return{}","defpost(self,url,body):""""""InternalAPIforPOSTrequestonaOTXURL:paramurl:URLtoretrieve:parambody:HTTPBodytosendinrequest:return:responseasdict""""""response=self.session().post(url,data=json.dumps(body),headers=self.headers,proxies=self.proxies)returnself.handle_response_errors(response).json()",OTXv2.py
AtomicConductor/conductor_client,a972f6615a60c5dd8285c679071504a112ece551,requests,urllib,"    def authorize_urllib(self):
        '''
        This is crazy magic that's apparently ok
        '''
        token = self.get_token()
        password_manager = urllib2.HTTPPasswordMgrWithDefaultRealm()
        password_manager.add_password(None, CONFIG['url'], token.split(':')[0], 'unused')
        auth = urllib2.HTTPBasicAuthHandler(password_manager)
        opener = urllib2.build_opener(auth)
        urllib2.install_opener(opener)",,delete,"defauthorize_urllib(self):'''Thisiscrazymagicthat'sapparentlyok'''token=self.get_token()password_manager=urllib2.HTTPPasswordMgrWithDefaultRealm()password_manager.add_password(None,CONFIG['url'],token.split(':')[0],'unused')auth=urllib2.HTTPBasicAuthHandler(password_manager)opener=urllib2.build_opener(auth)urllib2.install_opener(opener)",,
AtomicConductor/conductor_client,a972f6615a60c5dd8285c679071504a112ece551,requests,urllib,"def make_request(self, uri_path=""/"", headers=None, params=None, data=None, verb=None):
        '''
        verb: PUT, POST, GET, DELETE, HEAD
        '''

        # TODO: set Content Type to json if data arg
        if not headers:
            headers = {'Content-Type':'application/json'}
        logger.debug('headers are: %s', headers)


        # Construct URL
        conductor_url = urlparse.urljoin(CONFIG['url'], uri_path)
        logger.debug('conductor_url: %s', conductor_url)
        if params:
            conductor_url += '?'
            conductor_url += urllib.urlencode(params)
        logger.debug('conductor_url is %s', conductor_url)

        req = urllib2.Request(conductor_url, headers=headers, data=data)
        if verb:
            req.get_method = lambda: verb
        logger.debug('request is %s', req)

        logger.debug('trying to connect to app')
        handler = common.retry(lambda: urllib2.urlopen(req))
        response_string = handler.read()
        response_code = handler.getcode()
        logger.debug('response_code: %s', response_code)
        logger.debug('response_string is: %s', response_string)
        return response_string, response_code","    def make_request(self, uri_path=""/"", headers=None, params=None, data=None, verb=None):
        '''
        verb: PUT, POST, GET, DELETE, HEAD
        '''

        # TODO: set Content Type to json if data arg
        if not headers:
            headers = {'Content-Type':'application/json'}
        logger.debug('headers are: %s', headers)


        # Construct URL
        conductor_url = urlparse.urljoin(CONFIG['url'], uri_path)
        logger.debug('conductor_url: %s', conductor_url)

        if not verb:
            if data:
                verb = 'POST'
            else:
                verb = 'GET'

        auth = CONFIG['conductor_token']

        response = common.retry(
            lambda:
            getattr(requests, verb.lower())(
                conductor_url,
                auth=HTTPBasicAuth(CONFIG['conductor_token'], 'unused'),
                headers=headers,
                params=params,
                data=data)
        )


        logger.debug('response.status_code: %s', response.status_code)
        logger.debug('response.text is: %s', response.text)
        return response.text, response.status_code",update,"defmake_request(self,uri_path=""/"",headers=None,params=None,data=None,verb=None):'''verb:PUT,POST,GET,DELETE,HEAD'''#TODO:setContentTypetojsonifdataargifnotheaders:headers={'Content-Type':'application/json'}logger.debug('headersare:%s',headers)#ConstructURLconductor_url=urlparse.urljoin(CONFIG['url'],uri_path)logger.debug('conductor_url:%s',conductor_url)ifparams:conductor_url+='?'conductor_url+=urllib.urlencode(params)logger.debug('conductor_urlis%s',conductor_url)req=urllib2.Request(conductor_url,headers=headers,data=data)ifverb:req.get_method=lambda:verblogger.debug('requestis%s',req)logger.debug('tryingtoconnecttoapp')handler=common.retry(lambda:urllib2.urlopen(req))response_string=handler.read()response_code=handler.getcode()logger.debug('response_code:%s',response_code)logger.debug('response_stringis:%s',response_string)returnresponse_string,response_code","defmake_request(self,uri_path=""/"",headers=None,params=None,data=None,verb=None):'''verb:PUT,POST,GET,DELETE,HEAD'''#TODO:setContentTypetojsonifdataargifnotheaders:headers={'Content-Type':'application/json'}logger.debug('headersare:%s',headers)#ConstructURLconductor_url=urlparse.urljoin(CONFIG['url'],uri_path)logger.debug('conductor_url:%s',conductor_url)ifnotverb:ifdata:verb='POST'else:verb='GET'auth=CONFIG['conductor_token']response=common.retry(lambda:getattr(requests,verb.lower())(conductor_url,auth=HTTPBasicAuth(CONFIG['conductor_token'],'unused'),headers=headers,params=params,data=data))logger.debug('response.status_code:%s',response.status_code)logger.debug('response.textis:%s',response.text)returnresponse.text,response.status_code",conductor/lib/api_client.py
Azelphur/pyPushBullet,3f9928ddbdc8fc6d3f656ea4d74b45ac369ed502,requests,urllib," def _request(self, url, postdata=None):
        request = Request(url)
        request.add_header(""Accept"", ""application/json"")
        request.add_header(""Content-type"",""application/json"");
        auth = ""%s:"" % (self.apiKey)
        auth = auth.encode('ascii')
        auth = b64encode(auth)
        auth = b""Basic ""+auth
        request.add_header(""Authorization"", auth)
        request.add_header(""User-Agent"", ""pyPushBullet"")
        if postdata:
            postdata = json.dumps(postdata)
            postdata = postdata.encode('utf-8')
        response = urlopen(request, postdata)
        data = response.read()
        data = data.decode(""utf-8"")
        j = json.loads(data)
        return j","    def _request(self, method, url, postdata=None, params=None, files=None):
        headers = {""Accept"": ""application/json"",
                   ""Content-Type"": ""application/json"",
                   ""User-Agent"": ""pyPushBullet""}






        if postdata:
            postdata = json.dumps(postdata)

        r = requests.request(method,
                             url,
                             data=postdata,
                             params=params,
                             headers=headers,
                             files=files,
                             auth=HTTPBasicAuth(self.apiKey, """"))

        r.raise_for_status()
        return r.json()",update,"def_request(self,url,postdata=None):request=Request(url)request.add_header(""Accept"",""application/json"")request.add_header(""Content-type"",""application/json"");auth=""%s:""%(self.apiKey)auth=auth.encode('ascii')auth=b64encode(auth)auth=b""Basic""+authrequest.add_header(""Authorization"",auth)request.add_header(""User-Agent"",""pyPushBullet"")ifpostdata:postdata=json.dumps(postdata)postdata=postdata.encode('utf-8')response=urlopen(request,postdata)data=response.read()data=data.decode(""utf-8"")j=json.loads(data)returnj","def_request(self,method,url,postdata=None,params=None,files=None):headers={""Accept"":""application/json"",""Content-Type"":""application/json"",""User-Agent"":""pyPushBullet""}ifpostdata:postdata=json.dumps(postdata)r=requests.request(method,url,data=postdata,params=params,headers=headers,files=files,auth=HTTPBasicAuth(self.apiKey,""""))r.raise_for_status()returnr.json()",pushbullet.py
Azelphur/pyPushBullet,3f9928ddbdc8fc6d3f656ea4d74b45ac369ed502,requests,urllib,"def _request_multiform(self, url, postdata, files):
        request = Request(url)
        content_type, body = self._encode_multipart_formdata(postdata, files)
        request.add_header(""Accept"", ""application/json"")
        request.add_header(""Content-type"", content_type);
        auth = ""%s:"" % (self.apiKey)
        auth = auth.encode('ascii')
        auth = b64encode(auth)
        auth = b""Basic ""+auth
        request.add_header(""Authorization"", auth)
        request.add_header(""User-Agent"", ""pyPushBullet"")
        response = urlopen(request, body)
        data = response.read()
        data = data.decode(""utf-8"")
        j = json.loads(data)
        return j",,delete,"def_request_multiform(self,url,postdata,files):request=Request(url)content_type,body=self._encode_multipart_formdata(postdata,files)request.add_header(""Accept"",""application/json"")request.add_header(""Content-type"",content_type);auth=""%s:""%(self.apiKey)auth=auth.encode('ascii')auth=b64encode(auth)auth=b""Basic""+authrequest.add_header(""Authorization"",auth)request.add_header(""User-Agent"",""pyPushBullet"")response=urlopen(request,body)data=response.read()data=data.decode(""utf-8"")j=json.loads(data)returnj",,
Azelphur/pyPushBullet,3f9928ddbdc8fc6d3f656ea4d74b45ac369ed502,requests,urllib," def _encode_multipart_formdata(self, fields, files):
        '''
        from http://mattshaw.org/news/multi-part-form-post-with-files-in-python/
        '''
        def guess_type(filename):
            return mimetypes.guess_type(filename)[0] or 'application/octet-stream'
        
        BOUNDARY = '----------bound@ry_$'
        CRLF = '\r\n'
        L = []
        for key,value in fields.iteritems():
            L.append('--'+BOUNDARY)
            L.append('Content-Disposition: form-data; name=""%s""'%(key))
            L.append('')
            L.append(str(value))
            
        for (key, filename, value) in files:
            L.append('--'+BOUNDARY)
            L.append('Content-Disposition: form-data; name=""%s""; filename=""%s""'%(key, filename))
            L.append('Content-Type: %s'%(guess_type(filename)))
            L.append('')
            L.append(value)
            
        L.append('--'+BOUNDARY+'--')
        L.append('')
        body = CRLF.join(L)
        content_type = 'multipart/form-data; boundary=%s' % BOUNDARY
        return content_type, body",,delete,"def_encode_multipart_formdata(self,fields,files):'''fromhttp://mattshaw.org/news/multi-part-form-post-with-files-in-python/'''defguess_type(filename):returnmimetypes.guess_type(filename)[0]or'application/octet-stream'BOUNDARY='----------bound@ry_$'CRLF='\r\n'L=[]forkey,valueinfields.iteritems():L.append('--'+BOUNDARY)L.append('Content-Disposition:form-data;name=""%s""'%(key))L.append('')L.append(str(value))for(key,filename,value)infiles:L.append('--'+BOUNDARY)L.append('Content-Disposition:form-data;name=""%s"";filename=""%s""'%(key,filename))L.append('Content-Type:%s'%(guess_type(filename)))L.append('')L.append(value)L.append('--'+BOUNDARY+'--')L.append('')body=CRLF.join(L)content_type='multipart/form-data;boundary=%s'%BOUNDARYreturncontent_type,body",,
Azelphur/pyPushBullet,3f9928ddbdc8fc6d3f656ea4d74b45ac369ed502,requests,urllib,"def getDevices(self):
        return self._request(HOST + ""/devices"")[""devices""]"," def getDevices(self):
        """""" Get devices
            https://docs.pushbullet.com/v2/devices

            Get a list of devices, and data about them.
        """"""

        return self._request(""GET"", HOST + ""/devices"")[""devices""]",update,"defgetDevices(self):returnself._request(HOST+""/devices"")[""devices""]","defgetDevices(self):""""""Getdeviceshttps://docs.pushbullet.com/v2/devicesGetalistofdevices,anddataaboutthem.""""""returnself._request(""GET"",HOST+""/devices"")[""devices""]",pushbullet.py
Azelphur/pyPushBullet,3f9928ddbdc8fc6d3f656ea4d74b45ac369ed502,requests,urllib,"    def pushNote(self, device, title, body):
        data = {'type'      : 'note',
                'device_id' : device,
                'title'     : title,
                'body'      : body}
        return self._request(HOST + ""/pushes"", data)","    def pushNote(self, device_iden, title, body):
        """""" Push a note
            https://docs.pushbullet.com/v2/pushes

            Arguments:
            device_iden -- iden of device to push to
            title -- a title for the note
            body -- the body of the note
        """"""

        data = {""type"": ""note"",
                ""device_iden"": device_iden,
                ""title"": title,
                ""body"": body}
        return self._request(""POST"", HOST + ""/pushes"", data)",update,"defpushNote(self,device,title,body):data={'type':'note','device_id':device,'title':title,'body':body}returnself._request(HOST+""/pushes"",data)","defpushNote(self,device_iden,title,body):""""""Pushanotehttps://docs.pushbullet.com/v2/pushesArguments:device_iden--idenofdevicetopushtotitle--atitleforthenotebody--thebodyofthenote""""""data={""type"":""note"",""device_iden"":device_iden,""title"":title,""body"":body}returnself._request(""POST"",HOST+""/pushes"",data)",pushbullet.py
Azelphur/pyPushBullet,3f9928ddbdc8fc6d3f656ea4d74b45ac369ed502,requests,urllib,"    def pushAddress(self, device, name, address):
        data = {'type'      : 'address',
                'device_id' : device,
                'name'      : name,
                'address'   : address}
        return self._request(HOST + ""/pushes"", data)","def pushAddress(self, device_iden, name, address):
        """""" Push an address
            https://docs.pushbullet.com/v2/pushes

            Arguments:
            device_iden -- iden of device to push to
            name -- name for the address, eg ""Bobs house""
            address -- address of the address
        """"""

        data = {""type"": ""address"",
                ""device_iden"": device_iden,
                ""name"": name,
                ""address"": address}
        return self._request(""POST"", HOST + ""/pushes"", data)",update,"defpushAddress(self,device,name,address):data={'type':'address','device_id':device,'name':name,'address':address}returnself._request(HOST+""/pushes"",data)","defpushAddress(self,device_iden,name,address):""""""Pushanaddresshttps://docs.pushbullet.com/v2/pushesArguments:device_iden--idenofdevicetopushtoname--namefortheaddress,eg""Bobshouse""address--addressoftheaddress""""""data={""type"":""address"",""device_iden"":device_iden,""name"":name,""address"":address}returnself._request(""POST"",HOST+""/pushes"",data)",pushbullet.py
Azelphur/pyPushBullet,3f9928ddbdc8fc6d3f656ea4d74b45ac369ed502,requests,urllib,"    def pushList(self, device, title, items):
        data = {'type'      : 'list',
                'device_id' : device,
                'title'     : title,
                'items'     : items}
        return self._request(HOST + ""/pushes"", data)","    def pushList(self, device_iden, title, items):
        """""" Push a list
            https://docs.pushbullet.com/v2/pushes

            Arguments:
            device_iden -- iden of device to push to
            title -- a title for the list
            items -- a list of items
        """"""

        data = {""type"": ""list"",
                ""device_iden"": device_iden,
                ""title"": title,
                ""items"": items}

        return self._request(""POST"", HOST + ""/pushes"", data)",update,"defpushList(self,device,title,items):data={'type':'list','device_id':device,'title':title,'items':items}returnself._request(HOST+""/pushes"",data)","defpushList(self,device_iden,title,items):""""""Pushalisthttps://docs.pushbullet.com/v2/pushesArguments:device_iden--idenofdevicetopushtotitle--atitleforthelistitems--alistofitems""""""data={""type"":""list"",""device_iden"":device_iden,""title"":title,""items"":items}returnself._request(""POST"",HOST+""/pushes"",data)",pushbullet.py
Azelphur/pyPushBullet,3f9928ddbdc8fc6d3f656ea4d74b45ac369ed502,requests,urllib,"    def pushLink(self, device, title, url):
        data = {'type'      : 'link',
                'device_id' : device,
                'title'     : title,
                'url'     : url}
        return self._request(HOST + ""/pushes"", data)","def pushLink(self, device_iden, title, url):
        """""" Push a link
            https://docs.pushbullet.com/v2/pushes

            Arguments:
            device_iden -- iden of device to push to
            title -- link title
            url -- link url
        """"""

        data = {""type"": ""link"",
                ""device_iden"": device_iden,
                ""title"": title,
                ""url"": url}
        return self._request(""POST"", HOST + ""/pushes"", data)",update,"defpushLink(self,device,title,url):data={'type':'link','device_id':device,'title':title,'url':url}returnself._request(HOST+""/pushes"",data)","defpushLink(self,device_iden,title,url):""""""Pushalinkhttps://docs.pushbullet.com/v2/pushesArguments:device_iden--idenofdevicetopushtotitle--linktitleurl--linkurl""""""data={""type"":""link"",""device_iden"":device_iden,""title"":title,""url"":url}returnself._request(""POST"",HOST+""/pushes"",data)",pushbullet.py
Azelphur/pyPushBullet,3f9928ddbdc8fc6d3f656ea4d74b45ac369ed502,requests,urllib,"    def pushFile(self, device, file):
        data = {'type'      : 'file',
                'device_id' : device}
        filedata = ''
        with open(file, ""rb"") as f:
            filedata = f.read()
        return self._request_multiform(HOST + ""/pushes"", data, [('file', os.path.basename(file), filedata)])"," def pushFile(self, device_iden, file_name, file, file_type=None):
        """""" Push a file
            https://docs.pushbullet.com/v2/pushes
            https://docs.pushbullet.com/v2/upload-request

            Arguments:
            device_iden -- iden of device to push to
            file_name -- name of the file
            file -- a file object
            file_type -- file mimetype, if not set, python-magic will be used
        """"""

        if not file_type:
            mime = magic.Magic(mime=True)
            file_type = mime.from_buffer(file.read(1024))
            file.seek(0)

        data = {""file_name"": file_name,
                ""file_type"": file_type}

        upload_request = self._request(""GET"",
                                       HOST + ""/upload-request"",
                                       None,
                                       data)

        upload = requests.post(upload_request[""upload_url""],
                               data=upload_request[""data""],
                               files={""file"": file},
                               headers={""User-Agent"": ""pyPushBullet""})

        upload.raise_for_status()

        data = {""type"": ""file"",
                ""device_iden"": device_iden,
                ""file_name"": file_name,
                ""file_type"": file_type,
                ""file_url"": upload_request[""file_url""],
                ""body"": ""hello""}

        return self._request(""POST"", HOST + ""/pushes"", data)",update,"defpushFile(self,device,file):data={'type':'file','device_id':device}filedata=''withopen(file,""rb"")asf:filedata=f.read()returnself._request_multiform(HOST+""/pushes"",data,[('file',os.path.basename(file),filedata)])","defpushFile(self,device_iden,file_name,file,file_type=None):""""""Pushafilehttps://docs.pushbullet.com/v2/pusheshttps://docs.pushbullet.com/v2/upload-requestArguments:device_iden--idenofdevicetopushtofile_name--nameofthefilefile--afileobjectfile_type--filemimetype,ifnotset,python-magicwillbeused""""""ifnotfile_type:mime=magic.Magic(mime=True)file_type=mime.from_buffer(file.read(1024))file.seek(0)data={""file_name"":file_name,""file_type"":file_type}upload_request=self._request(""GET"",HOST+""/upload-request"",None,data)upload=requests.post(upload_request[""upload_url""],data=upload_request[""data""],files={""file"":file},headers={""User-Agent"":""pyPushBullet""})upload.raise_for_status()data={""type"":""file"",""device_iden"":device_iden,""file_name"":file_name,""file_type"":file_type,""file_url"":upload_request[""file_url""],""body"":""hello""}returnself._request(""POST"",HOST+""/pushes"",data)",pushbullet.py
Behappy123/market-maker,295a5e79521abf981e9f6ea9a88b3d53c233cc1a,requests,urllib,"class BitMEX(object):
    def __init__(self, base_url=None, symbol=None, login=None, password=None, otpToken=None, apiKey=None, apiSecret=None):
        self.base_url = base_url
        self.symbol = symbol
        self.token = None
        self.login = login
        self.password = password
        self.otpToken = otpToken
        self.apiKey = apiKey
        self.apiSecret = apiSecret","class BitMEX(object):
    def __init__(self, base_url=None, symbol=None, login=None, password=None, otpToken=None, apiKey=None, apiSecret=None):
        self.base_url = base_url
        self.symbol = symbol
        self.token = None
        self.login = login
        self.password = password
        self.otpToken = otpToken
        self.apiKey = apiKey
        self.apiSecret = apiSecret
        self.session = requests.Session()
        self.session.headers.update({'user-agent': 'liquidbot-' + constants.VERSION})",update,"classBitMEX(object):def__init__(self,base_url=None,symbol=None,login=None,password=None,otpToken=None,apiKey=None,apiSecret=None):self.base_url=base_urlself.symbol=symbolself.token=Noneself.login=loginself.password=passwordself.otpToken=otpTokenself.apiKey=apiKeyself.apiSecret=apiSecret","classBitMEX(object):def__init__(self,base_url=None,symbol=None,login=None,password=None,otpToken=None,apiKey=None,apiSecret=None):self.base_url=base_urlself.symbol=symbolself.token=Noneself.login=loginself.password=passwordself.otpToken=otpTokenself.apiKey=apiKeyself.apiSecret=apiSecretself.session=requests.Session()self.session.headers.update({'user-agent':'liquidbot-'+constants.VERSION})",bitmex.py
Behappy123/market-maker,295a5e79521abf981e9f6ea9a88b3d53c233cc1a,requests,urllib,"    def _curl_bitmex(self, api, query=None, postdict=None, timeout=3, verb=None):
        url = self.base_url + api

        # Handle data
        if query:
            url = url + ""?"" + urllib.urlencode(query)
        if postdict:
            postdata = json.dumps(postdict)
            request = urllib2.Request(url, postdata)
        else:
            request = urllib2.Request(url)

        # Handle custom verbs
        if verb:
            request.get_method = lambda: verb
        else:
            verb = 'POST' if postdict else 'GET'

        # Headers
        request.add_header('user-agent', 'liquidbot-' + constants.VERSION)
        request.add_header('Content-Type', 'application/json')

        # If API Key is specified, calculate signature.
        # When using API Key authentication, you must supply nonce, public key, and signature.
        if self.apiKey:
            nonce = int(round(time.time() * 1000))
            request.add_header('api-nonce', nonce)
            request.add_header('api-key', self.apiKey)
            request.add_header('api-signature', self._generate_signature(verb, url, nonce, postdict))

        # Otherwise use accessToken (returned by login with email/password/otp)
        elif self.token:
            request.add_header('accessToken', self.token)

        # Make the request
        try:
            response = urllib2.urlopen(request, timeout=timeout)
        except urllib2.HTTPError, e:











            # 401 - Auth error. Re-auth and re-run this request.
            if e.code == 401:
                if self.token == None:
                    if postdict: print postdict
                    print ""Login information or API Key incorrect, please check and restart.""
                    print e.readline()
                    exit(1)
                print ""Token expired, reauthenticating...""
                sleep(1)
                self.authenticate()
                return self._curl_bitmex(api, query, postdict, timeout, verb)

            # 404, can be thrown if order canceled does not exist.
            elif e.code == 404:
                if verb == 'DELETE':
                    print ""Order not found: %s"" % postdict['orderID']
                    return
                print ""Unable to contact the BitMEX API (404). "" + \
                    ""Request: %s \n %s"" % (url, json.dumps(postdict))
                exit(1)

            # 503 - BitMEX temporary downtime, likely due to a deploy. Try again
            elif e.code == 503:
                print ""Unable to contact the BitMEX API (503), retrying. "" + \
                    ""Request: %s \n %s"" % (url, json.dumps(postdict))
                sleep(1)
                return self._curl_bitmex(api, query, postdict, timeout, verb)
            # Unknown Error
            else:
                print ""Unhandled Error:"", e
                print ""Endpoint was: %s %s"" % (verb, api)
                exit(1)
        except urllib2.URLError, e:
            print ""Unable to contact the BitMEX API (URLError). Please check the URL. Retrying. "" + \






                ""Request: %s \n %s"" % (url, json.dumps(postdict))
            sleep(1)
            return self._curl_bitmex(api, query, postdict, timeout, verb)

        return json.loads(response.read())","def _curl_bitmex(self, api, query=None, postdict=None, timeout=3, verb=None):
        url = self.base_url + api










        # Handle custom verbs
        if not verb:


            verb = 'POST' if postdict else 'GET'

        headers = {'Content-Type': 'application/json'}



        # If API Key is specified, calculate signature.
        # When using API Key authentication, you must supply nonce, public key, and signature.
        if self.apiKey:
            nonce = int(round(time.time() * 1000))
            headers['api-nonce'] = nonce
            headers['api-key'] = self.apiKey
            headers['api-signature'] = self._generate_signature(verb, url, nonce, postdict)





        # Make the request
        try:
            data = None
            if postdict:
                data = json.dumps(postdict)
            req = requests.Request(verb, url, 
                data=data,
                params=query,
                headers=headers)
            prepped = self.session.prepare_request(req)
            response = self.session.send(prepped, timeout=timeout)
            # Make non-200s throw
            response.raise_for_status()

        except requests.exceptions.HTTPError, e:
            # 401 - Auth error. Re-auth and re-run this request.
            if response.status_code == 401:
                if self.token == None:

                    print ""Login information or API Key incorrect, please check and restart.""
                    if postdict: print postdict
                    exit(1)
                print ""Token expired, reauthenticating...""
                sleep(1)
                self.authenticate()
                return self._curl_bitmex(api, query, postdict, timeout, verb)

            # 404, can be thrown if order canceled does not exist.
            elif response.status_code == 404:
                if verb == 'DELETE':
                    print ""Order not found: %s"" % postdict['orderID']
                    return
                print ""Unable to contact the BitMEX API (404). "" + \
                    ""Request: %s \n %s"" % (url, json.dumps(postdict))
                exit(1)

            # 503 - BitMEX temporary downtime, likely due to a deploy. Try again
            elif response.status_code == 503:
                print ""Unable to contact the BitMEX API (503), retrying. "" + \
                    ""Request: %s \n %s"" % (url, json.dumps(postdict))
                sleep(1)
                return self._curl_bitmex(api, query, postdict, timeout, verb)
            # Unknown Error
            else:
                print ""Unhandled Error:"", e
                print ""Endpoint was: %s %s"" % (verb, api)
                exit(1)

        except requests.exceptions.Timeout, e:
            # Timeout, re-run this request
            print ""Timed out, retrying...""
            return self._curl_bitmex(api, query, postdict, timeout, verb)

        except requests.exceptions.ConnectionError, e:
            print ""Unable to contact the BitMEX API (ConnectionError). Please check the URL. Retrying. "" + \
                ""Request: %s \n %s"" % (url, json.dumps(postdict))
            sleep(1)
            return self._curl_bitmex(api, query, postdict, timeout, verb)

        return response.json()",update,"def_curl_bitmex(self,api,query=None,postdict=None,timeout=3,verb=None):url=self.base_url+api#Handledataifquery:url=url+""?""+urllib.urlencode(query)ifpostdict:postdata=json.dumps(postdict)request=urllib2.Request(url,postdata)else:request=urllib2.Request(url)#Handlecustomverbsifverb:request.get_method=lambda:verbelse:verb='POST'ifpostdictelse'GET'#Headersrequest.add_header('user-agent','liquidbot-'+constants.VERSION)request.add_header('Content-Type','application/json')#IfAPIKeyisspecified,calculatesignature.#WhenusingAPIKeyauthentication,youmustsupplynonce,publickey,andsignature.ifself.apiKey:nonce=int(round(time.time()*1000))request.add_header('api-nonce',nonce)request.add_header('api-key',self.apiKey)request.add_header('api-signature',self._generate_signature(verb,url,nonce,postdict))#OtherwiseuseaccessToken(returnedbyloginwithemail/password/otp)elifself.token:request.add_header('accessToken',self.token)#Maketherequesttry:response=urllib2.urlopen(request,timeout=timeout)excepturllib2.HTTPError,e:#401-Autherror.Re-authandre-runthisrequest.ife.code==401:ifself.token==None:ifpostdict:printpostdictprint""LogininformationorAPIKeyincorrect,pleasecheckandrestart.""printe.readline()exit(1)print""Tokenexpired,reauthenticating...""sleep(1)self.authenticate()returnself._curl_bitmex(api,query,postdict,timeout,verb)#404,canbethrownifordercanceleddoesnotexist.elife.code==404:ifverb=='DELETE':print""Ordernotfound:%s""%postdict['orderID']returnprint""UnabletocontacttheBitMEXAPI(404).""+\""Request:%s\n%s""%(url,json.dumps(postdict))exit(1)#503-BitMEXtemporarydowntime,likelyduetoadeploy.Tryagainelife.code==503:print""UnabletocontacttheBitMEXAPI(503),retrying.""+\""Request:%s\n%s""%(url,json.dumps(postdict))sleep(1)returnself._curl_bitmex(api,query,postdict,timeout,verb)#UnknownErrorelse:print""UnhandledError:"",eprint""Endpointwas:%s%s""%(verb,api)exit(1)excepturllib2.URLError,e:print""UnabletocontacttheBitMEXAPI(URLError).PleasechecktheURL.Retrying.""+\""Request:%s\n%s""%(url,json.dumps(postdict))sleep(1)returnself._curl_bitmex(api,query,postdict,timeout,verb)returnjson.loads(response.read())","def_curl_bitmex(self,api,query=None,postdict=None,timeout=3,verb=None):url=self.base_url+api#Handlecustomverbsifnotverb:verb='POST'ifpostdictelse'GET'headers={'Content-Type':'application/json'}#IfAPIKeyisspecified,calculatesignature.#WhenusingAPIKeyauthentication,youmustsupplynonce,publickey,andsignature.ifself.apiKey:nonce=int(round(time.time()*1000))headers['api-nonce']=nonceheaders['api-key']=self.apiKeyheaders['api-signature']=self._generate_signature(verb,url,nonce,postdict)#Maketherequesttry:data=Noneifpostdict:data=json.dumps(postdict)req=requests.Request(verb,url,data=data,params=query,headers=headers)prepped=self.session.prepare_request(req)response=self.session.send(prepped,timeout=timeout)#Makenon-200sthrowresponse.raise_for_status()exceptrequests.exceptions.HTTPError,e:#401-Autherror.Re-authandre-runthisrequest.ifresponse.status_code==401:ifself.token==None:print""LogininformationorAPIKeyincorrect,pleasecheckandrestart.""ifpostdict:printpostdictexit(1)print""Tokenexpired,reauthenticating...""sleep(1)self.authenticate()returnself._curl_bitmex(api,query,postdict,timeout,verb)#404,canbethrownifordercanceleddoesnotexist.elifresponse.status_code==404:ifverb=='DELETE':print""Ordernotfound:%s""%postdict['orderID']returnprint""UnabletocontacttheBitMEXAPI(404).""+\""Request:%s\n%s""%(url,json.dumps(postdict))exit(1)#503-BitMEXtemporarydowntime,likelyduetoadeploy.Tryagainelifresponse.status_code==503:print""UnabletocontacttheBitMEXAPI(503),retrying.""+\""Request:%s\n%s""%(url,json.dumps(postdict))sleep(1)returnself._curl_bitmex(api,query,postdict,timeout,verb)#UnknownErrorelse:print""UnhandledError:"",eprint""Endpointwas:%s%s""%(verb,api)exit(1)exceptrequests.exceptions.Timeout,e:#Timeout,re-runthisrequestprint""Timedout,retrying...""returnself._curl_bitmex(api,query,postdict,timeout,verb)exceptrequests.exceptions.ConnectionError,e:print""UnabletocontacttheBitMEXAPI(ConnectionError).PleasechecktheURL.Retrying.""+\""Request:%s\n%s""%(url,json.dumps(postdict))sleep(1)returnself._curl_bitmex(api,query,postdict,timeout,verb)returnresponse.json()",bitmex.py
BenjV/autosub-bootstrapbill,413d927a26d69fac90209cf08efe7ff2c6a84380,requests,urllib,"try:
            xml_sections = ET.parse(urllib.urlopen(url))
        except IOError, e:
            log.error(""Plex Media Server: Error while trying to contact: %s"" % e)
            return False
    else:
        #Fetch X-Plex-Token if it doesn't exist but a username/password do
        if not plexservertoken and plexserverusername and plexserverpassword:
            log.info(""Fetching a new X-Plex-Token from plex.tv"")
            authheader = ""Basic %s"" % base64.encodestring('%s:%s' % (plexserverusername, plexserverpassword))[:-1]

            request = urllib2.Request(""https://plex.tv/users/sign_in.xml"", '');
            request.add_header(""Authorization"", authheader)
            request.add_header(""X-Plex-Product"", ""AutoSub Notifier"")
            request.add_header(""X-Plex-Client-Identifier"", ""b3a6b24dcab2224bdb101fc6aa08ea5e2f3147d6"")
            request.add_header(""X-Plex-Version"", ""1.0"")


            response = urllib2.urlopen(request)

            auth_tree = ET.fromstring(response.read())
            plexservertoken = auth_tree.findall("".//authentication-token"")[0].text
            autosub.PLEXSERVERTOKEN = plexservertoken

        if plexservertoken:
            #Add X-Plex-Token header for myPlex support workaround
            response = urllib2.urlopen('%s/%s?X-Plex-Token=%s' % (
                ""%s:%s"" % (plexserverhost, plexserverport),
                'library/sections',
                plexservertoken
            ))

            xml_sections = ET.fromstring(response.read())

            sections = xml_sections.findall('Directory')

            if not sections:
                log.info(""Plex Media Server: Server not running on: %s:%s"" % (plexserverhost, plexserverport))
                return False

            for s in sections:
                if s.get('type') == ""show"":
                    try:
                        urllib2.urlopen('%s/%s?X-Plex-Token=%s' % (
                            ""%s:%s"" % (plexserverhost, plexserverport),
                            ""library/sections/%s/refresh"" % (s.get('key')),
                            plexservertoken
                        ))
                        log.info(""Plex Media Server: TV Shows library (%s) is currently updating."" % s.get('title'))
                        return True
                    except Exception, e:
                        log.error(""Plex Media Server: Error updating library section: %s"" % e)
                        return False

            return True","        try:
            xml_sections = ET.parse(requests.get(url))
        except IOError, e:
            log.error(""plexmediaserver: Error while trying to contact: %s"" % e)
            return False
    else:
        #Fetch X-Plex-Token if it doesn't exist but a username/password do
        if not plexservertoken and plexserverusername and plexserverpassword:
            log.info(""plexmediaserver: Fetching a new X-Plex-Token from plex.tv"")
            authheader = ""Basic %s"" % base64.encodestring('%s:%s' % (plexserverusername, plexserverpassword))[:-1]

            headers = {
                ""Authorization"": authheader,
                ""X-Plex-Product"": ""AutoSub Notifier"",
                ""X-Plex-Client-Identifier"": ""b3a6b24dcab2224bdb101fc6aa08ea5e2f3147d6"",
                ""X-Plex-Version"": ""1.0""
            }

            response = requests.post(""https://plex.tv/users/sign_in.xml"", headers=headers);

            auth_tree = ET.fromstring(response.text)
            plexservertoken = auth_tree.findall("".//authentication-token"")[0].text
            autosub.PLEXSERVERTOKEN = plexservertoken

        if plexservertoken:
            #Add X-Plex-Token header for myPlex support workaround
            response = requests.get('%s/%s?X-Plex-Token=%s' % (
                ""%s:%s"" % (plexserverhost, plexserverport),
                'library/sections',
                plexservertoken
            ))

            xml_sections = ET.fromstring(response.text)

            sections = xml_sections.findall('Directory')

            if not sections:
                log.info(""plexmediaserver: Server not running on: %s:%s"" % (plexserverhost, plexserverport))
                return False

            for s in sections:
                if s.get('type') == ""show"":
                    try:
                        requests.get('%s/%s?X-Plex-Token=%s' % (
                            ""%s:%s"" % (plexserverhost, plexserverport),
                            ""library/sections/%s/refresh"" % (s.get('key')),
                            plexservertoken
                        ))
                        log.info(""plexmediaserver: TV Shows library (%s) is currently updating."" % s.get('title'))
                        return True
                    except Exception, e:
                        log.error(""plexmediaserver: Error updating library section: %s"" % e)
                        return False

            return True",update,"try:xml_sections=ET.parse(urllib.urlopen(url))exceptIOError,e:log.error(""PlexMediaServer:Errorwhiletryingtocontact:%s""%e)returnFalseelse:#FetchX-Plex-Tokenifitdoesn'texistbutausername/passworddoifnotplexservertokenandplexserverusernameandplexserverpassword:log.info(""FetchinganewX-Plex-Tokenfromplex.tv"")authheader=""Basic%s""%base64.encodestring('%s:%s'%(plexserverusername,plexserverpassword))[:-1]request=urllib2.Request(""https://plex.tv/users/sign_in.xml"",'');request.add_header(""Authorization"",authheader)request.add_header(""X-Plex-Product"",""AutoSubNotifier"")request.add_header(""X-Plex-Client-Identifier"",""b3a6b24dcab2224bdb101fc6aa08ea5e2f3147d6"")request.add_header(""X-Plex-Version"",""1.0"")response=urllib2.urlopen(request)auth_tree=ET.fromstring(response.read())plexservertoken=auth_tree.findall("".//authentication-token"")[0].textautosub.PLEXSERVERTOKEN=plexservertokenifplexservertoken:#AddX-Plex-TokenheaderformyPlexsupportworkaroundresponse=urllib2.urlopen('%s/%s?X-Plex-Token=%s'%(""%s:%s""%(plexserverhost,plexserverport),'library/sections',plexservertoken))xml_sections=ET.fromstring(response.read())sections=xml_sections.findall('Directory')ifnotsections:log.info(""PlexMediaServer:Servernotrunningon:%s:%s""%(plexserverhost,plexserverport))returnFalseforsinsections:ifs.get('type')==""show"":try:urllib2.urlopen('%s/%s?X-Plex-Token=%s'%(""%s:%s""%(plexserverhost,plexserverport),""library/sections/%s/refresh""%(s.get('key')),plexservertoken))log.info(""PlexMediaServer:TVShowslibrary(%s)iscurrentlyupdating.""%s.get('title'))returnTrueexceptException,e:log.error(""PlexMediaServer:Errorupdatinglibrarysection:%s""%e)returnFalsereturnTrue","try:xml_sections=ET.parse(requests.get(url))exceptIOError,e:log.error(""plexmediaserver:Errorwhiletryingtocontact:%s""%e)returnFalseelse:#FetchX-Plex-Tokenifitdoesn'texistbutausername/passworddoifnotplexservertokenandplexserverusernameandplexserverpassword:log.info(""plexmediaserver:FetchinganewX-Plex-Tokenfromplex.tv"")authheader=""Basic%s""%base64.encodestring('%s:%s'%(plexserverusername,plexserverpassword))[:-1]headers={""Authorization"":authheader,""X-Plex-Product"":""AutoSubNotifier"",""X-Plex-Client-Identifier"":""b3a6b24dcab2224bdb101fc6aa08ea5e2f3147d6"",""X-Plex-Version"":""1.0""}response=requests.post(""https://plex.tv/users/sign_in.xml"",headers=headers);auth_tree=ET.fromstring(response.text)plexservertoken=auth_tree.findall("".//authentication-token"")[0].textautosub.PLEXSERVERTOKEN=plexservertokenifplexservertoken:#AddX-Plex-TokenheaderformyPlexsupportworkaroundresponse=requests.get('%s/%s?X-Plex-Token=%s'%(""%s:%s""%(plexserverhost,plexserverport),'library/sections',plexservertoken))xml_sections=ET.fromstring(response.text)sections=xml_sections.findall('Directory')ifnotsections:log.info(""plexmediaserver:Servernotrunningon:%s:%s""%(plexserverhost,plexserverport))returnFalseforsinsections:ifs.get('type')==""show"":try:requests.get('%s/%s?X-Plex-Token=%s'%(""%s:%s""%(plexserverhost,plexserverport),""library/sections/%s/refresh""%(s.get('key')),plexservertoken))log.info(""plexmediaserver:TVShowslibrary(%s)iscurrentlyupdating.""%s.get('title'))returnTrueexceptException,e:log.error(""plexmediaserver:Errorupdatinglibrarysection:%s""%e)returnFalsereturnTrue",autosub/notify/plexmediaserver.py
BotBotMe/botbot-plugins,a6ee91ee9ad6440449d56844638d230e9e07537f,requests,urllib,"    def search(self, line):
        message = line.text.encode('utf8')


        query = urllib.urlencode({'input': message,
                                  'appid': self.config['app_id']})
        xml = urllib.urlopen(self.url + query).read()

        try:
            tree = ElementTree.fromstring(xml)
        except ElementTree.ParseError:
            return ""Error parsing response from wolframalpha.com.""","    def search(self, line):
        message = line.text.encode('utf8')
        payload = {'input': message, 'appid': self.config['app_id']}

        response = requests.get(self.url, params=payload)



        try:
            tree = ElementTree.fromstring(response.content)
        except ElementTree.ParseError:
            return ""Error parsing response from wolframalpha.com.""",update,"defsearch(self,line):message=line.text.encode('utf8')query=urllib.urlencode({'input':message,'appid':self.config['app_id']})xml=urllib.urlopen(self.url+query).read()try:tree=ElementTree.fromstring(xml)exceptElementTree.ParseError:return""Errorparsingresponsefromwolframalpha.com.""","defsearch(self,line):message=line.text.encode('utf8')payload={'input':message,'appid':self.config['app_id']}response=requests.get(self.url,params=payload)try:tree=ElementTree.fromstring(response.content)exceptElementTree.ParseError:return""Errorparsingresponsefromwolframalpha.com.""",botbotme_plugins/plugins/wolfram.py
BotBotMe/botbot-plugins,a6ee91ee9ad6440449d56844638d230e9e07537f,requests,urllib,,"import pytest
from mock import patch
import requests
from botbotme_plugins.base import DummyApp
from botbotme_plugins.plugins import wolfram

class FakeResponse(object):
    """"""Dummy response from GitHub""""""
    status_code = 200
    content = """"""<?xml version='1.0' encoding='UTF-8'?>
<queryresult success='true'
    error='false'
    numpods='3'
    datatypes='City'
    timedout=''
    timedoutpods=''
    timing='1.48'
    parsetiming='0.775'
    parsetimedout='false'
    recalculate=''
    id='MSPa2221a5ca7e5e1i3gg90000046g17echc0h40615'
    host='http://www4b.wolframalpha.com'
    server='50'
    related='http://www4b.wolframalpha.com/api/v2/relatedQueries.jsp?id=MSPa2231a5ca7e5e1i3gg9000002df5d3ih5ie196f5&amp;s=50'
    version='2.6'>
<pod title='Input interpretation'
    scanner='Identity'
    id='Input'
    position='100'
    error='false'
    numsubpods='1'>
  <subpod title=''>
    <plaintext>convert 9:00 am PST | 11/03/2013 to UTC</plaintext>
      <img src='http://www4b.wolframalpha.com/Calculate/MSP/MSP2241a5ca7e5e1i3gg9000003bgehgb3bdf7bc1f?MSPStoreType=image/gif&amp;s=50'
        alt='convert 9:00 am PST | 11/03/2013 to UTC'
        title='convert 9:00 am PST | 11/03/2013 to UTC'
        width='271'
        height='18' />
  </subpod>
</pod>
<pod title='Result'
     scanner='Identity'
     id='Result'
     position='200'
     error='false'
     numsubpods='1'
     primary='true'>
  <subpod title='' primary='true'>
    <plaintext>4:00:00 pm GMT  |  Monday, March 11, 2013</plaintext>
    <img src='http://www4b.wolframalpha.com/Calculate/MSP/MSP2251a5ca7e5e1i3gg9000001a455cb2f0e511fc?MSPStoreType=image/gif&amp;s=50'
      alt='4:00:00 pm GMT  |  Monday, March 11, 2013'
      title='4:00:00 pm GMT  |  Monday, March 11, 2013'
      width='303'
      height='18' />
  </subpod>
</pod>
<pod title='Time difference from now (9:37:53 am GMT)'
     scanner='Date'
     id='TimeDifferenceFromNow (time)'
     position='300'
     error='false'
     numsubpods='1'>
  <subpod title=''>
    <plaintext>6 hours 22 minutes 6 seconds in the future</plaintext>
    <img src='http://www4b.wolframalpha.com/Calculate/MSP/MSP2261a5ca7e5e1i3gg9000003068g38g4a45bbi6?MSPStoreType=image/gif&amp;s=50'
        alt='6 hours 22 minutes 6 seconds in the future'
        title='6 hours 22 minutes 6 seconds in the future'
        width='272'
        height='18' />
  </subpod>
</pod>
<sources count='2'>
<source url='http://www.wolframalpha.com/sources/CityDataSourceInformationNotes.html'
text='City data' />
<source url='http://www.wolframalpha.com/sources/TimeZoneDataSourceInformationNotes.html'
    text='Time zone data' />
</sources>
</queryresult>
""""""


@pytest.fixture
def app():
    app = DummyApp(test_plugin=wolfram.Plugin())
    app.set_config('wolfram', {""app_id"": ""secret-appid""})
    return app


def test_github(app):
    # patch requests.get so we don't need to make a real call to GitHub 1Has a conversation.
    with patch.object(requests, 'get') as mock_get:
        mock_get.return_value = FakeResponse()
        responses = app.respond(""@What is 9 am PST in UTC ?"")
        mock_get.assert_called_with('http://api.wolframalpha.com/v2/query?',
            params={'input': 'What is 9 am PST in UTC ?',
                    'appid': 'secret-appid'})
        expected = u'Q: convert 9:00 am PST | 11/03/2013 to UTC\nA: 4:00:00 pm GMT  |  Monday, March 11, 2013'
        assert responses == [expected]",insert,,"importpytestfrommockimportpatchimportrequestsfrombotbotme_plugins.baseimportDummyAppfrombotbotme_plugins.pluginsimportwolframclassFakeResponse(object):""""""DummyresponsefromGitHub""""""status_code=200content=""""""<?xmlversion='1.0'encoding='UTF-8'?><queryresultsuccess='true'error='false'numpods='3'datatypes='City'timedout=''timedoutpods=''timing='1.48'parsetiming='0.775'parsetimedout='false'recalculate=''id='MSPa2221a5ca7e5e1i3gg90000046g17echc0h40615'host='http://www4b.wolframalpha.com'server='50'related='http://www4b.wolframalpha.com/api/v2/relatedQueries.jsp?id=MSPa2231a5ca7e5e1i3gg9000002df5d3ih5ie196f5&amp;s=50'version='2.6'><podtitle='Inputinterpretation'scanner='Identity'id='Input'position='100'error='false'numsubpods='1'><subpodtitle=''><plaintext>convert9:00amPST|11/03/2013toUTC</plaintext><imgsrc='http://www4b.wolframalpha.com/Calculate/MSP/MSP2241a5ca7e5e1i3gg9000003bgehgb3bdf7bc1f?MSPStoreType=image/gif&amp;s=50'alt='convert9:00amPST|11/03/2013toUTC'title='convert9:00amPST|11/03/2013toUTC'width='271'height='18'/></subpod></pod><podtitle='Result'scanner='Identity'id='Result'position='200'error='false'numsubpods='1'primary='true'><subpodtitle=''primary='true'><plaintext>4:00:00pmGMT|Monday,March11,2013</plaintext><imgsrc='http://www4b.wolframalpha.com/Calculate/MSP/MSP2251a5ca7e5e1i3gg9000001a455cb2f0e511fc?MSPStoreType=image/gif&amp;s=50'alt='4:00:00pmGMT|Monday,March11,2013'title='4:00:00pmGMT|Monday,March11,2013'width='303'height='18'/></subpod></pod><podtitle='Timedifferencefromnow(9:37:53amGMT)'scanner='Date'id='TimeDifferenceFromNow(time)'position='300'error='false'numsubpods='1'><subpodtitle=''><plaintext>6hours22minutes6secondsinthefuture</plaintext><imgsrc='http://www4b.wolframalpha.com/Calculate/MSP/MSP2261a5ca7e5e1i3gg9000003068g38g4a45bbi6?MSPStoreType=image/gif&amp;s=50'alt='6hours22minutes6secondsinthefuture'title='6hours22minutes6secondsinthefuture'width='272'height='18'/></subpod></pod><sourcescount='2'><sourceurl='http://www.wolframalpha.com/sources/CityDataSourceInformationNotes.html'text='Citydata'/><sourceurl='http://www.wolframalpha.com/sources/TimeZoneDataSourceInformationNotes.html'text='Timezonedata'/></sources></queryresult>""""""@pytest.fixturedefapp():app=DummyApp(test_plugin=wolfram.Plugin())app.set_config('wolfram',{""app_id"":""secret-appid""})returnappdeftest_github(app):#patchrequests.getsowedon'tneedtomakearealcalltoGitHub1Hasaconversation.withpatch.object(requests,'get')asmock_get:mock_get.return_value=FakeResponse()responses=app.respond(""@Whatis9amPSTinUTC?"")mock_get.assert_called_with('http://api.wolframalpha.com/v2/query?',params={'input':'Whatis9amPSTinUTC?','appid':'secret-appid'})expected=u'Q:convert9:00amPST|11/03/2013toUTC\nA:4:00:00pmGMT|Monday,March11,2013'assertresponses==[expected]",botbotme_plugins/tests/test_github.py
Chadsr/NordVPN-NetworkManager,b7e5afa847bb212a02ac624f13d592a81f23a41b,requests,urllib,"from urllib import error, request
def get_server_list(sort_by_load=False):
    try:
        resp = request.urlopen(API_ADDR + '/server', timeout=TIMEOUT)
        server_list = json.load(resp)

        if sort_by_load:
            return sorted(server_list, key=itemgetter('load'))
        else:
            return server_list
    except (error.URLError) or Exception as ex:
        logger.error(ex)
        return None","import requests
def get_server_list(sort_by_load=False):
    try:
        resp = requests.get(API_ADDR + '/server', timeout=TIMEOUT)
        server_list = resp.json()

        if sort_by_load:
            return sorted(server_list, key=itemgetter('load'))
        else:
            return server_list
    except Exception as ex:
        logger.error(ex)
        return None",update,"fromurllibimporterror,requestdefget_server_list(sort_by_load=False):try:resp=request.urlopen(API_ADDR+'/server',timeout=TIMEOUT)server_list=json.load(resp)ifsort_by_load:returnsorted(server_list,key=itemgetter('load'))else:returnserver_listexcept(error.URLError)orExceptionasex:logger.error(ex)returnNone","importrequestsdefget_server_list(sort_by_load=False):try:resp=requests.get(API_ADDR+'/server',timeout=TIMEOUT)server_list=resp.json()ifsort_by_load:returnsorted(server_list,key=itemgetter('load'))else:returnserver_listexceptExceptionasex:logger.error(ex)returnNone",
Chadsr/NordVPN-NetworkManager,b7e5afa847bb212a02ac624f13d592a81f23a41b,requests,urllib,"def get_nameservers():
    try:
        resp = request.urlopen(API_ADDR + '/dns/smart', timeout=TIMEOUT).read()
        return resp
    except (error.URLError) or Exception as ex:
        logger.error(ex)
        return None","def get_nameservers():
    try:
        resp = requests.get(API_ADDR + '/dns/smart', timeout=TIMEOUT)
        return resp.json()
    except Exception as ex:
        logger.error(ex)
        return None",update,"defget_nameservers():try:resp=request.urlopen(API_ADDR+'/dns/smart',timeout=TIMEOUT).read()returnrespexcept(error.URLError)orExceptionasex:logger.error(ex)returnNone","defget_nameservers():try:resp=requests.get(API_ADDR+'/dns/smart',timeout=TIMEOUT)returnresp.json()exceptExceptionasex:logger.error(ex)returnNone",nordnm/nordapi.py
Chadsr/NordVPN-NetworkManager,b7e5afa847bb212a02ac624f13d592a81f23a41b,requests,urllib,"def get_configs():
    try:
        resp = request.urlopen(API_ADDR + '/files/zipv2', timeout=TIMEOUT)
        return resp
    except (error.URLError) or Exception as ex:
        logger.error(ex)
        return None","def get_configs():
    try:
        resp = requests.get(API_ADDR + '/files/zipv2', timeout=TIMEOUT)
        return resp.content
    except Exception as ex:
        logger.error(ex)
        return None",update,"defget_configs():try:resp=request.urlopen(API_ADDR+'/files/zipv2',timeout=TIMEOUT)returnrespexcept(error.URLError)orExceptionasex:logger.error(ex)returnNone","defget_configs():try:resp=requests.get(API_ADDR+'/files/zipv2',timeout=TIMEOUT)returnresp.contentexceptExceptionasex:logger.error(ex)returnNone",nordnm/nordapi.py
Clinical-Genomics/scout,c96dca5ec69583fa9edf2f7ec246f62669e5cbdd,requests,urllib,"    try:
        LOG.info(""Requesting %s"", url)
        response = urllib.request.urlopen(url, timeout=20)
        if url.endswith("".gz""):
            LOG.info(""Decompress zipped file"")
            data = gzip.decompress(response.read())  # a `bytes` object
        else:
            data = response.read()  # a `bytes` object
        decoded_data = data.decode(""utf-8"")
    except HTTPError as err:
        LOG.warning(""Something went wrong, perhaps the api key is not valid?"")
        raise err
    except URLError as err:
        LOG.warning(""Something went wrong, are you connected to internet?"")
        raise err
    except timeout:
        LOG.error(""socket timed out - URL %s"", url)
        raise ValueError

    if ""Error"" in decoded_data:
        raise URLError(""Seems like url {} does not exist"".format(url))

    return decoded_data","    try:
        LOG.info(""Requesting %s"", url)
        response = requests.get(url, timeout=20)
        if response.status_code != 200:
            response.raise_for_status()
        LOG.info(""Encoded to %s"", response.encoding)
    except requests.exceptions.HTTPError as err:



        LOG.warning(""Something went wrong, perhaps the api key is not valid?"")
        raise err
    except requests.exceptions.MissingSchema as err:
        LOG.warning(""Something went wrong, perhaps url is invalid?"")
        raise err
    except requests.exceptions.Timeout as err:
        LOG.error(""socket timed out - URL %s"", url)
        raise err




    return response",update,"try:LOG.info(""Requesting%s"",url)response=urllib.request.urlopen(url,timeout=20)ifurl.endswith("".gz""):LOG.info(""Decompresszippedfile"")data=gzip.decompress(response.read())#a`bytes`objectelse:data=response.read()#a`bytes`objectdecoded_data=data.decode(""utf-8"")exceptHTTPErroraserr:LOG.warning(""Somethingwentwrong,perhapstheapikeyisnotvalid?"")raiseerrexceptURLErroraserr:LOG.warning(""Somethingwentwrong,areyouconnectedtointernet?"")raiseerrexcepttimeout:LOG.error(""sockettimedout-URL%s"",url)raiseValueErrorif""Error""indecoded_data:raiseURLError(""Seemslikeurl{}doesnotexist"".format(url))returndecoded_data","try:LOG.info(""Requesting%s"",url)response=requests.get(url,timeout=20)ifresponse.status_code!=200:response.raise_for_status()LOG.info(""Encodedto%s"",response.encoding)exceptrequests.exceptions.HTTPErroraserr:LOG.warning(""Somethingwentwrong,perhapstheapikeyisnotvalid?"")raiseerrexceptrequests.exceptions.MissingSchemaaserr:LOG.warning(""Somethingwentwrong,perhapsurlisinvalid?"")raiseerrexceptrequests.exceptions.Timeoutaserr:LOG.error(""sockettimedout-URL%s"",url)raiseerrreturnresponse",scout/utils/scout_requests.py
Clinical-Genomics/scout,c96dca5ec69583fa9edf2f7ec246f62669e5cbdd,requests,urllib,"try:
        exac_lines = fetch_resource(url)
    except URLError:
        LOG.info(""Failed to fetch exac constraint scores file from ftp server"")
        LOG.info(""Try to fetch from google bucket..."")
        url = (
            ""https://storage.googleapis.com/gnomad-public/legacy/exacv1_downloads/release0.3.1""
            ""/manuscript_data/forweb_cleaned_exac_r03_march16_z_data_pLI.txt.gz""
        )","    try:
        exac_lines = fetch_resource(url)
    except requests.exceptions.HTTPError:
        LOG.info(""Failed to fetch exac constraint scores file from ftp server"")
        LOG.info(""Try to fetch from google bucket..."")
        url = (
            ""https://storage.googleapis.com/gnomad-public/legacy/exacv1_downloads/release0.3.1""
            ""/manuscript_data/forweb_cleaned_exac_r03_march16_z_data_pLI.txt.gz""
        )",update,"try:exac_lines=fetch_resource(url)exceptURLError:LOG.info(""Failedtofetchexacconstraintscoresfilefromftpserver"")LOG.info(""Trytofetchfromgooglebucket..."")url=(""https://storage.googleapis.com/gnomad-public/legacy/exacv1_downloads/release0.3.1""""/manuscript_data/forweb_cleaned_exac_r03_march16_z_data_pLI.txt.gz"")","try:exac_lines=fetch_resource(url)exceptrequests.exceptions.HTTPError:LOG.info(""Failedtofetchexacconstraintscoresfilefromftpserver"")LOG.info(""Trytofetchfromgooglebucket..."")url=(""https://storage.googleapis.com/gnomad-public/legacy/exacv1_downloads/release0.3.1""""/manuscript_data/forweb_cleaned_exac_r03_march16_z_data_pLI.txt.gz"")",scout/utils/scout_requests.py
Clinical-Genomics/scout,c96dca5ec69583fa9edf2f7ec246f62669e5cbdd,requests,urllib," try:
        resp = urllib.request.urlopen(base_url.format(refseq_acc))
        xml_response = ET.parse(resp)
        version = xml_response.find(""IdList"").find(""Id"").text or version

    except HTTPError:
        LOG.warning(""Something went wrong, perhaps the refseq accession is not valid?"")
    except URLError:
        LOG.warning(""Something went wrong, are you connected to internet?"")
    except AttributeError:
        LOG.warning(""refseq accession not found"")

    return version","    try:
        resp = get_request(base_url.format(refseq_acc))
        tree = ElementTree.fromstring(resp.content)
        version = tree.find(""IdList"").find(""Id"").text or version

    except (
        requests.exceptions.HTTPError,
        requests.exceptions.MissingSchema,
        AttributeError,
    ):
        LOG.warning(""refseq accession not found"")

    return version",update,"try:resp=urllib.request.urlopen(base_url.format(refseq_acc))xml_response=ET.parse(resp)version=xml_response.find(""IdList"").find(""Id"").textorversionexceptHTTPError:LOG.warning(""Somethingwentwrong,perhapstherefseqaccessionisnotvalid?"")exceptURLError:LOG.warning(""Somethingwentwrong,areyouconnectedtointernet?"")exceptAttributeError:LOG.warning(""refseqaccessionnotfound"")returnversion","try:resp=get_request(base_url.format(refseq_acc))tree=ElementTree.fromstring(resp.content)version=tree.find(""IdList"").find(""Id"").textorversionexcept(requests.exceptions.HTTPError,requests.exceptions.MissingSchema,AttributeError,):LOG.warning(""refseqaccessionnotfound"")returnversion",scout/utils/scout_requests.py
CorruptComputer/ProtonDB-Tags,805cf641626b89b4df5d3d621fb151d1d12089c5,requests,urllib,"def is_native(app_id):
    # Wait 1 second before continuing, as Steam only allows 10 requests per 10 seconds, otherwise you get rate limited for a few minutes.
    time.sleep(1)

    try:
        # Thanks to u/FurbyOnSteroid for finding this! https://www.reddit.com/r/linux_gaming/comments/bxqsvs/protondb_to_steam_library_tool/eqal68r/
        req = urllib.request.urlopen(""https://store.steampowered.com/api/appdetails?appids="" + app_id + ""&filters=platforms"")
        data = req.read()
        enc = req.info().get_content_charset('utf-8')
        steam_api_result = json.loads(str(data.decode(enc)))

        # If steam can't find the game it will be False
        if steam_api_result[app_id][""success""] in [""True"", ""true"", True]:
            return (steam_api_result[app_id][""data""][""platforms""][""linux""] in [""True"", ""true"", True])

        return False
    except urllib.error.HTTPError:
        print(""Error pulling info from Steam API for "" + app_id + "" (you're probably being rate-limited)"")
        return False","def is_native(app_id):
    # Wait 1 second before continuing, as Steam only allows 10 requests per 10 seconds, otherwise you get rate limited for a few minutes.
    time.sleep(1)

    # Thanks to u/FurbyOnSteroid for finding this! https://www.reddit.com/r/linux_gaming/comments/bxqsvs/protondb_to_steam_library_tool/eqal68r/
    r = requests.get(""https://store.steampowered.com/api/appdetails?appids={}&filters=platforms"".format(app_id))
    if r.status_code != 200:
        print(""Error pulling info from Steam API for {}. You're probably being rate-limited"".format(app_id))
        return False


    steam_api_result = r.json()



    # If steam can't find the game it will be False
    if steam_api_result[app_id][""success""] in [""True"", ""true"", True]:
        return (steam_api_result[app_id][""data""][""platforms""][""linux""] in [""True"", ""true"", True])

    return False",update,"defis_native(app_id):#Wait1secondbeforecontinuing,asSteamonlyallows10requestsper10seconds,otherwiseyougetratelimitedforafewminutes.time.sleep(1)try:#Thankstou/FurbyOnSteroidforfindingthis!https://www.reddit.com/r/linux_gaming/comments/bxqsvs/protondb_to_steam_library_tool/eqal68r/req=urllib.request.urlopen(""https://store.steampowered.com/api/appdetails?appids=""+app_id+""&filters=platforms"")data=req.read()enc=req.info().get_content_charset('utf-8')steam_api_result=json.loads(str(data.decode(enc)))#Ifsteamcan'tfindthegameitwillbeFalseifsteam_api_result[app_id][""success""]in[""True"",""true"",True]:return(steam_api_result[app_id][""data""][""platforms""][""linux""]in[""True"",""true"",True])returnFalseexcepturllib.error.HTTPError:print(""ErrorpullinginfofromSteamAPIfor""+app_id+""(you'reprobablybeingrate-limited)"")returnFalse","defis_native(app_id):#Wait1secondbeforecontinuing,asSteamonlyallows10requestsper10seconds,otherwiseyougetratelimitedforafewminutes.time.sleep(1)#Thankstou/FurbyOnSteroidforfindingthis!https://www.reddit.com/r/linux_gaming/comments/bxqsvs/protondb_to_steam_library_tool/eqal68r/r=requests.get(""https://store.steampowered.com/api/appdetails?appids={}&filters=platforms"".format(app_id))ifr.status_code!=200:print(""ErrorpullinginfofromSteamAPIfor{}.You'reprobablybeingrate-limited"".format(app_id))returnFalsesteam_api_result=r.json()#Ifsteamcan'tfindthegameitwillbeFalseifsteam_api_result[app_id][""success""]in[""True"",""true"",True]:return(steam_api_result[app_id][""data""][""platforms""][""linux""]in[""True"",""true"",True])returnFalse",ProtonDB-to-Steam-Library.py
CorruptComputer/ProtonDB-Tags,805cf641626b89b4df5d3d621fb151d1d12089c5,requests,urllib,"def get_protondb_rating(app_id):
    req = urllib.request.urlopen(""https://www.protondb.com/api/v1/reports/summaries/"" + str(app_id) + "".json"")
    data = req.read()
    enc = req.info().get_content_charset('utf-8')
    protondb_json = json.loads(str(data.decode(enc)))

    return protondb_json[""trendingTier""]","def get_protondb_rating(app_id):
    r = requests.get(""https://www.protondb.com/api/v1/reports/summaries/{}.json"".format(app_id))
    if r.status_code != 200:
        raise ProtonDBError()
    protondb_json = r.json()
    # use trendingTier as this reflects a more up-to-date rating rather than an all-time rating
    return protondb_json[""trendingTier""]",update,"defget_protondb_rating(app_id):req=urllib.request.urlopen(""https://www.protondb.com/api/v1/reports/summaries/""+str(app_id)+"".json"")data=req.read()enc=req.info().get_content_charset('utf-8')protondb_json=json.loads(str(data.decode(enc)))returnprotondb_json[""trendingTier""]","defget_protondb_rating(app_id):r=requests.get(""https://www.protondb.com/api/v1/reports/summaries/{}.json"".format(app_id))ifr.status_code!=200:raiseProtonDBError()protondb_json=r.json()#usetrendingTierasthisreflectsamoreup-to-dateratingratherthananall-timeratingreturnprotondb_json[""trendingTier""]",ProtonDB-to-Steam-Library.py
Crystalnix/termius-cli,e397df611f3a7ce759f04c63c7ef945a8e9f9496,requests,urllib,"import base64
import json
try:
    import urllib.request as urllib2
except ImportError:
    import urllib2

from .utils import to_bytes, to_str



class API(object):

    API_URL = 'https://serverauditor.com/api/v1/'

    def get_auth_key(self, username, password):
        """""" Returns user's auth token. """"""

        request = urllib2.Request(self.API_URL + ""token/auth/"")
        auth_str = '%s:%s' % (username, password)
        auth = base64.encodestring(to_bytes(auth_str)).replace(b'\n', b'')
        request.add_header(""Authorization"", ""Basic %s"" % to_str(auth))
        response = urllib2.urlopen(request)
        return json.loads(to_str(response.read()))

    def get_keys_and_connections(self, username, auth_key):
        """""" Gets current keys and connections.

        Sends request for getting keys and connections using username and auth_key.
        """"""




        auth_header = ""ApiKey %s:%s"" % (username, auth_key)



        request = urllib2.Request(self.API_URL + ""terminal/ssh_key/?limit=100"")
        request.add_header(""Authorization"", auth_header)
        request.add_header(""Content-Type"", ""application/json"")
        response = urllib2.urlopen(request)
        keys = json.loads(to_str(response.read()))['objects']

        request = urllib2.Request(self.API_URL + ""terminal/connection/?limit=100"")
        request.add_header(""Authorization"", auth_header)
        request.add_header(""Content-Type"", ""application/json"")
        response = urllib2.urlopen(request)
        connections = json.loads(to_str(response.read()))['objects']

        return keys, connections

    def create_keys_and_connections(self, hosts, username, auth_key):
        """""" Creates keys and connections using hosts' configs.

        Sends request for creation keys and connections using username and key.
        """"""

        auth_header = ""ApiKey %s:%s"" % (username, auth_key)


        for host in hosts:





            key_numbers = []
            for ssh_key in host['ssh_key']:
                request = urllib2.Request(self.API_URL + ""terminal/ssh_key/"")
                request.add_header(""Authorization"", auth_header)
                request.add_header(""Content-Type"", ""application/json"")
                response = urllib2.urlopen(request, to_bytes(json.dumps(ssh_key)))
                key_numbers.append(int(response.headers['Location'].rstrip('/').rsplit('/', 1)[-1]))

            key = None
            if key_numbers:
                key = key_numbers[0]

            connection = {
                ""hostname"": host['hostname'],
                ""label"": host['host'],
                ""ssh_key"": key,
                ""ssh_password"": host['password'],
                ""ssh_username"": host['user'],
                ""port"": host['port']
            }
            request = urllib2.Request(self.API_URL + ""terminal/connection/"")
            request.add_header(""Authorization"", auth_header)
            request.add_header(""Content-Type"", ""application/json"")
            urllib2.urlopen(request, to_bytes(json.dumps(connection)))

        return","import requests
from requests.auth import AuthBase







class ServerauditorAuth(AuthBase):

    def __init__(self, username, apikey):
        self.username = username
        self.apikey = apikey














    @property
    def auth_header(self):
        return ""ApiKey {username}:{apikey}"".format(
            username=username, apikey=apikey
        )

    def __call__(self, request):
        request.headers['Authorization'] = self.auth_header
        return request







class API(object):









    host = 'serverauditor.com'
    base_url = 'https://{}/api/'.format(host)

    def __init__(self, username=None, apikey=None):
        if username and apikey:
            self.auth = ServerauditorAuth(username, apikey)
        else:
            self.auth = None

    def set_auth(self, username, apikey):
        self.auth = ServerauditorAuth(username, apikey)






    def request_url(self, endpoint):
        return self.base_url + endpoint


    def login(self, username, password):
        """"""Returns user's auth token.""""""
        response = requests.get(self.request_url(""token/auth/""),
                               auth=(username, password))
        assert response.status_code == 200








        response_payload = response.json()
        apikey = response_payload.pop('key')
        self.set_auth(username, apikey)
        return response_payload",update,"importbase64importjsontry:importurllib.requestasurllib2exceptImportError:importurllib2from.utilsimportto_bytes,to_strclassAPI(object):API_URL='https://serverauditor.com/api/v1/'defget_auth_key(self,username,password):""""""Returnsuser'sauthtoken.""""""request=urllib2.Request(self.API_URL+""token/auth/"")auth_str='%s:%s'%(username,password)auth=base64.encodestring(to_bytes(auth_str)).replace(b'\n',b'')request.add_header(""Authorization"",""Basic%s""%to_str(auth))response=urllib2.urlopen(request)returnjson.loads(to_str(response.read()))defget_keys_and_connections(self,username,auth_key):""""""Getscurrentkeysandconnections.Sendsrequestforgettingkeysandconnectionsusingusernameandauth_key.""""""auth_header=""ApiKey%s:%s""%(username,auth_key)request=urllib2.Request(self.API_URL+""terminal/ssh_key/?limit=100"")request.add_header(""Authorization"",auth_header)request.add_header(""Content-Type"",""application/json"")response=urllib2.urlopen(request)keys=json.loads(to_str(response.read()))['objects']request=urllib2.Request(self.API_URL+""terminal/connection/?limit=100"")request.add_header(""Authorization"",auth_header)request.add_header(""Content-Type"",""application/json"")response=urllib2.urlopen(request)connections=json.loads(to_str(response.read()))['objects']returnkeys,connectionsdefcreate_keys_and_connections(self,hosts,username,auth_key):""""""Createskeysandconnectionsusinghosts'configs.Sendsrequestforcreationkeysandconnectionsusingusernameandkey.""""""auth_header=""ApiKey%s:%s""%(username,auth_key)forhostinhosts:key_numbers=[]forssh_keyinhost['ssh_key']:request=urllib2.Request(self.API_URL+""terminal/ssh_key/"")request.add_header(""Authorization"",auth_header)request.add_header(""Content-Type"",""application/json"")response=urllib2.urlopen(request,to_bytes(json.dumps(ssh_key)))key_numbers.append(int(response.headers['Location'].rstrip('/').rsplit('/',1)[-1]))key=Noneifkey_numbers:key=key_numbers[0]connection={""hostname"":host['hostname'],""label"":host['host'],""ssh_key"":key,""ssh_password"":host['password'],""ssh_username"":host['user'],""port"":host['port']}request=urllib2.Request(self.API_URL+""terminal/connection/"")request.add_header(""Authorization"",auth_header)request.add_header(""Content-Type"",""application/json"")urllib2.urlopen(request,to_bytes(json.dumps(connection)))return","importrequestsfromrequests.authimportAuthBaseclassServerauditorAuth(AuthBase):def__init__(self,username,apikey):self.username=usernameself.apikey=apikey@propertydefauth_header(self):return""ApiKey{username}:{apikey}"".format(username=username,apikey=apikey)def__call__(self,request):request.headers['Authorization']=self.auth_headerreturnrequestclassAPI(object):host='serverauditor.com'base_url='https://{}/api/'.format(host)def__init__(self,username=None,apikey=None):ifusernameandapikey:self.auth=ServerauditorAuth(username,apikey)else:self.auth=Nonedefset_auth(self,username,apikey):self.auth=ServerauditorAuth(username,apikey)defrequest_url(self,endpoint):returnself.base_url+endpointdeflogin(self,username,password):""""""Returnsuser'sauthtoken.""""""response=requests.get(self.request_url(""token/auth/""),auth=(username,password))assertresponse.status_code==200response_payload=response.json()apikey=response_payload.pop('key')self.set_auth(username,apikey)returnresponse_payload",serverauditor_sshconfig/core/api.py
DMOJ/online-judge,72d419d08df7602cc2df40ad22ad6ac9cc394f50,requests,urllib,"        try:
            request = urllib.request.urlopen(self.mathoid_url, urlencode({
                'q': reescape.sub(lambda m: '\\' + m.group(0), formula).encode('utf-8'),
                'type': 'tex' if formula.startswith('\displaystyle') else 'inline-tex'
            }))
        except urllib.error.HTTPError as e:
            if e.code == 400:
                logger.error('Mathoid failed to render: %s\n%s', formula, e.read())
            else:
                logger.exception('Failed to connect to mathoid for: %s' % formula)


            return
        except Exception:
            logger.exception('Failed to connect to mathoid for: %s' % formula)
            return

        with closing(request) as f:
            data = f.read()
            try:
                data = json.loads(data)
            except ValueError:
                logger.exception('Invalid mathoid response for: %s\n%s', formula, data)
                return

        if not data['success']:
            logger.error('Mathoid failure for: %s\n%s', formula, data)
            return","        try:
            response = requests.post(self.mathoid_url, data={
                'q': reescape.sub(lambda m: '\\' + m.group(0), formula).encode('utf-8'),
                'type': 'tex' if formula.startswith('\displaystyle') else 'inline-tex'
            })
            response.raise_for_status()
            data = response.json()
        except requests.ConnectionError:
            logger.exception('Failed to connect to mathoid for: %s', formula)
            return
        except requests.HTTPError as e:
            logger.error('Mathoid failed to render: %s\n%s', formula, e.response.text)
            return
        except Exception:
            logger.exception('Failed to connect to mathoid for: %s', formula)
            return









        if not data['success']:
            logger.error('Mathoid failure for: %s\n%s', formula, data)
            return",update,"try:request=urllib.request.urlopen(self.mathoid_url,urlencode({'q':reescape.sub(lambdam:'\\'+m.group(0),formula).encode('utf-8'),'type':'tex'ifformula.startswith('\displaystyle')else'inline-tex'}))excepturllib.error.HTTPErrorase:ife.code==400:logger.error('Mathoidfailedtorender:%s\n%s',formula,e.read())else:logger.exception('Failedtoconnecttomathoidfor:%s'%formula)returnexceptException:logger.exception('Failedtoconnecttomathoidfor:%s'%formula)returnwithclosing(request)asf:data=f.read()try:data=json.loads(data)exceptValueError:logger.exception('Invalidmathoidresponsefor:%s\n%s',formula,data)returnifnotdata['success']:logger.error('Mathoidfailurefor:%s\n%s',formula,data)return","try:response=requests.post(self.mathoid_url,data={'q':reescape.sub(lambdam:'\\'+m.group(0),formula).encode('utf-8'),'type':'tex'ifformula.startswith('\displaystyle')else'inline-tex'})response.raise_for_status()data=response.json()exceptrequests.ConnectionError:logger.exception('Failedtoconnecttomathoidfor:%s',formula)returnexceptrequests.HTTPErrorase:logger.error('Mathoidfailedtorender:%s\n%s',formula,e.response.text)returnexceptException:logger.exception('Failedtoconnecttomathoidfor:%s',formula)returnifnotdata['success']:logger.error('Mathoidfailurefor:%s\n%s',formula,data)return",judge/utils/mathoid.py
DuckBoss/JJMumbleBot,e9fc6b5aeb4aa09b82ca0e0d302ff4c61b0ced1a,requests,urllib,"def get_vid_list(search):
    url = ""https://www.youtube.com/results?search_query="" + search.replace("" "", ""+"")
    req = urllib.request.Request(url)
    with urllib.request.urlopen(req) as response:
        html = response.read()
    soup = BeautifulSoup(html, 'html.parser')
    all_searches = soup.findAll(attrs={'class': 'yt-uix-tile-link'})
    search_results_list = []","def get_vid_list(search):
    url = ""https://www.youtube.com/results?search_query="" + search.replace("" "", ""+"")
    req = requests.get(url)
    html = req.text

    soup = BeautifulSoup(html, 'html.parser')
    all_searches = soup.findAll(attrs={'class': 'yt-uix-tile-link'})
    search_results_list = []",update,"defget_vid_list(search):url=""https://www.youtube.com/results?search_query=""+search.replace("""",""+"")req=urllib.request.Request(url)withurllib.request.urlopen(req)asresponse:html=response.read()soup=BeautifulSoup(html,'html.parser')all_searches=soup.findAll(attrs={'class':'yt-uix-tile-link'})search_results_list=[]","defget_vid_list(search):url=""https://www.youtube.com/results?search_query=""+search.replace("""",""+"")req=requests.get(url)html=req.textsoup=BeautifulSoup(html,'html.parser')all_searches=soup.findAll(attrs={'class':'yt-uix-tile-link'})search_results_list=[]",
Grazfather/BlackHatPython,6b85f63cfed95b21736639f399fcfa957efe27ad,requests,urllib,"def web_bruter(self):
        while not self.password_q.empty() and not self.found:
            brute = self.password_q.get().rstrip()
            jar = cookielib.FileCookieJar(""cookies"")
            opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(jar))

            response = opener.open(target_url)

            page = response.read()

            print ""Trying: %s : %s (%d left)"" % (self.username, brute, self.password_q.qsize())

            # parse out the hidden fields
            parser = BruteParser()
            parser.feed(page)

            post_tags = parser.tag_results

            # add our username nad password fields
            post_tags[username_field] = self.username
            post_tags[password_field] = brute

            login_data = urllib.urlencode(post_tags)
            login_response = opener.open(target_post, login_data)

            login_result = login_response.read()

            if success_check in login_result:
                self.found = True","    def web_bruter(self):
        while not self.password_q.empty() and not self.found:
            brute = self.password_q.get().rstrip()



            response = requests.get(target_url)

            page = response.text

            print ""Trying: %s : %s (%d left)"" % (self.username, brute, self.password_q.qsize())

            # parse out the hidden fields
            parser = BruteParser()
            parser.feed(page)

            post_tags = parser.tag_results

            # add our username nad password fields
            post_tags[username_field] = self.username
            post_tags[password_field] = brute

            login_response = requests.post(target_post, data=post_tags)


            login_result = login_response.text

            if success_check in login_result:
                self.found = True",update,"defweb_bruter(self):whilenotself.password_q.empty()andnotself.found:brute=self.password_q.get().rstrip()jar=cookielib.FileCookieJar(""cookies"")opener=urllib2.build_opener(urllib2.HTTPCookieProcessor(jar))response=opener.open(target_url)page=response.read()print""Trying:%s:%s(%dleft)""%(self.username,brute,self.password_q.qsize())#parseoutthehiddenfieldsparser=BruteParser()parser.feed(page)post_tags=parser.tag_results#addourusernamenadpasswordfieldspost_tags[username_field]=self.usernamepost_tags[password_field]=brutelogin_data=urllib.urlencode(post_tags)login_response=opener.open(target_post,login_data)login_result=login_response.read()ifsuccess_checkinlogin_result:self.found=True","defweb_bruter(self):whilenotself.password_q.empty()andnotself.found:brute=self.password_q.get().rstrip()response=requests.get(target_url)page=response.textprint""Trying:%s:%s(%dleft)""%(self.username,brute,self.password_q.qsize())#parseoutthehiddenfieldsparser=BruteParser()parser.feed(page)post_tags=parser.tag_results#addourusernamenadpasswordfieldspost_tags[username_field]=self.usernamepost_tags[password_field]=brutelogin_response=requests.post(target_post,data=post_tags)login_result=login_response.textifsuccess_checkinlogin_result:self.found=True",Chapter5/joomla_killer.py
HDI-Project/ATM,5379902c34dce9a45faf48bbf977395cb0bc3395,requests,urllib,"try:
            response = urllib.request.urlopen(PUBLIC_IP_URL, timeout=2)
            data = str(response.read().strip())
            # pull an ip-looking set of numbers from the response
            match = re.search('\d+\.\d+\.\d+\.\d+', data)
            if match:
                public_ip = match.group()","        try:
            public_ip = requests.get(PUBLIC_IP_URL).json()['ip']

",update,"try:response=urllib.request.urlopen(PUBLIC_IP_URL,timeout=2)data=str(response.read().strip())#pullanip-lookingsetofnumbersfromtheresponsematch=re.search('\d+\.\d+\.\d+\.\d+',data)ifmatch:public_ip=match.group()",try:public_ip=requests.get(PUBLIC_IP_URL).json()['ip'],atm/utilities.py
HDI-Project/ATM,5379902c34dce9a45faf48bbf977395cb0bc3395,requests,urllib,"logger.debug('downloading data from %s...' % url)
    f = urllib.request.urlopen(url)
    data = f.read()
    with open(path, 'wb') as outfile:
        outfile.write(data)
    logger.info('file saved at %s' % path)","   logger.debug('downloading data from %s...' % url)    data = requests.get(url).text    with open(path, 'wb') as outfile:        outfile.write(data)    logger.info('file saved at %s' % path)",update,"logger.debug('downloadingdatafrom%s...'%url)f=urllib.request.urlopen(url)data=f.read()withopen(path,'wb')asoutfile:outfile.write(data)logger.info('filesavedat%s'%path)","logger.debug('downloadingdatafrom%s...'%url)data=requests.get(url).textwithopen(path,'wb')asoutfile:outfile.write(data)logger.info('filesavedat%s'%path)",atm/utilities.py
HXLStandard/libhxl-python,6927750f4bb33efc3f7b6b2e0b28c69af2c310e0,requests,urllib,"if re.match(r'^(?:https?|ftp)://', origin):
        if sys.version_info < (3,):
            return urllib2.urlopen(origin)
        else:
            return urllib.request.urlopen(origin)","    if re.match(r'^(?:https?|ftp)://', origin):
        response = requests.get(origin, stream=True)
        return response.raw
",update,"ifre.match(r'^(?:https?|ftp)://',origin):ifsys.version_info<(3,):returnurllib2.urlopen(origin)else:returnurllib.request.urlopen(origin)","ifre.match(r'^(?:https?|ftp)://',origin):response=requests.get(origin,stream=True)returnresponse.raw",hxl/io.py
JohnMaguire/Cardinal,ce0e2bf11fd4cb2b9da6c30e0a306e6d263cb994,requests,urllib,"except urllib_error.HTTPError:
            cardinal.sendMsg(channel,
                             ""Couldn't find %s#%d"" % (repo, int(query)))","except requests.exceptions.HTTPError:
            cardinal.sendMsg(channel,
                             ""Couldn't find %s#%d"" % (repo, int(query)))",update,"excepturllib_error.HTTPError:cardinal.sendMsg(channel,""Couldn'tfind%s#%d""%(repo,int(query)))","exceptrequests.exceptions.HTTPError:cardinal.sendMsg(channel,""Couldn'tfind%s#%d""%(repo,int(query)))",plugins/github/plugin.py
JohnMaguire/Cardinal,ce0e2bf11fd4cb2b9da6c30e0a306e6d263cb994,requests,urllib,"try:
            if len(groups) == 3:
                self._show_issue(cardinal,
                                 channel,
                                 '%s/%s' % (groups[0], groups[1]),
                                 int(groups[2]))
            elif len(groups) == 2:
                self._show_repo(cardinal,
                                channel,
                                '%s/%s' % (groups[0], groups[1]))
        except urllib_error.HTTPError:
            raise EventRejectedMessage","        try:
            if len(groups) == 3:
                yield self._show_issue(cardinal,
                                       channel,
                                       '%s/%s' % (groups[0], groups[1]),
                                       int(groups[2]))
            elif len(groups) == 2:
                yield self._show_repo(cardinal,
                                      channel,
                                      '%s/%s' % (groups[0], groups[1]))
        except requests.exceptions.HTTPError:
            raise EventRejectedMessage",update,"try:iflen(groups)==3:self._show_issue(cardinal,channel,'%s/%s'%(groups[0],groups[1]),int(groups[2]))eliflen(groups)==2:self._show_repo(cardinal,channel,'%s/%s'%(groups[0],groups[1]))excepturllib_error.HTTPError:raiseEventRejectedMessage","try:iflen(groups)==3:yieldself._show_issue(cardinal,channel,'%s/%s'%(groups[0],groups[1]),int(groups[2]))eliflen(groups)==2:yieldself._show_repo(cardinal,channel,'%s/%s'%(groups[0],groups[1]))exceptrequests.exceptions.HTTPError:raiseEventRejectedMessage",plugins/github/plugin.py
JohnMaguire/Cardinal,ce0e2bf11fd4cb2b9da6c30e0a306e6d263cb994,requests,urllib,"def _form_request(self, endpoint, params={}):
        # Make request to specified endpoint and return JSON decoded result
        uh = urllib.request.urlopen(""https://api.github.com/"" +
                             endpoint + ""?"" +
                             urllib.parse.urlencode(params))




        return json.load(uh)","    @defer.inlineCallbacks
    def _form_request(self, endpoint, params=None):
        if params is None:
            params = {}

        r = yield deferToThread(requests.get, ""https://api.github.com/"" + endpoint,
                                params=params)
        r.raise_for_status()

        return r.json()",update,"def_form_request(self,endpoint,params={}):#MakerequesttospecifiedendpointandreturnJSONdecodedresultuh=urllib.request.urlopen(""https://api.github.com/""+endpoint+""?""+urllib.parse.urlencode(params))returnjson.load(uh)","@defer.inlineCallbacksdef_form_request(self,endpoint,params=None):ifparamsisNone:params={}r=yielddeferToThread(requests.get,""https://api.github.com/""+endpoint,params=params)r.raise_for_status()returnr.json()",plugins/github/plugin.py
KilledMufasa/AmputatorBot,5b194a1d9dddc00c927992e595392c18622d5e99,requests,urllib,,"headers = ['Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:69.0) Gecko/20100101 Firefox/69.0',
		'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36',
		'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36',
		'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36',
		'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',
		'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.1 (KHTML, like Gecko) Chrome/21.0.1180.83 Safari/537.1',
		'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.1',
		'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',
		'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.157 Safari/537.36',
		'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:65.0) Gecko/20100101 Firefox/65.0']",insert,,"headers=['Mozilla/5.0(WindowsNT10.0;Win64;x64;rv:69.0)Gecko/20100101Firefox/69.0','Mozilla/5.0(WindowsNT10.0;Win64;x64)AppleWebKit/537.36(KHTML,likeGecko)Chrome/74.0.3729.169Safari/537.36','Mozilla/5.0(WindowsNT6.1;Win64;x64)AppleWebKit/537.36(KHTML,likeGecko)Chrome/63.0.3239.132Safari/537.36','Mozilla/5.0(WindowsNT6.1;Win64;x64)AppleWebKit/537.36(KHTML,likeGecko)Chrome/74.0.3729.169Safari/537.36','Mozilla/5.0(WindowsNT10.0;Win64;x64)AppleWebKit/537.36(KHTML,likeGecko)Chrome/60.0.3112.113Safari/537.36','Mozilla/5.0(WindowsNT6.1;WOW64)AppleWebKit/537.1(KHTML,likeGecko)Chrome/21.0.1180.83Safari/537.1','Mozilla/5.0(WindowsNT6.1;WOW64;rv:40.0)Gecko/20100101Firefox/40.1','Mozilla/5.0(WindowsNT10.0;Win64;x64;rv:66.0)Gecko/20100101Firefox/66.0','Mozilla/5.0(WindowsNT10.0;Win64;x64)AppleWebKit/537.36(KHTML,likeGecko)Chrome/74.0.3729.157Safari/537.36','Mozilla/5.0(WindowsNT10.0;Win64;x64;rv:65.0)Gecko/20100101Firefox/65.0']",submissions_bot.py
KilledMufasa/AmputatorBot,5b194a1d9dddc00c927992e595392c18622d5e99,requests,urllib,,"def random_headers():
	# Get randomized user agent, set default accept and request English page, all of this is done to prevent 403 errors.
	return {'User-Agent': choice(headers),'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8','Accept-Language':'en-US'}",insert,,"defrandom_headers():#Getrandomizeduseragent,setdefaultacceptandrequestEnglishpage,allofthisisdonetoprevent403errors.return{'User-Agent':choice(headers),'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8','Accept-Language':'en-US'}",comments_bot.py
KilledMufasa/AmputatorBot,5b194a1d9dddc00c927992e595392c18622d5e99,requests,urllib,"Premake an urllib request (to fetch the submitted amp page)	
						req = urllib.request.Request(comments_urls[x])
						req.add_header('User-Agent', 'Mozilla/5.0 (Android 7.0; Mobile; rv:54.0) Gecko/54.0 Firefox/54.0')
						req.add_header('Accept', 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8')
						req.add_header('Accept-Charset', 'ISO-8859-1,utf-8;q=0.7,*;q=0.3')
						req.add_header('Accept-Encoding', 'none')
						req.add_header('Accept-Language', 'en-US,en;q=0.8')
						req.add_header('Connection', 'keep-alive')
						req.add_header('Referer', 'www.reddit.com')",,delete,"Premakeanurllibrequest(tofetchthesubmittedamppage)req=urllib.request.Request(comments_urls[x])req.add_header('User-Agent','Mozilla/5.0(Android7.0;Mobile;rv:54.0)Gecko/54.0Firefox/54.0')req.add_header('Accept','text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8')req.add_header('Accept-Charset','ISO-8859-1,utf-8;q=0.7,*;q=0.3')req.add_header('Accept-Encoding','none')req.add_header('Accept-Language','en-US,en;q=0.8')req.add_header('Connection','keep-alive')req.add_header('Referer','www.reddit.com')",,comments_bot.py
KilledMufasa/AmputatorBot,5b194a1d9dddc00c927992e595392c18622d5e99,requests,urllib,"content = urlopen(comments_urls[x])

                                                        # Make the received data readable
                                                        print(""Making a soup...\n"")
                                                        soup = BeautifulSoup(content, features= ""lxml"")","req = requests.get(comments_urls[x],headers=random_headers())

							# Make the received data readable
							print(""Making a soup...\n"")
							soup = BeautifulSoup(req.text, features= ""lxml"")",update,"content=urlopen(comments_urls[x])#Makethereceiveddatareadableprint(""Makingasoup...\n"")soup=BeautifulSoup(content,features=""lxml"")","req=requests.get(comments_urls[x],headers=random_headers())#Makethereceiveddatareadableprint(""Makingasoup...\n"")soup=BeautifulSoup(req.text,features=""lxml"")",comments_bot.py
KilledMufasa/AmputatorBot,5b194a1d9dddc00c927992e595392c18622d5e99,requests,urllib,submission_content = urlopen(submission.url),"req = requests.get(submission.url,headers=random_headers())",update,submission_content=urlopen(submission.url),"req=requests.get(submission.url,headers=random_headers())",submissions_bot.py
Koed00/django-whoshere,749308d2f10508967c4ca4003dacc3a5a2e04af5,requests,urllib,"try:
        response = urllib.urlopen('https://www.telize.com/geoip/{}'.format(ip))
    except URLError:
        location = {'city': 'error', 'country': 'error'}
    else:
        data = json.loads(response.read())
        location = {'city': data.get('city', 'unknown'), 'country': data.get('country', 'unknown')}
        cache.set(key, location, TIMEOUT)
    return location","    try:
        request = requests.get('https://www.telize.com/geoip/{}'.format(ip))
    except requests.exceptions.RequestException:
        location = {'city': 'error', 'country': 'error'}
    else:
        data = request.json()
        location = {'city': data.get('city', 'unknown'), 'country': data.get('country', 'unknown')}
        cache.set(key, location, TIMEOUT)
    return location",update,"try:response=urllib.urlopen('https://www.telize.com/geoip/{}'.format(ip))exceptURLError:location={'city':'error','country':'error'}else:data=json.loads(response.read())location={'city':data.get('city','unknown'),'country':data.get('country','unknown')}cache.set(key,location,TIMEOUT)returnlocation","try:request=requests.get('https://www.telize.com/geoip/{}'.format(ip))exceptrequests.exceptions.RequestException:location={'city':'error','country':'error'}else:data=request.json()location={'city':data.get('city','unknown'),'country':data.get('country','unknown')}cache.set(key,location,TIMEOUT)returnlocation",django_whoshere/apps.py
Linaro/lava,b037a86607eec04cf8169d9d7c434cc332329699,requests,urllib,"utils.mkdir(self.notification.test_job.output_dir)
                # only write the file once
                if not os.path.exists(job_data_file):
                    with gzip.open(job_data_file, 'wb') as output:
                        output.write(simplejson.dumps(callback_data).encode('utf-8'))
        headers = {}

        if callback_data:
            if callback_data.get('token') is not None:
                headers['Authorization'] = callback_data['token']
            if self.content_type == NotificationCallback.JSON:
                callback_data = simplejson.dumps(callback_data).encode(""utf-8"")
                headers['Content-Type'] = 'application/json'


            else:
                callback_data = urlencode(callback_data).encode(""utf-8"")
                headers['Content-Type'] = 'application/x-www-form-urlencoded'

        if self.url:
            try:
                logger.info(""Sending request to callback url %s"" % self.url)
                request = Request(self.url, callback_data, headers)
                urlopen(request)

            except Exception as ex:
                logger.warning(""Problem sending request to %s: %s"" % (
                    self.url, ex))","utils.mkdir(self.notification.test_job.output_dir)
                # only write the file once
                if not os.path.exists(job_data_file):
                    with gzip.open(job_data_file, 'wb') as output:
                        output.write(simplejson.dumps(data).encode('utf-8'))
        try:
            logger.info(""Sending request to callback url %s"" % self.url)
            headers = {}
            if self.token is not None:
                headers['Authorization'] = self.token

            if self.method == NotificationCallback.GET:
                ret = requests.get(self.url, headers=headers)
            elif self.content_type == NotificationCallback.JSON:
                ret = requests.post(self.url, json=data, headers=headers)
            else:
                ret = requests.post(self.url, data=data, headers=headers)
            ret.raise_for_status()







        except Exception as ex:
            logger.warning(""Problem sending request to %s: %s"" % (
                self.url, ex))",update,"utils.mkdir(self.notification.test_job.output_dir)#onlywritethefileonceifnotos.path.exists(job_data_file):withgzip.open(job_data_file,'wb')asoutput:output.write(simplejson.dumps(callback_data).encode('utf-8'))headers={}ifcallback_data:ifcallback_data.get('token')isnotNone:headers['Authorization']=callback_data['token']ifself.content_type==NotificationCallback.JSON:callback_data=simplejson.dumps(callback_data).encode(""utf-8"")headers['Content-Type']='application/json'else:callback_data=urlencode(callback_data).encode(""utf-8"")headers['Content-Type']='application/x-www-form-urlencoded'ifself.url:try:logger.info(""Sendingrequesttocallbackurl%s""%self.url)request=Request(self.url,callback_data,headers)urlopen(request)exceptExceptionasex:logger.warning(""Problemsendingrequestto%s:%s""%(self.url,ex))","utils.mkdir(self.notification.test_job.output_dir)#onlywritethefileonceifnotos.path.exists(job_data_file):withgzip.open(job_data_file,'wb')asoutput:output.write(simplejson.dumps(data).encode('utf-8'))try:logger.info(""Sendingrequesttocallbackurl%s""%self.url)headers={}ifself.tokenisnotNone:headers['Authorization']=self.tokenifself.method==NotificationCallback.GET:ret=requests.get(self.url,headers=headers)elifself.content_type==NotificationCallback.JSON:ret=requests.post(self.url,json=data,headers=headers)else:ret=requests.post(self.url,data=data,headers=headers)ret.raise_for_status()exceptExceptionasex:logger.warning(""Problemsendingrequestto%s:%s""%(self.url,ex))",lava_scheduler_app/models.py
MTG/freesound,8080e9500abdc1b7be7c7a76ef01b4a7524e24f8,requests,urllib,"def queries_stats_ajax(request):
    req = urllib2.Request(""http://mtg-logserver.s.upf.edu/graylog/api/search/universal/relative/terms?query=%2A&range=1209600&filter=streams%3A531051bee4b0f1248696785a&field=query"")
    base64string = base64.b64encode('%s:%s' % (settings.GRAYLOG_USERNAME,
        settings.GRAYLOG_PASSWORD))
    req.add_header(""Authorization"", ""Basic %s"" % base64string)
    return JsonResponse(json.load(urllib2.urlopen(req)))","def queries_stats_ajax(request):
    params = {
        'query': ':',
        'range': 14 * 60 * 60 * 24,
        'filter': 'streams:531051bee4b0f1248696785a',
        'field': 'query'
    }
    req = requests.get('http://mtg-logserver.s.upf.edu/graylog/api/search/universal/relative/terms',
            auth=(settings.GRAYLOG_USERNAME, settings.GRAYLOG_PASSWORD), params=params)
    return JsonResponse(req.json())",update,"defqueries_stats_ajax(request):req=urllib2.Request(""http://mtg-logserver.s.upf.edu/graylog/api/search/universal/relative/terms?query=%2A&range=1209600&filter=streams%3A531051bee4b0f1248696785a&field=query"")base64string=base64.b64encode('%s:%s'%(settings.GRAYLOG_USERNAME,settings.GRAYLOG_PASSWORD))req.add_header(""Authorization"",""Basic%s""%base64string)returnJsonResponse(json.load(urllib2.urlopen(req)))","defqueries_stats_ajax(request):params={'query':':','range':14*60*60*24,'filter':'streams:531051bee4b0f1248696785a','field':'query'}req=requests.get('http://mtg-logserver.s.upf.edu/graylog/api/search/universal/relative/terms',auth=(settings.GRAYLOG_USERNAME,settings.GRAYLOG_PASSWORD),params=params)returnJsonResponse(req.json())",monitor/views.py
MTG/freesound,8080e9500abdc1b7be7c7a76ef01b4a7524e24f8,requests,urllib,"@login_required
def api_usage_stats_ajax(request):
    query = urllib.urlencode({
        'query': 'api_client_username:%s' % (request.user.username),
        'range': '604800',
        'filter': 'streams:530f2ec5e4b0f124869546d0',
        'interval': 'day'
    })
    req = urllib2.Request(""http://mtg-logserver.s.upf.edu/graylog/api/search/universal/relative/histogram?%s"" % (query))
    base64string = base64.b64encode('%s:%s' % (settings.GRAYLOG_USERNAME,
        settings.GRAYLOG_PASSWORD))
    req.add_header(""Authorization"", ""Basic %s"" % base64string)
    return JsonResponse(json.load(urllib2.urlopen(req)))","@login_required
def api_usage_stats_ajax(request, client_id):
    params = {
        'query': 'api_client_id:%s' % (client_id),
        'range': 14 * 60 * 60 * 24,
        'filter': 'streams:530f2ec5e4b0f124869546d0',
        'interval': 'day'
    }
    req = requests.get('http://mtg-logserver.s.upf.edu/graylog/api/search/universal/relative/histogram',
            auth=(settings.GRAYLOG_USERNAME, settings.GRAYLOG_PASSWORD), params=params)
    try:
        return JsonResponse(req.json())
    except requests.HTTPError:
        return JsonResponse({'error': 'Error at collecting usage stats'})",update,"@login_requireddefapi_usage_stats_ajax(request):query=urllib.urlencode({'query':'api_client_username:%s'%(request.user.username),'range':'604800','filter':'streams:530f2ec5e4b0f124869546d0','interval':'day'})req=urllib2.Request(""http://mtg-logserver.s.upf.edu/graylog/api/search/universal/relative/histogram?%s""%(query))base64string=base64.b64encode('%s:%s'%(settings.GRAYLOG_USERNAME,settings.GRAYLOG_PASSWORD))req.add_header(""Authorization"",""Basic%s""%base64string)returnJsonResponse(json.load(urllib2.urlopen(req)))","@login_requireddefapi_usage_stats_ajax(request,client_id):params={'query':'api_client_id:%s'%(client_id),'range':14*60*60*24,'filter':'streams:530f2ec5e4b0f124869546d0','interval':'day'}req=requests.get('http://mtg-logserver.s.upf.edu/graylog/api/search/universal/relative/histogram',auth=(settings.GRAYLOG_USERNAME,settings.GRAYLOG_PASSWORD),params=params)try:returnJsonResponse(req.json())exceptrequests.HTTPError:returnJsonResponse({'error':'Erroratcollectingusagestats'})",monitor/views.py
Microsoft/agogosml,fd5d48383b14704fbccd73a2063bcdef5ede600d,requests,urllib,"encoded_data = urllib.parse.urlencode(data).encode('utf-8')
    req = urllib.request.Request(OUTPUT_URL, encoded_data)
    response = urllib.request.urlopen(req)
    logging.info('Response received from output writer')
    # TO DO: Design retry policy based on BL. For now, print result
    print(response.read().decode('utf-8'))","    request = requests.post(OUTPUT_URL, data=str(data))
    if request.status_code != 200:
        logging.error(
            ""Error with a request {} and message not sent was {}"".format(
                request.status_code, data))
    else:
        logging.info(""{} Response received from output writer"".format(
            request.status_code))",update,"encoded_data=urllib.parse.urlencode(data).encode('utf-8')req=urllib.request.Request(OUTPUT_URL,encoded_data)response=urllib.request.urlopen(req)logging.info('Responsereceivedfromoutputwriter')#TODO:DesignretrypolicybasedonBL.Fornow,printresultprint(response.read().decode('utf-8'))","request=requests.post(OUTPUT_URL,data=str(data))ifrequest.status_code!=200:logging.error(""Errorwitharequest{}andmessagenotsentwas{}"".format(request.status_code,data))else:logging.info(""{}Responsereceivedfromoutputwriter"".format(request.status_code))",sample_app/main.py
Nervengift/kvvliveapi,4a256ab05c7809d086bc12e3235b6e0e41793f32,requests,urllib,"def _query(path, params={}):
    params[""key""] = API_KEY
    url = API_BASE + path + ""?"" + urlencode(params)
    req = _urllib.Request(url)

    #try:
    handle = _urllib.urlopen(req)
    #except IOError as e:
    #    if hasattr(e, ""code""):
    #        if e.code != 403:
    #            print(""We got another error"")
    #            print(e.code)
    #        else:
    #            print(e.headers)
    #            print(e.headers[""www-authenticate""])
    #    return None; #TODO: Schoenere Fehlerbehandlung

    return json.loads(handle.read().decode(""utf8""))","def _query(path, params={}):
    params[""key""] = API_KEY
    response = requests.get(""{}{}"".format(API_BASE, path), params=params)
    return response.json()












",update,"def_query(path,params={}):params[""key""]=API_KEYurl=API_BASE+path+""?""+urlencode(params)req=_urllib.Request(url)#try:handle=_urllib.urlopen(req)#exceptIOErrorase:#ifhasattr(e,""code""):#ife.code!=403:#print(""Wegotanothererror"")#print(e.code)#else:#print(e.headers)#print(e.headers[""www-authenticate""])#returnNone;#TODO:SchoenereFehlerbehandlungreturnjson.loads(handle.read().decode(""utf8""))","def_query(path,params={}):params[""key""]=API_KEYresponse=requests.get(""{}{}"".format(API_BASE,path),params=params)returnresponse.json()",kvvliveapi/KVV.py
Nettacker/Nettacker,e09cda0967872260e1065cd8f444118790091a6e,requests,urllib,"def _update(__version__, __code_name__, language):
    from core.compatible import version
    if version() is 2:
        from urllib import urlopen
    if version() is 3:
        from urllib.request import urlopen
    try:
        data = urlopen(url).read()
        if version() is 3:
            data = data.decode(""utf-8"")
        if __version__ + ' ' + __code_name__ == data.rsplit('\n')[0]:","def _update(__version__, __code_name__, language):





    try:
        data = requests.get(url, headers={""User-Agent"" : ""OWASP Nettacker""}).content
        if version() is 3:
            data = data.decode(""utf-8"")
        if __version__ + ' ' + __code_name__ == data.rsplit('\n')[0]:",update,"def_update(__version__,__code_name__,language):fromcore.compatibleimportversionifversion()is2:fromurllibimporturlopenifversion()is3:fromurllib.requestimporturlopentry:data=urlopen(url).read()ifversion()is3:data=data.decode(""utf-8"")if__version__+''+__code_name__==data.rsplit('\n')[0]:","def_update(__version__,__code_name__,language):try:data=requests.get(url,headers={""User-Agent"":""OWASPNettacker""}).contentifversion()is3:data=data.decode(""utf-8"")if__version__+''+__code_name__==data.rsplit('\n')[0]:",core/update.py
Nettacker/Nettacker,e09cda0967872260e1065cd8f444118790091a6e,requests,urllib,"def _check(__version__, __code_name__, language):
    from core.compatible import version
    if version() is 2:
        from urllib import urlopen
    if version() is 3:
        from urllib.request import urlopen
    try:
        data = urlopen(url).read()
        if version() is 3:
            data = data.decode(""utf-8"")
        if __version__ + ' ' + __code_name__ == data.rsplit('\n')[0]:","def _check(__version__, __code_name__, language):





    try:
        data = requests.get(url, headers={""User-Agent"": ""OWASP Nettacker""}).content
        if version() is 3:
            data = data.decode(""utf-8"")
        if __version__ + ' ' + __code_name__ == data.rsplit('\n')[0]:",update,"def_check(__version__,__code_name__,language):fromcore.compatibleimportversionifversion()is2:fromurllibimporturlopenifversion()is3:fromurllib.requestimporturlopentry:data=urlopen(url).read()ifversion()is3:data=data.decode(""utf-8"")if__version__+''+__code_name__==data.rsplit('\n')[0]:","def_check(__version__,__code_name__,language):try:data=requests.get(url,headers={""User-Agent"":""OWASPNettacker""}).contentifversion()is3:data=data.decode(""utf-8"")if__version__+''+__code_name__==data.rsplit('\n')[0]:",core/update.py
PSLmodels/Tax-Calculator,8edbd7d8322c2974c192ceb767061e428b025fcc,requests,urllib,"if os.path.isfile(assump):
                txt = open(assump, 'r').read()
            elif assump.startswith('http'):
                txt = urllib.request.urlopen(assump).read().decode()


            else:
                txt = assump
            (cons_dict,","if os.path.isfile(assump):
                txt = open(assump, 'r').read()
            elif assump.startswith('http'):
                req = requests.get(assump)
                req.raise_for_status()
                txt = req.text
            else:
                txt = assump
            (cons_dict,",update,"ifos.path.isfile(assump):txt=open(assump,'r').read()elifassump.startswith('http'):txt=urllib.request.urlopen(assump).read().decode()else:txt=assump(cons_dict,","ifos.path.isfile(assump):txt=open(assump,'r').read()elifassump.startswith('http'):req=requests.get(assump)req.raise_for_status()txt=req.textelse:txt=assump(cons_dict,",taxcalc/calculator.py
PSLmodels/Tax-Calculator,8edbd7d8322c2974c192ceb767061e428b025fcc,requests,urllib,"if os.path.isfile(reform):
                txt = open(reform, 'r').read()
            elif reform.startswith('http'):
                txt = urllib.request.urlopen(reform).read().decode()


            else:
                txt = reform
            rpol_dict = (","            if os.path.isfile(reform):
                txt = open(reform, 'r').read()
            elif reform.startswith('http'):
                req = requests.get(reform)
                req.raise_for_status()
                txt = req.text
            else:
                txt = reform
            rpol_dict = (",update,"ifos.path.isfile(reform):txt=open(reform,'r').read()elifreform.startswith('http'):txt=urllib.request.urlopen(reform).read().decode()else:txt=reformrpol_dict=(","ifos.path.isfile(reform):txt=open(reform,'r').read()elifreform.startswith('http'):req=requests.get(reform)req.raise_for_status()txt=req.textelse:txt=reformrpol_dict=(",taxcalc/calculator.py
PSLmodels/Tax-Calculator,8edbd7d8322c2974c192ceb767061e428b025fcc,requests,urllib,"           if os.path.isfile(assump):
                txt = open(assump, 'r').read()
            elif assump.startswith('http'):
                txt = urllib.request.urlopen(assump).read().decode()


            else:
                txt = assump
            # strip out //-comments without changing line numbers","            if os.path.isfile(assump):
                txt = open(assump, 'r').read()
            elif assump.startswith('http'):
                req = requests.get(assump)
                req.raise_for_status()
                txt = req.text
            else:
                txt = assump
            # strip out //-comments without changing line numbers",update,"ifos.path.isfile(assump):txt=open(assump,'r').read()elifassump.startswith('http'):txt=urllib.request.urlopen(assump).read().decode()else:txt=assump#stripout//-commentswithoutchanginglinenumbers","ifos.path.isfile(assump):txt=open(assump,'r').read()elifassump.startswith('http'):req=requests.get(assump)req.raise_for_status()txt=req.textelse:txt=assump#stripout//-commentswithoutchanginglinenumbers",taxcalc/calculator.py
PSLmodels/Tax-Calculator,8edbd7d8322c2974c192ceb767061e428b025fcc,requests,urllib,"    with pytest.raises(ValueError):
        Calculator.read_json_assumptions(list())
    with pytest.raises(urllib.request.URLError):
        Calculator.read_json_assumptions('http://unknown-url')
    assump_filename = assumption_file.name
    file_dict = Calculator.read_json_assumptions(assump_filename)","    with pytest.raises(ValueError):
        Calculator.read_json_assumptions(list())
    with pytest.raises(requests.exceptions.ConnectionError):
        Calculator.read_json_assumptions('http://unknown-url')
    assump_filename = assumption_file.name
    file_dict = Calculator.read_json_assumptions(assump_filename)",update,withpytest.raises(ValueError):Calculator.read_json_assumptions(list())withpytest.raises(urllib.request.URLError):Calculator.read_json_assumptions('http://unknown-url')assump_filename=assumption_file.namefile_dict=Calculator.read_json_assumptions(assump_filename),withpytest.raises(ValueError):Calculator.read_json_assumptions(list())withpytest.raises(requests.exceptions.ConnectionError):Calculator.read_json_assumptions('http://unknown-url')assump_filename=assumption_file.namefile_dict=Calculator.read_json_assumptions(assump_filename),taxcalc/tests/test_calculator.py
Pitt-CSC/PittAPI,81dba02bc4757d3ad75b4f64f30d20c21ac6c1f5,requests,urllib,"    def _retrieve_from_url(url):
        page = urlopen(url)
        soup = BeautifulSoup(page.read(), 'html.parser')
        courses = soup.findAll(""tr"", {""class"": ""odd""})
        courses_even = soup.findAll(""tr"", {""class"": ""even""})
        courses.extend(courses_even)","s = requests.session()                  page = s.get(url)
        soup = BeautifulSoup(page.text, 'html.parser')
        courses = soup.findAll(""tr"", {""class"": ""odd""})
        courses_even = soup.findAll(""tr"", {""class"": ""even""})
        courses.extend(courses_even)",update,"def_retrieve_from_url(url):page=urlopen(url)soup=BeautifulSoup(page.read(),'html.parser')courses=soup.findAll(""tr"",{""class"":""odd""})courses_even=soup.findAll(""tr"",{""class"":""even""})courses.extend(courses_even)","s=requests.session()page=s.get(url)soup=BeautifulSoup(page.text,'html.parser')courses=soup.findAll(""tr"",{""class"":""odd""})courses_even=soup.findAll(""tr"",{""class"":""even""})courses.extend(courses_even)",
Pitt-CSC/PittAPI,81dba02bc4757d3ad75b4f64f30d20c21ac6c1f5,requests,urllib,"page = urlopen(url)
        soup = BeautifulSoup(page.read(), 'html.parser')","        page = s.get(url)
        soup = BeautifulSoup(page.text, 'html.parser')
",update,"page=urlopen(url)soup=BeautifulSoup(page.read(),'html.parser')","page=s.get(url)soup=BeautifulSoup(page.text,'html.parser')",pittAPI.py
Pitt-CSC/PittAPI,81dba02bc4757d3ad75b4f64f30d20c21ac6c1f5,requests,urllib,"if personurl.lower().startswith(""https://"") and hasattr(ssl, '_create_unverified_context'):
                    ct = ssl._create_unverified_context()
                    f = urlopen(personurl, context=ct)
                else:
                    f = urlopen(personurl)","if personurl.lower().startswith(""https://"") and hasattr(ssl, '_create_unverified_context'):
                    ct = ssl._create_unverified_context()
                    f = s.get(personurl, context=ct)
                else:
                    f = s.get(personurl)",update,"ifpersonurl.lower().startswith(""https://"")andhasattr(ssl,'_create_unverified_context'):ct=ssl._create_unverified_context()f=urlopen(personurl,context=ct)else:f=urlopen(personurl)","ifpersonurl.lower().startswith(""https://"")andhasattr(ssl,'_create_unverified_context'):ct=ssl._create_unverified_context()f=s.get(personurl,context=ct)else:f=s.get(personurl)",pittAPI.py
PsychoinformaticsLab/pliers,f345b457001b862bb13cba32bb8e40ad3f4d8398,requests,urllib,"tmpdir = tempfile.mkdtemp()
    _file = os.path.join(tmpdir, os.path.basename(url))
    urllib.urlretrieve(url, _file)","    tmpdir = tempfile.mkdtemp()
    _file = os.path.join(tmpdir, os.path.basename(url))
    url = 'http://www.example.com/image.jpg'
    r = requests.get(url)
    with open(_file, 'wb') as f:
        f.write(r.content)",update,"tmpdir=tempfile.mkdtemp()_file=os.path.join(tmpdir,os.path.basename(url))urllib.urlretrieve(url,_file)","tmpdir=tempfile.mkdtemp()_file=os.path.join(tmpdir,os.path.basename(url))url='http://www.example.com/image.jpg'r=requests.get(url)withopen(_file,'wb')asf:f.write(r.content)",featurex/datasets/text.py
PsychoinformaticsLab/pliers,f345b457001b862bb13cba32bb8e40ad3f4d8398,requests,urllib,"def test_dicts(self):
        """"""
        Check that all text dictionaries download successfully.
        """"""
        datasets = _load_datasets()
        for dataset in datasets.keys():
            try:
                data = fetch_dictionary(dataset, save=False)
            except:
                print(""Dataset failed: {0}"".format(dataset))
                data = None
                
                # Determine cause of error.
                try:
                    urllib2.urlopen(datasets[dataset][""url""])
                except urllib2.HTTPError, e:
                    print(""HTTP Error: {0}"".format(e.code))
                except urllib2.URLError, e:
                    print(""URL Error: {0}"".format(e.args))
            self.assertIsInstance(data, DataFrame)","def test_dicts_exist_at_url_and_initialize(self):
        """"""
        Check that all text dictionaries download successfully.
        """"""
        datasets = _load_datasets()
        for name, dataset in datasets.items():
            r = requests.head(dataset['url'])
            assert r.status_code == requests.codes.ok
            ## read_excel() is doing some weird things, so disable for the moment
            # data = fetch_dictionary(name, save=False)
            # assert isinstance(data.shape, tuple)",update,"deftest_dicts(self):""""""Checkthatalltextdictionariesdownloadsuccessfully.""""""datasets=_load_datasets()fordatasetindatasets.keys():try:data=fetch_dictionary(dataset,save=False)except:print(""Datasetfailed:{0}"".format(dataset))data=None#Determinecauseoferror.try:urllib2.urlopen(datasets[dataset][""url""])excepturllib2.HTTPError,e:print(""HTTPError:{0}"".format(e.code))excepturllib2.URLError,e:print(""URLError:{0}"".format(e.args))self.assertIsInstance(data,DataFrame)","deftest_dicts_exist_at_url_and_initialize(self):""""""Checkthatalltextdictionariesdownloadsuccessfully.""""""datasets=_load_datasets()forname,datasetindatasets.items():r=requests.head(dataset['url'])assertr.status_code==requests.codes.ok##read_excel()isdoingsomeweirdthings,sodisableforthemoment#data=fetch_dictionary(name,save=False)#assertisinstance(data.shape,tuple)",featurex/tests/test_datasets.py
RIPE-NCC/ripe-atlas-cousteau,462d1a4464ae2bb09a859fd9a1720de430d011d9,requests,urllib,,"http_methods = {
        ""GET"": requests.get,
        ""POST"": requests.post,
        ""DELETE"": requests.delete
    }",insert,,"http_methods={""GET"":requests.get,""POST"":requests.post,""DELETE"":requests.delete}",ripe/atlas/cousteau/request.py
RIPE-NCC/ripe-atlas-cousteau,462d1a4464ae2bb09a859fd9a1720de430d011d9,requests,urllib,"def get(self):
        """"""
        Makes the HTTP GET to the url.
        """"""
        self.build_url()
        req = urllib2.Request(self.url)
        req.add_header('Content-Type', 'application/json')
        req.add_header('Accept', 'application/json')
        req.add_header('User-Agent', self.http_agent)
        try:
            response = urllib2.urlopen(req)
        except urllib2.HTTPError as exc:
            log = {
                ""HTTP_MSG"": ""HTTP ERROR %d: %s"" % (exc.code, exc.msg),
                ""ADDITIONAL_MSG"": exc.read()
            }
            return False, log

        return True, json.load(response)","def get_headers(self):
        """"""Return header for the HTTP request.""""""
        return {
            ""User-Agent"": self.http_agent,
            ""Content-Type"": ""application/json"",
            ""Accept"": ""application/json""
        }

    def http_method(self, method):
        """"""
        Execute the given HTTP method and returns if it's success or not
        and the response as a string if not success and as python object after
        unjson if it's success.
        """"""
        return_json_methods = [""GET"", ""POST""]
        self.build_url()

        try:
            response = self.get_http_method(method)
            is_success = response.ok
            if is_success and method in return_json_methods:
                response_message = response.json()
            else:
                response_message = response.text
        except requests.exceptions.RequestException as exc:
            is_success = False
            response_message = exc.args

        return is_success, response_message

    def get_http_method(self, method):
        """"""Gets the http method that will be called from the requests library""""""
        return self.http_methods[method](self.url, **self.http_method_args)",update,"defget(self):""""""MakestheHTTPGETtotheurl.""""""self.build_url()req=urllib2.Request(self.url)req.add_header('Content-Type','application/json')req.add_header('Accept','application/json')req.add_header('User-Agent',self.http_agent)try:response=urllib2.urlopen(req)excepturllib2.HTTPErrorasexc:log={""HTTP_MSG"":""HTTPERROR%d:%s""%(exc.code,exc.msg),""ADDITIONAL_MSG"":exc.read()}returnFalse,logreturnTrue,json.load(response)","defget_headers(self):""""""ReturnheaderfortheHTTPrequest.""""""return{""User-Agent"":self.http_agent,""Content-Type"":""application/json"",""Accept"":""application/json""}defhttp_method(self,method):""""""ExecutethegivenHTTPmethodandreturnsifit'ssuccessornotandtheresponseasastringifnotsuccessandaspythonobjectafterunjsonifit'ssuccess.""""""return_json_methods=[""GET"",""POST""]self.build_url()try:response=self.get_http_method(method)is_success=response.okifis_successandmethodinreturn_json_methods:response_message=response.json()else:response_message=response.textexceptrequests.exceptions.RequestExceptionasexc:is_success=Falseresponse_message=exc.argsreturnis_success,response_messagedefget_http_method(self,method):""""""Getsthehttpmethodthatwillbecalledfromtherequestslibrary""""""returnself.http_methods[method](self.url,**self.http_method_args)",ripe/atlas/cousteau/request.py
RIPE-NCC/ripe-atlas-cousteau,462d1a4464ae2bb09a859fd9a1720de430d011d9,requests,urllib,"def build_url(self):
        """"""
        Builds the request's url combining server, url_path, key
        classes attributes.
        """"""
        if self.key:
            self.url = ""https://%s%s?key=%s"" % (
                self.server, self.url_path, self.key
            )
        else:
            self.url = ""https://%s%s"" % (self.server, self.url_path)","def build_url(self):
        """"""
        Builds the request's url combining server and url_path
        classes attributes.
        """"""
        self.url = ""https://{0}{1}"".format(self.server, self.url_path)

    def get(self, **url_params):
        """"""
        Makes the HTTP GET to the url.
        """"""
        if url_params:
            self.http_method_args[""params""].update(url_params)
        return self.http_method(""GET"")",update,"defbuild_url(self):""""""Buildstherequest'surlcombiningserver,url_path,keyclassesattributes.""""""ifself.key:self.url=""https://%s%s?key=%s""%(self.server,self.url_path,self.key)else:self.url=""https://%s%s""%(self.server,self.url_path)","defbuild_url(self):""""""Buildstherequest'surlcombiningserverandurl_pathclassesattributes.""""""self.url=""https://{0}{1}"".format(self.server,self.url_path)defget(self,**url_params):""""""MakestheHTTPGETtotheurl.""""""ifurl_params:self.http_method_args[""params""].update(url_params)returnself.http_method(""GET"")",ripe/atlas/cousteau/request.py
RIPE-NCC/ripe-atlas-cousteau,462d1a4464ae2bb09a859fd9a1720de430d011d9,requests,urllib,"def post(self):
        """"""
        Makes the HTTP POST to the url sending post_data.
        """"""
        self.build_url()
        self._construct_post_data()
        post_data = json.dumps(self.post_data)
        req = urllib2.Request(self.url)
        req.add_header('Content-Type', 'application/json')
        req.add_header('Accept', 'application/json')
        req.add_header('User-Agent', self.http_agent)
        try:
            response = urllib2.urlopen(req, post_data)
        except urllib2.HTTPError as exc:
            log = {
                ""HTTP_MSG"": ""HTTP ERROR %d: %s"" % (exc.code, exc.msg),
                ""ADDITIONAL_MSG"": exc.read()
            }
            return False, log

        return True, json.load(response)","def post(self):
        """"""
        Makes the HTTP POST to the url sending post_data.
        """"""

        self._construct_post_data()














        post_args = {""json"": self.post_data}
        self.http_method_args.update(post_args)

        return self.http_method(""POST"")",update,"defpost(self):""""""MakestheHTTPPOSTtotheurlsendingpost_data.""""""self.build_url()self._construct_post_data()post_data=json.dumps(self.post_data)req=urllib2.Request(self.url)req.add_header('Content-Type','application/json')req.add_header('Accept','application/json')req.add_header('User-Agent',self.http_agent)try:response=urllib2.urlopen(req,post_data)excepturllib2.HTTPErrorasexc:log={""HTTP_MSG"":""HTTPERROR%d:%s""%(exc.code,exc.msg),""ADDITIONAL_MSG"":exc.read()}returnFalse,logreturnTrue,json.load(response)","defpost(self):""""""MakestheHTTPPOSTtotheurlsendingpost_data.""""""self._construct_post_data()post_args={""json"":self.post_data}self.http_method_args.update(post_args)returnself.http_method(""POST"")",ripe/atlas/cousteau/request.py
RIPE-NCC/ripe-atlas-cousteau,462d1a4464ae2bb09a859fd9a1720de430d011d9,requests,urllib,"def delete(self):
        """"""
        Makes the HTTP DELETE to the url.
        """"""
        self.build_url()
        req = urllib2.Request(self.url)
        req.add_header('Content-Type', 'application/json')
        req.add_header('Accept', 'application/json')
        req.add_header('User-Agent', self.http_agent)
        req.get_method = lambda: 'DELETE'
        try:
            response = urllib2.urlopen(req)
        except urllib2.HTTPError as exc:
            log = {
                ""HTTP_MSG"": ""HTTP ERROR %d: %s"" % (exc.code, exc.msg),
                ""ADDITIONAL_MSG"": exc.read()
            }
            return False, log

        return True, response","def delete(self):
        """"""
        Makes the HTTP DELETE to the url.
        """"""
        return self.http_method(""DELETE"")",update,"defdelete(self):""""""MakestheHTTPDELETEtotheurl.""""""self.build_url()req=urllib2.Request(self.url)req.add_header('Content-Type','application/json')req.add_header('Accept','application/json')req.add_header('User-Agent',self.http_agent)req.get_method=lambda:'DELETE'try:response=urllib2.urlopen(req)excepturllib2.HTTPErrorasexc:log={""HTTP_MSG"":""HTTPERROR%d:%s""%(exc.code,exc.msg),""ADDITIONAL_MSG"":exc.read()}returnFalse,logreturnTrue,response","defdelete(self):""""""MakestheHTTPDELETEtotheurl.""""""returnself.http_method(""DELETE"")",ripe/atlas/cousteau/request.py
RIPE-NCC/ripe-atlas-cousteau,462d1a4464ae2bb09a859fd9a1720de430d011d9,requests,urllib,"class TestAtlasRequest(unittest.TestCase):
    def setUp(self):
        self.create_source = AtlasSource(
            **{""type"": ""area"", ""value"": ""WW"", ""requested"": 3}
        )
        self.change_source = AtlasChangeSource(
            **{""value"": ""3,4"", ""requested"": 3, ""action"": ""add""}
        )
        self.measurement = Ping(**{
            ""target"": ""testing"", ""af"": 6,
            ""description"": ""testing""
        })","class TestAtlasRequest(unittest.TestCase):
    def setUp(self):
        self.request = AtlasRequest(**{
            ""key"": ""blaaaa"",
            ""server"": ""test"",
            ""url_path"": ""testing""





        })",update,"classTestAtlasRequest(unittest.TestCase):defsetUp(self):self.create_source=AtlasSource(**{""type"":""area"",""value"":""WW"",""requested"":3})self.change_source=AtlasChangeSource(**{""value"":""3,4"",""requested"":3,""action"":""add""})self.measurement=Ping(**{""target"":""testing"",""af"":6,""description"":""testing""})","classTestAtlasRequest(unittest.TestCase):defsetUp(self):self.request=AtlasRequest(**{""key"":""blaaaa"",""server"":""test"",""url_path"":""testing""})",tests/test_requests.py
RIPE-NCC/ripe-atlas-cousteau,462d1a4464ae2bb09a859fd9a1720de430d011d9,requests,urllib,,"def test_headers(self):
        """"""Tests header fields of the request.""""""
        expected_output = {
            ""User-Agent"": ""RIPE ATLAS Cousteau v{0}"".format(__version__),
            ""Content-Type"": ""application/json"",
            ""Accept"": ""application/json""
        }
        self.assertEqual(expected_output, self.request.get_headers())

    def test_http_method_args(self):
        """"""Tests initial args that will be passed later to HTTP method.""""""
        expected_output = {
            ""params"": {""key"": ""blaaaa""},
            ""headers"": {
                ""User-Agent"": ""RIPE ATLAS Cousteau v{0}"".format(__version__),
                ""Content-Type"": ""application/json"",
                ""Accept"": ""application/json""
            }
        }
        self.assertEqual(expected_output, self.request.http_method_args)

    def test_get_method(self):
        """"""Tests GET reuest method""""""
        extra_params = {""bull"": ""shit"", ""cow"": ""shit"", ""horse"": ""shit""}
        expected_args = {
            ""params"": {
                ""key"": ""blaaaa"", ""bull"": ""shit"",
                ""cow"": ""shit"", ""horse"": ""shit""
            },
            ""headers"": {
                ""User-Agent"": ""RIPE ATLAS Cousteau v{0}"".format(__version__),
                ""Content-Type"": ""application/json"",
                ""Accept"": ""application/json""
            }
        }
        with mock.patch('ripe.atlas.cousteau.request.AtlasRequest.http_method') as mock_get:
            mock_get.return_value = True
            self.request.get(**extra_params)
            self.assertEqual(self.request.http_method_args, expected_args)",insert,,"deftest_headers(self):""""""Testsheaderfieldsoftherequest.""""""expected_output={""User-Agent"":""RIPEATLASCousteauv{0}"".format(__version__),""Content-Type"":""application/json"",""Accept"":""application/json""}self.assertEqual(expected_output,self.request.get_headers())deftest_http_method_args(self):""""""TestsinitialargsthatwillbepassedlatertoHTTPmethod.""""""expected_output={""params"":{""key"":""blaaaa""},""headers"":{""User-Agent"":""RIPEATLASCousteauv{0}"".format(__version__),""Content-Type"":""application/json"",""Accept"":""application/json""}}self.assertEqual(expected_output,self.request.http_method_args)deftest_get_method(self):""""""TestsGETreuestmethod""""""extra_params={""bull"":""shit"",""cow"":""shit"",""horse"":""shit""}expected_args={""params"":{""key"":""blaaaa"",""bull"":""shit"",""cow"":""shit"",""horse"":""shit""},""headers"":{""User-Agent"":""RIPEATLASCousteauv{0}"".format(__version__),""Content-Type"":""application/json"",""Accept"":""application/json""}}withmock.patch('ripe.atlas.cousteau.request.AtlasRequest.http_method')asmock_get:mock_get.return_value=Trueself.request.get(**extra_params)self.assertEqual(self.request.http_method_args,expected_args)",tests/test_requests.py
RIPE-NCC/ripe-atlas-cousteau,462d1a4464ae2bb09a859fd9a1720de430d011d9,requests,urllib,"    def test_url_build(self):
        request = AtlasCreateRequest(**{
            ""measurements"": [self.measurement], ""sources"": [self.create_source]
        })
        request.build_url()
        self.assertNotEquals(getattr(request, ""url"", None), None)","    def test_url_build(self):
        """"""Tests build of the url of the request.""""""
        self.request.build_url()
        self.assertEqual(self.request.url, ""https://testtesting"")

",update,"deftest_url_build(self):request=AtlasCreateRequest(**{""measurements"":[self.measurement],""sources"":[self.create_source]})request.build_url()self.assertNotEquals(getattr(request,""url"",None),None)","deftest_url_build(self):""""""Testsbuildoftheurloftherequest.""""""self.request.build_url()self.assertEqual(self.request.url,""https://testtesting"")",tests/test_requests.py
RomelTorres/alpha_vantage,1280ac545923dcefde7656408b96f22b97a91530,requests,urllib,"if sys.version_info.major == 3:
            response = urllib.request.urlopen(url)
        else:
            response = urllib.urlopen(url)
        url_response = response.read()","        # if sys.version_info.major == 3:
        #     response = urllib.request.urlopen(url)
        # else:
        #     response = urllib.urlopen(url)
        response = requests.get(url)
        #url_response = response.read()",update,ifsys.version_info.major==3:response=urllib.request.urlopen(url)else:response=urllib.urlopen(url)url_response=response.read(),#ifsys.version_info.major==3:#response=urllib.request.urlopen(url)#else:#response=urllib.urlopen(url)response=requests.get(url)#url_response=response.read(),alpha_vantage/alphavantage.py
RomelTorres/alpha_vantage,1280ac545923dcefde7656408b96f22b97a91530,requests,urllib,"        if 'json' in self.output_format.lower() or 'pandas' in \
                self.output_format.lower():
            json_response = loads(url_response)
            if not json_response:
                raise ValueError(
                    'Error getting data from the api, no return was given.')
            elif ""Error Message"" in json_response:","if 'json' in self.output_format.lower() or 'pandas' in \
                self.output_format.lower():
            json_response = response.json()
            if ""Error Message"" in json_response:",update,"if'json'inself.output_format.lower()or'pandas'in\self.output_format.lower():json_response=loads(url_response)ifnotjson_response:raiseValueError('Errorgettingdatafromtheapi,noreturnwasgiven.')elif""ErrorMessage""injson_response:","if'json'inself.output_format.lower()or'pandas'in\self.output_format.lower():json_response=response.json()if""ErrorMessage""injson_response:",alpha_vantage/alphavantage.py
RomelTorres/alpha_vantage,1280ac545923dcefde7656408b96f22b97a91530,requests,urllib,csv_response = csv.reader(url_response.splitlines()),csv_response = csv.reader(response.text.splitlines()),update,csv_response=csv.reader(url_response.splitlines()),csv_response=csv.reader(response.text.splitlines()),alpha_vantage/alphavantage.py
SamSchott/maestral,89dd92b40e1b86b809cd90998287d642d3e08355,requests,urllib,"    try:
        if hasattr(ssl, '_create_unverified_context'):
            context = ssl._create_unverified_context()
            page = urlopen(API_URL, context=context)
        else:
            page = urlopen(API_URL)","    try:
        r = requests.get(GITHUB_RELEAES_API)
        data = r.json()

        releases = [item['tag_name'].replace('v', '') for item in data]
        release_notes = ['### ' + item['tag_name'] + '\n\n' + item['body']
                         for item in data]",update,"try:ifhasattr(ssl,'_create_unverified_context'):context=ssl._create_unverified_context()page=urlopen(API_URL,context=context)else:page=urlopen(API_URL)","try:r=requests.get(GITHUB_RELEAES_API)data=r.json()releases=[item['tag_name'].replace('v','')foritemindata]release_notes=['###'+item['tag_name']+'\n\n'+item['body']foritemindata]",maestral/utils/updates.py
SamSchott/maestral,89dd92b40e1b86b809cd90998287d642d3e08355,requests,urllib,"try:
            data = page.read()

            if not isinstance(data, str):
                data = data.decode()
            data = json.loads(data)

            releases = [item['tag_name'].replace('v', '') for item in data]
            release_notes = ['### ' + item['tag_name'] + '\n\n' + item['body']
                             for item in data]

            try:
                current_release_index = releases.index(current_version)
            except ValueError:
                # if current release cannot be found online, just
                # show release notes from newest release w/o history
                current_release_index = 1

            update_release_notes = release_notes[0:current_release_index]
            update_release_notes = '\n'.join(update_release_notes)

            new_version = get_newer_version(current_version, releases)
        except Exception:
            error_msg = 'Unable to retrieve information.'
    except HTTPError:
        error_msg = 'Unable to retrieve information.'","
        try:
            current_release_index = releases.index(current_version)
        except ValueError:
            # if current release cannot be found online, just
            # show release notes from newest release w/o history
            current_release_index = 1

        update_release_notes = release_notes[0:current_release_index]
        update_release_notes = '\n'.join(update_release_notes)

        new_version = get_newer_version(current_version, releases)

    except requests.exceptions.HTTPError:",update,"try:data=page.read()ifnotisinstance(data,str):data=data.decode()data=json.loads(data)releases=[item['tag_name'].replace('v','')foritemindata]release_notes=['###'+item['tag_name']+'\n\n'+item['body']foritemindata]try:current_release_index=releases.index(current_version)exceptValueError:#ifcurrentreleasecannotbefoundonline,just#showreleasenotesfromnewestreleasew/ohistorycurrent_release_index=1update_release_notes=release_notes[0:current_release_index]update_release_notes='\n'.join(update_release_notes)new_version=get_newer_version(current_version,releases)exceptException:error_msg='Unabletoretrieveinformation.'exceptHTTPError:error_msg='Unabletoretrieveinformation.'","try:current_release_index=releases.index(current_version)exceptValueError:#ifcurrentreleasecannotbefoundonline,just#showreleasenotesfromnewestreleasew/ohistorycurrent_release_index=1update_release_notes=release_notes[0:current_release_index]update_release_notes='\n'.join(update_release_notes)new_version=get_newer_version(current_version,releases)exceptrequests.exceptions.HTTPError:",maestral/utils/updates.py
SamSchott/maestral,89dd92b40e1b86b809cd90998287d642d3e08355,requests,urllib,except URLError:,except CONNECTION_ERRORS:,update,exceptURLError:,exceptCONNECTION_ERRORS:,maestral/utils/updates.py
SickGear/SickGear,b371c835439dac5182d04cc39df914d24175f9f4,requests,urllib,from urllib import urlencode,from requests.compat import urlencode,update,fromurllibimporturlencode,fromrequests.compatimporturlencode,
SpiderOak/netkes,d68d0de5f44dc93ec04e0a105c592d038f890f8d,requests,urllib,"def update_enterprise_settings(self, settings):
        try:
            return self.client.post_json('partner/settings', settings)
        except urllib2.HTTPError, err:
            if err.code == 400:
                raise self.BadParams()
            raise","def update_enterprise_settings(self, settings):
        try:
            return self.client.post_json('partner/settings', settings)
        except requests.exceptions.HTTPError, err:
            if err.response.status_code == 400:
                raise self.BadParams()
            raise",update,"defupdate_enterprise_settings(self,settings):try:returnself.client.post_json('partner/settings',settings)excepturllib2.HTTPError,err:iferr.code==400:raiseself.BadParams()raise","defupdate_enterprise_settings(self,settings):try:returnself.client.post_json('partner/settings',settings)exceptrequests.exceptions.HTTPError,err:iferr.response.status_code==400:raiseself.BadParams()raise",netkes/account_mgr/accounts_api.py
SpiderOak/netkes,d68d0de5f44dc93ec04e0a105c592d038f890f8d,requests,urllib," def update_enterprise_password(self, new_password):
        try:
            return self.client.post_json('partner/password', new_password)
        except urllib2.HTTPError, err:
            if err.code == 400:
                raise self.BadParams()
            raise","def update_enterprise_password(self, new_password):
        try:
            return self.client.post_json('partner/password', new_password)
        except requests.exceptions.HTTPError, err:
            if err.response.status_code == 400:
                raise self.BadParams()
            raise",update,"defupdate_enterprise_password(self,new_password):try:returnself.client.post_json('partner/password',new_password)excepturllib2.HTTPError,err:iferr.code==400:raiseself.BadParams()raise","defupdate_enterprise_password(self,new_password):try:returnself.client.post_json('partner/password',new_password)exceptrequests.exceptions.HTTPError,err:iferr.response.status_code==400:raiseself.BadParams()raise",netkes/account_mgr/accounts_api.py
SpiderOak/netkes,d68d0de5f44dc93ec04e0a105c592d038f890f8d,requests,urllib,"def create_policy(self, policy_info):
        try:
            return self.client.post_json('devicepolicies/', policy_info)
        except urllib2.HTTPError, err:
            if err.code == 400:
                raise self.BadParams()
            elif err.code == 409:
                data = json.loads(err.read())
                if 'inherits_from' in data['conflicts']:
                    raise self.BadPolicy()
            raise","def create_policy(self, policy_info):
        try:
            return self.client.post_json('devicepolicies/', policy_info)
        except requests.exceptions.HTTPError, err:
            if err.response.status_code == 400:
                raise self.BadParams()
            elif err.response.status_code == 409:
                data = err.response.json()
                if 'inherits_from' in data['conflicts']:
                    raise self.BadPolicy()
            raise",update,"defcreate_policy(self,policy_info):try:returnself.client.post_json('devicepolicies/',policy_info)excepturllib2.HTTPError,err:iferr.code==400:raiseself.BadParams()eliferr.code==409:data=json.loads(err.read())if'inherits_from'indata['conflicts']:raiseself.BadPolicy()raise","defcreate_policy(self,policy_info):try:returnself.client.post_json('devicepolicies/',policy_info)exceptrequests.exceptions.HTTPError,err:iferr.response.status_code==400:raiseself.BadParams()eliferr.response.status_code==409:data=err.response.json()if'inherits_from'indata['conflicts']:raiseself.BadPolicy()raise",netkes/account_mgr/accounts_api.py
SpiderOak/netkes,d68d0de5f44dc93ec04e0a105c592d038f890f8d,requests,urllib,"def get_policy(self, policy_id):
        try:
            return self.client.get_json('devicepolicies/%d' % (policy_id,))
        except urllib2.HTTPError, err:
            if err.code == 404:
                raise self.NotFound()
            raise","    def get_policy(self, policy_id):
        try:
            return self.client.get_json('devicepolicies/%d' % (policy_id,))
        except requests.exceptions.HTTPError, err:
            if err.response.status_code == 404:
                raise self.NotFound()
            raise",update,"defget_policy(self,policy_id):try:returnself.client.get_json('devicepolicies/%d'%(policy_id,))excepturllib2.HTTPError,err:iferr.code==404:raiseself.NotFound()raise","defget_policy(self,policy_id):try:returnself.client.get_json('devicepolicies/%d'%(policy_id,))exceptrequests.exceptions.HTTPError,err:iferr.response.status_code==404:raiseself.NotFound()raise",netkes/account_mgr/accounts_api.py
SpiderOak/netkes,d68d0de5f44dc93ec04e0a105c592d038f890f8d,requests,urllib,"    def edit_policy(self, policy_id, policy_info):
        try:
            self.client.post_json('devicepolicies/%d' % (policy_id,), policy_info)
        except urllib2.HTTPError, err:
            if err.code == 404:
                raise self.NotFound()
            elif err.code == 400:
                raise self.BadParams()
            elif err.code == 409:
                data = json.loads(err.read())
                if 'inherits_from' in data['conflicts']:
                    raise self.BadPolicy()
            raise","
    def edit_policy(self, policy_id, policy_info):
        try:
            self.client.post_json('devicepolicies/%d' % (policy_id,), policy_info)
        except requests.exceptions.HTTPError, err:
            if err.response.status_code == 404:
                raise self.NotFound()
            elif err.response.status_code == 400:
                raise self.BadParams()
            elif err.response.status_code == 409:
                data = err.response.json()
                if 'inherits_from' in data['conflicts']:
                    raise self.BadPolicy()
            raise",update,"defedit_policy(self,policy_id,policy_info):try:self.client.post_json('devicepolicies/%d'%(policy_id,),policy_info)excepturllib2.HTTPError,err:iferr.code==404:raiseself.NotFound()eliferr.code==400:raiseself.BadParams()eliferr.code==409:data=json.loads(err.read())if'inherits_from'indata['conflicts']:raiseself.BadPolicy()raise","defedit_policy(self,policy_id,policy_info):try:self.client.post_json('devicepolicies/%d'%(policy_id,),policy_info)exceptrequests.exceptions.HTTPError,err:iferr.response.status_code==404:raiseself.NotFound()eliferr.response.status_code==400:raiseself.BadParams()eliferr.response.status_code==409:data=err.response.json()if'inherits_from'indata['conflicts']:raiseself.BadPolicy()raise",netkes/account_mgr/accounts_api.py
SpiderOak/netkes,d68d0de5f44dc93ec04e0a105c592d038f890f8d,requests,urllib,"
    def delete_policy(self, policy_id):
        try:
            self.client.delete('devicepolicies/%d' % (policy_id,))
        except urllib2.HTTPError, err:
            if err.code == 404:
                raise self.NotFound()
            elif err.code == 409:
                data = json.loads(err.read())
                if 'policy_id' in data['conflicts']:
                    raise self.PolicyInUse()
            raise","    def delete_policy(self, policy_id):
        try:
            self.client.delete('devicepolicies/%d' % (policy_id,))
        except requests.exceptions.HTTPError, err:
            if err.response.status_code == 404:
                raise self.NotFound()
            elif err.response.status_code == 409:
                data = err.response.json()
                if 'policy_id' in data['conflicts']:
                    raise self.PolicyInUse()
            raise",update,"defdelete_policy(self,policy_id):try:self.client.delete('devicepolicies/%d'%(policy_id,))excepturllib2.HTTPError,err:iferr.code==404:raiseself.NotFound()eliferr.code==409:data=json.loads(err.read())if'policy_id'indata['conflicts']:raiseself.PolicyInUse()raise","defdelete_policy(self,policy_id):try:self.client.delete('devicepolicies/%d'%(policy_id,))exceptrequests.exceptions.HTTPError,err:iferr.response.status_code==404:raiseself.NotFound()eliferr.response.status_code==409:data=err.response.json()if'policy_id'indata['conflicts']:raiseself.PolicyInUse()raise",netkes/account_mgr/accounts_api.py
SpiderOak/netkes,d68d0de5f44dc93ec04e0a105c592d038f890f8d,requests,urllib,"        try:
            resp = self.client.post_json_raw_response(
                'groups/', group_info)
        except urllib2.HTTPError, err:
            if err.code == 400:
                raise self.BadParams()
            elif err.code == 409:
                data = json.loads(err.read())
                if 'name' in data['conflicts']:
                    raise self.DuplicateGroupName()
                elif 'plan_id' in data['conflicts']:
                    raise self.BadPlan()
            raise
        return int(resp.info()['location'].rsplit('/', 1)[-1])","        try:
            resp = self.client.post_json_raw_response(
                'groups/', group_info)
        except requests.exceptions.HTTPError, err:
            if err.response.status_code == 400:
                raise self.BadParams()
            elif err.response.status_code == 409:
                data = err.response.json()
                if 'name' in data['conflicts']:
                    raise self.DuplicateGroupName()
                elif 'plan_id' in data['conflicts']:
                    raise self.BadPlan()
            raise
        return int(resp.headers['Location'].rsplit('/', 1)[-1])",update,"try:resp=self.client.post_json_raw_response('groups/',group_info)excepturllib2.HTTPError,err:iferr.code==400:raiseself.BadParams()eliferr.code==409:data=json.loads(err.read())if'name'indata['conflicts']:raiseself.DuplicateGroupName()elif'plan_id'indata['conflicts']:raiseself.BadPlan()raisereturnint(resp.info()['location'].rsplit('/',1)[-1])","try:resp=self.client.post_json_raw_response('groups/',group_info)exceptrequests.exceptions.HTTPError,err:iferr.response.status_code==400:raiseself.BadParams()eliferr.response.status_code==409:data=err.response.json()if'name'indata['conflicts']:raiseself.DuplicateGroupName()elif'plan_id'indata['conflicts']:raiseself.BadPlan()raisereturnint(resp.headers['Location'].rsplit('/',1)[-1])",netkes/account_mgr/accounts_api.py
SpiderOak/netkes,d68d0de5f44dc93ec04e0a105c592d038f890f8d,requests,urllib,"    def get_group(self, group_id):
        try:
            return self.client.get_json('groups/%d' % (group_id,))
        except urllib2.HTTPError, err:
            if err.code == 404:
                raise self.NotFound()
            raise","    def get_group(self, group_id):
        try:
            return self.client.get_json('groups/%d' % (group_id,))
        except requests.exceptions.HTTPError, err:
            if err.response.status_code == 404:
                raise self.NotFound()
            raise",update,"defget_group(self,group_id):try:returnself.client.get_json('groups/%d'%(group_id,))excepturllib2.HTTPError,err:iferr.code==404:raiseself.NotFound()raise","defget_group(self,group_id):try:returnself.client.get_json('groups/%d'%(group_id,))exceptrequests.exceptions.HTTPError,err:iferr.response.status_code==404:raiseself.NotFound()raise",netkes/account_mgr/accounts_api.py
SpiderOak/netkes,d68d0de5f44dc93ec04e0a105c592d038f890f8d,requests,urllib,"    def edit_group(self, group_id, group_info):
        try:
            self.client.post_json('groups/%d' % (group_id,), group_info)
        except urllib2.HTTPError, err:
            if err.code == 404:
                raise self.NotFound()
            elif err.code == 400:
                raise self.BadParams()
            elif err.code == 409:
                data = json.loads(err.read())
                if 'name' in data['conflicts']:
                    raise self.DuplicateGroupName()
                elif 'plan_id' in data['conflicts']:","
    def edit_group(self, group_id, group_info):
        try:
            self.client.post_json('groups/%d' % (group_id,), group_info)
        except requests.exceptions.HTTPError, err:
            if err.response.status_code == 404:
                raise self.NotFound()
            elif err.response.status_code == 400:
                raise self.BadParams()
            elif err.response.status_code == 409:
                data = err.response.json()
                if 'name' in data['conflicts']:
                    raise self.DuplicateGroupName()
                elif 'plan_id' in data['conflicts']:",update,"defedit_group(self,group_id,group_info):try:self.client.post_json('groups/%d'%(group_id,),group_info)excepturllib2.HTTPError,err:iferr.code==404:raiseself.NotFound()eliferr.code==400:raiseself.BadParams()eliferr.code==409:data=json.loads(err.read())if'name'indata['conflicts']:raiseself.DuplicateGroupName()elif'plan_id'indata['conflicts']:","defedit_group(self,group_id,group_info):try:self.client.post_json('groups/%d'%(group_id,),group_info)exceptrequests.exceptions.HTTPError,err:iferr.response.status_code==404:raiseself.NotFound()eliferr.response.status_code==400:raiseself.BadParams()eliferr.response.status_code==409:data=err.response.json()if'name'indata['conflicts']:raiseself.DuplicateGroupName()elif'plan_id'indata['conflicts']:",netkes/account_mgr/accounts_api.py
SpiderOak/netkes,d68d0de5f44dc93ec04e0a105c592d038f890f8d,requests,urllib,"        except urllib2.HTTPError, err:
            if err.code == 404:
                raise self.NotFound()
            raise","                self.client.delete('groups/%d' % (group_id,))
        except requests.exceptions.HTTPError, err:
            if err.response.status_code == 404:
                raise self.NotFound()
            raise",update,"excepturllib2.HTTPError,err:iferr.code==404:raiseself.NotFound()raise","self.client.delete('groups/%d'%(group_id,))exceptrequests.exceptions.HTTPError,err:iferr.response.status_code==404:raiseself.NotFound()raise",netkes/account_mgr/accounts_api.py
SpiderOak/netkes,d68d0de5f44dc93ec04e0a105c592d038f890f8d,requests,urllib,"   def create_user(self, user_info):
        try:
            return self.client.post_json('users/', user_info)
        except urllib2.HTTPError, err:
            if err.code == 400:
                raise self.BadParams()
            if err.code == 402:
                raise self.PaymentRequired()
            elif err.code == 409:
                data = json.loads(err.read())
                if 'username' in data['conflicts']:
                    raise self.DuplicateUsername()
                if 'email' in data['conflicts']:","
    def create_user(self, user_info):
        try:
            return self.client.post_json('users/', user_info)
        except requests.exceptions.HTTPError, err:
            if err.response.status_code == 400:
                raise self.BadParams()
            if err.response.status_code == 402:
                raise self.PaymentRequired()
            elif err.response.status_code == 409:
                data = err.response.json()
                if 'username' in data['conflicts']:
                    raise self.DuplicateUsername()
                if 'email' in data['conflicts']:",update,"defcreate_user(self,user_info):try:returnself.client.post_json('users/',user_info)excepturllib2.HTTPError,err:iferr.code==400:raiseself.BadParams()iferr.code==402:raiseself.PaymentRequired()eliferr.code==409:data=json.loads(err.read())if'username'indata['conflicts']:raiseself.DuplicateUsername()if'email'indata['conflicts']:","defcreate_user(self,user_info):try:returnself.client.post_json('users/',user_info)exceptrequests.exceptions.HTTPError,err:iferr.response.status_code==400:raiseself.BadParams()iferr.response.status_code==402:raiseself.PaymentRequired()eliferr.response.status_code==409:data=err.response.json()if'username'indata['conflicts']:raiseself.DuplicateUsername()if'email'indata['conflicts']:",netkes/account_mgr/accounts_api.py
SpiderOak/netkes,d68d0de5f44dc93ec04e0a105c592d038f890f8d,requests,urllib,"        try:
            return self.client.get_json(
                'users/%s' % (username_or_email,))
        except urllib2.HTTPError, err:
            if err.code == 404:
                raise self.NotFound()
            raise","        try:
            return self.client.get_json(
                'users/%s' % (username_or_email,))
        except requests.exceptions.HTTPError, err:
            if err.response.status_code == 404:
                raise self.NotFound()
            raise",update,"try:returnself.client.get_json('users/%s'%(username_or_email,))excepturllib2.HTTPError,err:iferr.code==404:raiseself.NotFound()raise","try:returnself.client.get_json('users/%s'%(username_or_email,))exceptrequests.exceptions.HTTPError,err:iferr.response.status_code==404:raiseself.NotFound()raise",netkes/account_mgr/accounts_api.py
SpiderOak/netkes,d68d0de5f44dc93ec04e0a105c592d038f890f8d,requests,urllib,"    def list_devices(self, username_or_email):
        try:
            return self.client.get_json(
                'users/%s/devices' % (username_or_email,))
        except urllib2.HTTPError, err:
            if err.code == 404:
                raise self.NotFound()
            raise","    def list_devices(self, username_or_email):
        try:
            return self.client.get_json(
                'users/%s/devices' % (username_or_email,))
        except requests.exceptions.HTTPError, err:
            if err.response.status_code == 404:
                raise self.NotFound()
            raise",update,"deflist_devices(self,username_or_email):try:returnself.client.get_json('users/%s/devices'%(username_or_email,))excepturllib2.HTTPError,err:iferr.code==404:raiseself.NotFound()raise","deflist_devices(self,username_or_email):try:returnself.client.get_json('users/%s/devices'%(username_or_email,))exceptrequests.exceptions.HTTPError,err:iferr.response.status_code==404:raiseself.NotFound()raise",netkes/account_mgr/accounts_api.py
SpiderOak/netkes,d68d0de5f44dc93ec04e0a105c592d038f890f8d,requests,urllib,"        try:
            return self.client.get_json(
                'users/%s/shares/' % (username_or_email,))
        except urllib2.HTTPError, err:
            if err.code == 404:
                raise self.NotFound()
            raise","        try:
            return self.client.get_json(
                'users/%s/shares/' % (username_or_email,))
        except requests.exceptions.HTTPError, err:
            if err.response.status_code == 404:
                raise self.NotFound()
            raise",update,"try:returnself.client.get_json('users/%s/shares/'%(username_or_email,))excepturllib2.HTTPError,err:iferr.code==404:raiseself.NotFound()raise","try:returnself.client.get_json('users/%s/shares/'%(username_or_email,))exceptrequests.exceptions.HTTPError,err:iferr.response.status_code==404:raiseself.NotFound()raise",netkes/account_mgr/accounts_api.py
SpiderOak/netkes,d68d0de5f44dc93ec04e0a105c592d038f890f8d,requests,urllib,"    def get_share(self, username_or_email, room_key):
        try:
            return self.client.get_json(
                'users/%s/shares/%s' % (username_or_email, room_key))
        except urllib2.HTTPError, err:
            if err.code == 404:
                raise self.NotFound()
            raise","   def get_share(self, username_or_email, room_key):
        try:
            return self.client.get_json(
                'users/%s/shares/%s' % (username_or_email, room_key))
        except requests.exceptions.HTTPError, err:
            if err.response.status_code == 404:
                raise self.NotFound()
            raise",update,"defget_share(self,username_or_email,room_key):try:returnself.client.get_json('users/%s/shares/%s'%(username_or_email,room_key))excepturllib2.HTTPError,err:iferr.code==404:raiseself.NotFound()raise","defget_share(self,username_or_email,room_key):try:returnself.client.get_json('users/%s/shares/%s'%(username_or_email,room_key))exceptrequests.exceptions.HTTPError,err:iferr.response.status_code==404:raiseself.NotFound()raise",netkes/account_mgr/accounts_api.py
SpiderOak/netkes,d68d0de5f44dc93ec04e0a105c592d038f890f8d,requests,urllib,"        except urllib2.HTTPError, err:
            if err.code == 404:
                raise self.NotFound()
            raise","        except requests.exceptions.HTTPError, err:
            if err.response.status_code == 404:
                raise self.NotFound()
            raise",update,"excepturllib2.HTTPError,err:iferr.code==404:raiseself.NotFound()raise","exceptrequests.exceptions.HTTPError,err:iferr.response.status_code==404:raiseself.NotFound()raise",netkes/account_mgr/accounts_api.py
SpiderOak/netkes,d68d0de5f44dc93ec04e0a105c592d038f890f8d,requests,urllib,"        except urllib2.HTTPError, err:
            if err.code == 404:
                raise self.NotFound()
            elif err.code == 400:
                raise self.BadParams()
            elif err.code == 402:
                raise self.QuotaExceeded()
            elif err.code == 409:
                data = json.loads(err.read())","        except requests.exceptions.HTTPError, err:
            if err.response.status_code == 404:
                raise self.NotFound()
            elif err.response.status_code == 400:
                raise self.BadParams()
            elif err.response.status_code == 402:
                raise self.QuotaExceeded()
            elif err.response.status_code == 409:
                data = err.response.json()",update,"excepturllib2.HTTPError,err:iferr.code==404:raiseself.NotFound()eliferr.code==400:raiseself.BadParams()eliferr.code==402:raiseself.QuotaExceeded()eliferr.code==409:data=json.loads(err.read())","exceptrequests.exceptions.HTTPError,err:iferr.response.status_code==404:raiseself.NotFound()eliferr.response.status_code==400:raiseself.BadParams()eliferr.response.status_code==402:raiseself.QuotaExceeded()eliferr.response.status_code==409:data=err.response.json()",netkes/account_mgr/accounts_api.py
SpiderOak/netkes,d68d0de5f44dc93ec04e0a105c592d038f890f8d,requests,urllib,"    def delete_user(self, username_or_email):
        try:
            self.client.delete('users/%s' % (username_or_email,))
        except urllib2.HTTPError, err:
            if err.code == 404:
                raise self.NotFound()
            raise

    def send_activation_email(self, username_or_email, data={}):
        try:
            self.client.post_json('users/%s?action=sendactivationemail' % (
                username_or_email,), data)
        except urllib2.HTTPError, err:
            if err.code == 404:
                raise self.NotFound()
            elif err.code == 409:
                raise self.EmailNotSent()
            raise","   def delete_user(self, username_or_email):
        try:
            self.client.delete('users/%s' % (username_or_email,))
        except requests.exceptions.HTTPError, err:
            if err.response.status_code == 404:
                raise self.NotFound()
            raise

    def send_activation_email(self, username_or_email, data={}):
        try:
            self.client.post_json('users/%s?action=sendactivationemail' % (
                username_or_email,), data)
        except requests.exceptions.HTTPError, err:
            if err.response.status_code == 404:
                raise self.NotFound()
            elif err.response.status_code == 409:
                raise self.EmailNotSent()
            raise",update,"defdelete_user(self,username_or_email):try:self.client.delete('users/%s'%(username_or_email,))excepturllib2.HTTPError,err:iferr.code==404:raiseself.NotFound()raisedefsend_activation_email(self,username_or_email,data={}):try:self.client.post_json('users/%s?action=sendactivationemail'%(username_or_email,),data)excepturllib2.HTTPError,err:iferr.code==404:raiseself.NotFound()eliferr.code==409:raiseself.EmailNotSent()raise","defdelete_user(self,username_or_email):try:self.client.delete('users/%s'%(username_or_email,))exceptrequests.exceptions.HTTPError,err:iferr.response.status_code==404:raiseself.NotFound()raisedefsend_activation_email(self,username_or_email,data={}):try:self.client.post_json('users/%s?action=sendactivationemail'%(username_or_email,),data)exceptrequests.exceptions.HTTPError,err:iferr.response.status_code==404:raiseself.NotFound()eliferr.response.status_code==409:raiseself.EmailNotSent()raise",netkes/account_mgr/accounts_api.py
SpiderOak/netkes,d68d0de5f44dc93ec04e0a105c592d038f890f8d,requests,urllib,"_DEFAULT_HANDLERS = [
    urllib2.ProxyHandler,
    urllib2.HTTPDefaultErrorHandler,
    urllib2.HTTPRedirectHandler,
    urllib2.HTTPErrorProcessor,
    urllib2.HTTPHandler,
]


def _make_opener(url):
    opener = urllib2.OpenerDirector()
    for handler_class in _DEFAULT_HANDLERS:
        opener.add_handler(handler_class())
    opener.add_handler(VerifiedHTTPSHandler())
    return opener


class RequestWithMethod(urllib2.Request):
    _method = None

    def set_method(self, method):
        self._method = method

    def get_method(self):
        return self._method or urllib2.Request.get_method(self)",,delete,"_DEFAULT_HANDLERS=[urllib2.ProxyHandler,urllib2.HTTPDefaultErrorHandler,urllib2.HTTPRedirectHandler,urllib2.HTTPErrorProcessor,urllib2.HTTPHandler,]def_make_opener(url):opener=urllib2.OpenerDirector()forhandler_classin_DEFAULT_HANDLERS:opener.add_handler(handler_class())opener.add_handler(VerifiedHTTPSHandler())returnopenerclassRequestWithMethod(urllib2.Request):_method=Nonedefset_method(self,method):self._method=methoddefget_method(self):returnself._methodorurllib2.Request.get_method(self)",,
SpiderOak/netkes,d68d0de5f44dc93ec04e0a105c592d038f890f8d,requests,urllib,"except urllib2.HTTPError, err:","        except requests.exceptions.HTTPError, err:",update,"excepturllib2.HTTPError,err:","exceptrequests.exceptions.HTTPError,err:",netkes/account_mgr/accounts_api.py
SpiderOak/netkes,d68d0de5f44dc93ec04e0a105c592d038f890f8d,requests,urllib,"        except urllib2.HTTPError, err:","        except requests.exceptions.HTTPError, err:",update,"excepturllib2.HTTPError,err:","exceptrequests.exceptions.HTTPError,err:",netkes/account_mgr/accounts_api.py
SpiderOak/netkes,d68d0de5f44dc93ec04e0a105c592d038f890f8d,requests,urllib,"        except urllib2.HTTPError, err:","        except requests.exceptions.HTTPError, err:",update,"excepturllib2.HTTPError,err:","exceptrequests.exceptions.HTTPError,err:",netkes/account_mgr/accounts_api.py
StackStorm/st2contrib,6a32852fdbd750e8727c9a0b34e1e47844850375,requests,urllib,"class OpenIncident(VictorOpsAction):
  def run(self, severity, entity, message=None):
    prms = {
    ""message_type"": severity,
    ""timestamp"": int(time.time()),
    ""entity_id"":entity,
    ""state_message"":message
    }
    post_data = json.dumps(prms).encode()
    data = urllib.urlopen(self.url, post_data)","class OpenIncident(VictorOpsAction):
    def run(self, severity, entity, message=None):
        prms = {
                ""message_type"": severity,
                ""timestamp"": int(time.time()),
                ""entity_id"": entity,
                ""state_message"": message}
        post_data = json.dumps(prms)
        data = requests.post(self.url, post_data)",update,"classOpenIncident(VictorOpsAction):defrun(self,severity,entity,message=None):prms={""message_type"":severity,""timestamp"":int(time.time()),""entity_id"":entity,""state_message"":message}post_data=json.dumps(prms).encode()data=urllib.urlopen(self.url,post_data)","classOpenIncident(VictorOpsAction):defrun(self,severity,entity,message=None):prms={""message_type"":severity,""timestamp"":int(time.time()),""entity_id"":entity,""state_message"":message}post_data=json.dumps(prms)data=requests.post(self.url,post_data)",packs/victorops/actions/open_incident.py
StevenBlack/hosts,5186071948529beb5c1da68f23c47403fa972160,requests,urllib,"def get_file_by_url(url, retries=3, delay=10):
    """"""
    Get a file data located at a particular URL.

    Parameters
    ----------
    url : str
        The URL at which to access the data.

    Returns
    -------
    url_data : str or None
        The data retrieved at that URL from the file. Returns None if the
        attempted retrieval is unsuccessful.

    Note
    ----
    - BeautifulSoup is used in this case to avoid having to search in which
        format we have to encode or decode data before parsing it to UTF-8.
    """"""

    while retries:
        try:
            with urlopen(url) as f:
                soup = BeautifulSoup(f.read(), ""lxml"").get_text()
                return ""\n"".join(list(map(domain_to_idna, soup.split(""\n""))))
        except Exception as e:
            if 'failure in name resolution' in str(e):
                print('No internet connection! Retrying in {} seconds'.format(delay))
                time.sleep(delay)
                retries -= 1
                continue
            break
    print(""Problem getting file: "", url)","def get_file_by_url(url, params, **kwargs):
    return requests.get(url=url, params=params, **kwargs).text






























",update,"defget_file_by_url(url,retries=3,delay=10):""""""GetafiledatalocatedataparticularURL.Parameters----------url:strTheURLatwhichtoaccessthedata.Returns-------url_data:strorNoneThedataretrievedatthatURLfromthefile.ReturnsNoneiftheattemptedretrievalisunsuccessful.Note-----BeautifulSoupisusedinthiscasetoavoidhavingtosearchinwhichformatwehavetoencodeordecodedatabeforeparsingittoUTF-8.""""""whileretries:try:withurlopen(url)asf:soup=BeautifulSoup(f.read(),""lxml"").get_text()return""\n"".join(list(map(domain_to_idna,soup.split(""\n""))))exceptExceptionase:if'failureinnameresolution'instr(e):print('Nointernetconnection!Retryingin{}seconds'.format(delay))time.sleep(delay)retries-=1continuebreakprint(""Problemgettingfile:"",url)","defget_file_by_url(url,params,**kwargs):returnrequests.get(url=url,params=params,**kwargs).text",updateHostsFile.py
UltrosBot/Ultros,87c4d15e25cc5467bb6ae60fb7d03244c752292d,requests,urllib,"    def post(self, url, data):
        data = json.dumps(data)
        self.log.debug(""Posting data: %s"" % data)

        data = urllib.urlencode({""data"": data})
        req = urllib2.Request(
            url, data, {'Content-Type': 'application/json'}
        )

        result = urllib2.urlopen(req).read()
        self.log.debug(""Result: %s"" % result)
        return result

    def get(self, url):
        return urllib2.urlopen(url).read()","    def post(self, url, data):
        data = {""data"": json.dumps(data)}
        self.log.debug(""Posting data: %s"" % data)

        req = requests.post(
            url, data, headers={'Content-Type': 'application/json'}

        )

        result = req.text
        self.log.debug(""Result: %s"" % result)
        return result

    def get(self, url):
        return requests.get(url).text",update,"defpost(self,url,data):data=json.dumps(data)self.log.debug(""Postingdata:%s""%data)data=urllib.urlencode({""data"":data})req=urllib2.Request(url,data,{'Content-Type':'application/json'})result=urllib2.urlopen(req).read()self.log.debug(""Result:%s""%result)returnresultdefget(self,url):returnurllib2.urlopen(url).read()","defpost(self,url,data):data={""data"":json.dumps(data)}self.log.debug(""Postingdata:%s""%data)req=requests.post(url,data,headers={'Content-Type':'application/json'})result=req.textself.log.debug(""Result:%s""%result)returnresultdefget(self,url):returnrequests.get(url).text",system/metrics.py
WeblateOrg/weblate,ee55805e56d41cdc181147fb837f4a8dcd816191,requests,urllib,"def download_languages(self):
        """"""Download list of supported languages from a service.""""""
        data = self.json_status_req('{0}/listPairs'.format(self.url))
        return [
            (item['sourceLanguage'], item['targetLanguage'])
            for item in data['responseData']","    def download_languages(self):
        """"""Download list of supported languages from a service.""""""
        data = self.request_status(""get"", '{0}/listPairs'.format(self.url))
        return [
            (item['sourceLanguage'], item['targetLanguage'])
            for item in data['responseData']",update,"defdownload_languages(self):""""""Downloadlistofsupportedlanguagesfromaservice.""""""data=self.json_status_req('{0}/listPairs'.format(self.url))return[(item['sourceLanguage'],item['targetLanguage'])foritemindata['responseData']","defdownload_languages(self):""""""Downloadlistofsupportedlanguagesfromaservice.""""""data=self.request_status(""get"",'{0}/listPairs'.format(self.url))return[(item['sourceLanguage'],item['targetLanguage'])foritemindata['responseData']",
WeblateOrg/weblate,ee55805e56d41cdc181147fb837f4a8dcd816191,requests,urllib," def download_translations(self, source, language, text, unit, user):
        """"""Download list of possible translations from Apertium.""""""
        args = {'langpair': '{0}|{1}'.format(source, language), 'q': text}
        response = self.json_status_req('{0}/translate'.format(self.url), **args)



        return [
            {","
    def download_translations(self, source, language, text, unit, user):
        """"""Download list of possible translations from Apertium.""""""
        args = {'langpair': '{0}|{1}'.format(source, language), 'q': text}
        response = self.request_status(
            ""get"", '{0}/translate'.format(self.url), params=args
        )

        return [
            {",update,"defdownload_translations(self,source,language,text,unit,user):""""""DownloadlistofpossibletranslationsfromApertium.""""""args={'langpair':'{0}|{1}'.format(source,language),'q':text}response=self.json_status_req('{0}/translate'.format(self.url),**args)return[{","defdownload_translations(self,source,language,text,unit,user):""""""DownloadlistofpossibletranslationsfromApertium.""""""args={'langpair':'{0}|{1}'.format(source,language),'q':text}response=self.request_status(""get"",'{0}/translate'.format(self.url),params=args)return[{",weblate/machinery/baidu.py
WeblateOrg/weblate,ee55805e56d41cdc181147fb837f4a8dcd816191,requests,urllib,"        response = self.json_req(BAIDU_API, **args)


        if 'error_code' in response:
            raise MachineTranslationError(
                'Error {error_code}: {error_msg}'.format(**response)
            )","        response = self.request(""get"", BAIDU_API, params=args)
        payload = response.json()

        if 'error_code' in payload:
            raise MachineTranslationError(
                'Error {error_code}: {error_msg}'.format(**payload)
            )",update,"response=self.json_req(BAIDU_API,**args)if'error_code'inresponse:raiseMachineTranslationError('Error{error_code}:{error_msg}'.format(**response))","response=self.request(""get"",BAIDU_API,params=args)payload=response.json()if'error_code'inpayload:raiseMachineTranslationError('Error{error_code}:{error_msg}'.format(**payload))",weblate/machinery/baidu.py
WeblateOrg/weblate,ee55805e56d41cdc181147fb837f4a8dcd816191,requests,urllib,"def json_req(
        self,
        url,
        http_post=False,
        skip_auth=False,
        raw=False,
        json_body=False,
        **kwargs
    ):
        """"""Perform JSON request.""""""
        # JSON body requires using POST
        if json_body:
            http_post = True

        # Encode params
        if kwargs:
            if json_body:
                params = json.dumps(kwargs)
            else:
                params = urlencode(kwargs)
        else:
            if json_body:
                params = '{}'
            else:
                params = ''

        # Store for exception handling
        self.request_url = url
        self.request_params = params

        # Append parameters
        if params and not http_post:
            url = '?'.join((url, params))

        # Create request object with custom headers
        request = Request(url)
        request.add_header('User-Agent', USER_AGENT)
        request.add_header('Referer', get_site_url())
        # Optional authentication
        if not skip_auth:
            self.authenticate(request)

        # Fire request
        if http_post:
            handle = urlopen(request, params.encode('utf-8'), timeout=5.0)
        else:
            handle = urlopen(request, timeout=5.0)

        # Read and possibly convert response
        text = handle.read()
        # Needed for Microsoft
        if text[:3] == b'\xef\xbb\xbf':
            text = text.decode('UTF-8-sig')
        else:
            text = text.decode('utf-8')
        # Replace literal \t
        text = text.strip().replace('\t', '\\t').replace('\r', '\\r')
        # Needed for Google
        while ',,' in text or '[,' in text:
            text = text.replace(',,', ',null,').replace('[,', '[')

        if raw:
            return text

        # Parse and return JSON
        return json.loads(text)"," def request(self, method, url, skip_auth=False, **kwargs):








        """"""Perform JSON request.""""""
        # Create custom headers
        headers = {
            'Referer': get_site_url(),
            'Accept': 'application/json; charset=utf-8',
        }
        if ""headers"" in kwargs:
            headers.update(kwargs.pop(""headers""))





















        # Optional authentication
        if not skip_auth:
            headers.update(self.get_authentication())

        # Fire request
        return request(method, url, headers=headers, timeout=5.0, **kwargs)",update,"defjson_req(self,url,http_post=False,skip_auth=False,raw=False,json_body=False,**kwargs):""""""PerformJSONrequest.""""""#JSONbodyrequiresusingPOSTifjson_body:http_post=True#Encodeparamsifkwargs:ifjson_body:params=json.dumps(kwargs)else:params=urlencode(kwargs)else:ifjson_body:params='{}'else:params=''#Storeforexceptionhandlingself.request_url=urlself.request_params=params#Appendparametersifparamsandnothttp_post:url='?'.join((url,params))#Createrequestobjectwithcustomheadersrequest=Request(url)request.add_header('User-Agent',USER_AGENT)request.add_header('Referer',get_site_url())#Optionalauthenticationifnotskip_auth:self.authenticate(request)#Firerequestifhttp_post:handle=urlopen(request,params.encode('utf-8'),timeout=5.0)else:handle=urlopen(request,timeout=5.0)#Readandpossiblyconvertresponsetext=handle.read()#NeededforMicrosoftiftext[:3]==b'\xef\xbb\xbf':text=text.decode('UTF-8-sig')else:text=text.decode('utf-8')#Replaceliteral\ttext=text.strip().replace('\t','\\t').replace('\r','\\r')#NeededforGooglewhile',,'intextor'[,'intext:text=text.replace(',,',',null,').replace('[,','[')ifraw:returntext#ParseandreturnJSONreturnjson.loads(text)","defrequest(self,method,url,skip_auth=False,**kwargs):""""""PerformJSONrequest.""""""#Createcustomheadersheaders={'Referer':get_site_url(),'Accept':'application/json;charset=utf-8',}if""headers""inkwargs:headers.update(kwargs.pop(""headers""))#Optionalauthenticationifnotskip_auth:headers.update(self.get_authentication())#Firerequestreturnrequest(method,url,headers=headers,timeout=5.0,**kwargs)",weblate/machinery/base.py
WeblateOrg/weblate,ee55805e56d41cdc181147fb837f4a8dcd816191,requests,urllib,"    def json_status_req(self, url, http_post=False, skip_auth=False, **kwargs):
        """"""Perform JSON request with checking response status.""""""
        # Perform request
        response = self.json_req(url, http_post, skip_auth, **kwargs)

        # Check response status
        if response['responseStatus'] != 200:
            raise MachineTranslationError(response['responseDetails'])

        # Return data
        return response","    def request_status(self, method, url, **kwargs):
        response = self.request(method, url, **kwargs)
        payload = response.json()
























        # Check response status
        if payload['responseStatus'] != 200:
            raise MachineTranslationError(payload['responseDetails'])

        # Return data
        return payload",update,"defjson_status_req(self,url,http_post=False,skip_auth=False,**kwargs):""""""PerformJSONrequestwithcheckingresponsestatus.""""""#Performrequestresponse=self.json_req(url,http_post,skip_auth,**kwargs)#Checkresponsestatusifresponse['responseStatus']!=200:raiseMachineTranslationError(response['responseDetails'])#Returndatareturnresponse","defrequest_status(self,method,url,**kwargs):response=self.request(method,url,**kwargs)payload=response.json()#Checkresponsestatusifpayload['responseStatus']!=200:raiseMachineTranslationError(payload['responseDetails'])#Returndatareturnpayload",weblate/machinery/base.py
WeblateOrg/weblate,ee55805e56d41cdc181147fb837f4a8dcd816191,requests,urllib,"        response = self.json_req(

            DEEPL_API,
            http_post=True,
            auth_key=settings.MT_DEEPL_KEY,
            text=text,
            source_lang=source,
            target_lang=language,

        )","        response = self.request(
            ""post"",
            DEEPL_API,
            data={
                'auth_key': settings.MT_DEEPL_KEY,
                'text': text,
                'source_lang': source,
                'target_lang': language,
            },
        )
        payload = response.json()",update,"response=self.json_req(DEEPL_API,http_post=True,auth_key=settings.MT_DEEPL_KEY,text=text,source_lang=source,target_lang=language,)","response=self.request(""post"",DEEPL_API,data={'auth_key':settings.MT_DEEPL_KEY,'text':text,'source_lang':source,'target_lang':language,},)payload=response.json()",weblate/machinery/deepl.py
WeblateOrg/weblate,ee55805e56d41cdc181147fb837f4a8dcd816191,requests,urllib,"response = self.json_req('https://glosbe.com/gapi/translate', **params)"," response = self.request(
            ""get"", 'https://glosbe.com/gapi/translate', params=params
        )
        payload = response.json()",update,"response=self.json_req('https://glosbe.com/gapi/translate',**params)","response=self.request(""get"",'https://glosbe.com/gapi/translate',params=params)payload=response.json()",weblate/machinery/glosbe.py
WeblateOrg/weblate,ee55805e56d41cdc181147fb837f4a8dcd816191,requests,urllib,"    def download_languages(self):
        """"""List of supported languages.""""""
        response = self.json_req(
            GOOGLE_API_ROOT + 'languages', key=settings.MT_GOOGLE_KEY
        )


        if 'error' in response:
            raise MachineTranslationError(response['error']['message'])

        return [d['language'] for d in response['data']['languages']]","    def download_languages(self):
        """"""List of supported languages.""""""
        response = self.request(
            ""get"", GOOGLE_API_ROOT + 'languages', params={'key': settings.MT_GOOGLE_KEY}
        )
        payload = response.json()

        if 'error' in payload:
            raise MachineTranslationError(payload['error']['message'])

        return [d['language'] for d in payload['data']['languages']]",update,"defdownload_languages(self):""""""Listofsupportedlanguages.""""""response=self.json_req(GOOGLE_API_ROOT+'languages',key=settings.MT_GOOGLE_KEY)if'error'inresponse:raiseMachineTranslationError(response['error']['message'])return[d['language']fordinresponse['data']['languages']]","defdownload_languages(self):""""""Listofsupportedlanguages.""""""response=self.request(""get"",GOOGLE_API_ROOT+'languages',params={'key':settings.MT_GOOGLE_KEY})payload=response.json()if'error'inpayload:raiseMachineTranslationError(payload['error']['message'])return[d['language']fordinpayload['data']['languages']]",weblate/machinery/google.py
WeblateOrg/weblate,ee55805e56d41cdc181147fb837f4a8dcd816191,requests,urllib,"def download_translations(self, source, language, text, unit, user):
        """"""Download list of possible translations from a service.""""""
        response = self.json_req(

            GOOGLE_API_ROOT,
            key=settings.MT_GOOGLE_KEY,
            q=text,
            source=source,
            target=language,
            format='text',


        )


        if 'error' in response:
            raise MachineTranslationError(response['error']['message'])

        translation = response['data']['translations'][0]['translatedText']","    def download_translations(self, source, language, text, unit, user):
        """"""Download list of possible translations from a service.""""""
        response = self.request(
            ""get"",
            GOOGLE_API_ROOT,
            params={
                'key': settings.MT_GOOGLE_KEY,
                'q': text,
                'source': source,
                'target': language,
                'format': 'text',
            },
        )
        payload = response.json()

        if 'error' in payload:
            raise MachineTranslationError(payload['error']['message'])

        translation = payload['data']['translations'][0]['translatedText']",update,"defdownload_translations(self,source,language,text,unit,user):""""""Downloadlistofpossibletranslationsfromaservice.""""""response=self.json_req(GOOGLE_API_ROOT,key=settings.MT_GOOGLE_KEY,q=text,source=source,target=language,format='text',)if'error'inresponse:raiseMachineTranslationError(response['error']['message'])translation=response['data']['translations'][0]['translatedText']","defdownload_translations(self,source,language,text,unit,user):""""""Downloadlistofpossibletranslationsfromaservice.""""""response=self.request(""get"",GOOGLE_API_ROOT,params={'key':settings.MT_GOOGLE_KEY,'q':text,'source':source,'target':language,'format':'text',},)payload=response.json()if'error'inpayload:raiseMachineTranslationError(payload['error']['message'])translation=payload['data']['translations'][0]['translatedText']",weblate/machinery/apertium.py
WeblateOrg/weblate,ee55805e56d41cdc181147fb837f4a8dcd816191,requests,urllib,"response = self.json_req(TRANSLATE_URL, **args)"," response = self.request(""get"", TRANSLATE_URL, params=args)
        # Microsoft tends to use utf-8-sig instead of plain utf-8
        response.encoding = response.apparent_encoding
        payload = response.json()",update,"response=self.json_req(TRANSLATE_URL,**args)","response=self.request(""get"",TRANSLATE_URL,params=args)#Microsofttendstouseutf-8-siginsteadofplainutf-8response.encoding=response.apparent_encodingpayload=response.json()",weblate/machinery/microsoft.py
WeblateOrg/weblate,ee55805e56d41cdc181147fb837f4a8dcd816191,requests,urllib,"        response = self.json_status_req(
            'https://mymemory.translated.net/api/get', **args
        )","        response = self.request_status(
            ""get"", 'https://mymemory.translated.net/api/get', params=args
        )",update,"response=self.json_status_req('https://mymemory.translated.net/api/get',**args)","response=self.request_status(""get"",'https://mymemory.translated.net/api/get',params=args)",weblate/machinery/mymemory.py
WeblateOrg/weblate,ee55805e56d41cdc181147fb837f4a8dcd816191,requests,urllib,"def download_translations(self, source, language, text, unit, user):
        """"""Download list of possible translations from a service.""""""
        response = self.json_req(
            NETEASE_API_ROOT, http_post=True, json_body=True, lang=source, content=text
        )


        if not response['success']:
            raise MachineTranslationError(response['message'])

        translation = response['relatedObject']['content'][0]['transContent']","    def download_translations(self, source, language, text, unit, user):
        """"""Download list of possible translations from a service.""""""
        response = self.request(
            ""post"", NETEASE_API_ROOT, json={""lang"": source, ""content"": text}
        )
        payload = response.json()

        if not payload['success']:
            raise MachineTranslationError(payload['message'])

        translation = payload['relatedObject']['content'][0]['transContent']",update,"defdownload_translations(self,source,language,text,unit,user):""""""Downloadlistofpossibletranslationsfromaservice.""""""response=self.json_req(NETEASE_API_ROOT,http_post=True,json_body=True,lang=source,content=text)ifnotresponse['success']:raiseMachineTranslationError(response['message'])translation=response['relatedObject']['content'][0]['transContent']","defdownload_translations(self,source,language,text,unit,user):""""""Downloadlistofpossibletranslationsfromaservice.""""""response=self.request(""post"",NETEASE_API_ROOT,json={""lang"":source,""content"":text})payload=response.json()ifnotpayload['success']:raiseMachineTranslationError(payload['message'])translation=payload['relatedObject']['content'][0]['transContent']",weblate/machinery/apertium.py
WeblateOrg/weblate,ee55805e56d41cdc181147fb837f4a8dcd816191,requests,urllib,"            data = self.json_req('{0}/languages/'.format(self.url))

        except HTTPError as error:
            if error.code == 404:
                return []
            raise
        return [","            response = self.request(""get"", '{0}/languages/'.format(self.url))
            data = response.json()
        except HTTPError as error:
            if error.response.status_code == 404:
                return []
            raise
        return [",update,data=self.json_req('{0}/languages/'.format(self.url))exceptHTTPErroraserror:iferror.code==404:return[]raisereturn[,"response=self.request(""get"",'{0}/languages/'.format(self.url))data=response.json()exceptHTTPErroraserror:iferror.response.status_code==404:return[]raisereturn[",weblate/machinery/tmserver.py
WorldWideTelescope/pywwt,9e00ca03915ffe912f3d2ed0cbaba3e342b812d2,requests,urllib,"import os
import socket
import urllib
import urllib2
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

class WWTLayer(object):
    def __init__(self, name, id, fields):
        self.name = name
        self.id = id
        self.fields = fields
        
class WWTController(object):
    
    def __init__(self, ip=None):
        if ip is None:
            self.ip = socket.gethostbyname(socket.gethostname())
        else:
            self.ip = ip
        if not self._check_for_server():
            raise Exception(""WWT has not been started at this address, "" +
                            ""or is unreachable, or is not the required version."")
        self.layers = {}
        
    def _check_for_server(self):
        try:
            u = urllib2.urlopen(""http://%s:5050/layerapi.aspx?cmd=version"" % 
                                (self.ip))
            version_str = u.read()
            soup = BeautifulSoup(version_str)
            tag = soup.layerapi.version
            version_numbers = tag.string.split(""."")
            if float(""."".join(version_numbers[:2])) > 2.8:
                return True
        except:
            pass
        return False
    
    def change_mode(self, mode, **kwargs):
        my_url = ""http://%s:5050/layerApi.aspx?cmd=mode&lookat=%s"" % (self.ip, 
                                                                      mode)
        u = urllib2.urlopen(my_url)
        
    def new_layer(self, frame, name, fields=None,
                  color=None, start_date=None,
                  end_date=None, fade_type=None,
                  fade_range=None):
        if name in self.layers:
            raise Exception(""Layer already exists with this name! Choose a different one."")
        if fields is None:
            field_string = """"
        else:
            field_string = ""\t"".join(fields)
        params_dict = {}
        params_dict[""name""] = name
        params_dict[""frame""] = frame
        if start_date is not None:
            params_dict[""startdate""] = start_date
        if end_date is not None:
            params_dict[""enddate""] = end_date
        if fade_type is not None:
            params_dict[""fadetype""] = fade_type
        if fade_range is not None:
            params_dict[""faderange""] = fade_range
        params = urllib.urlencode(params_dict)
        my_url = ""http://%s:5050/layerApi.aspx?cmd=new&%s"" % (self.ip, params)
        u = urllib2.urlopen(my_url, data=field_string)
        layer_str = u.read()
        soup = BeautifulSoup(layer_str)
        layer_id = soup.layerapi.findChild(name=""newlayerid"").string
        if len(layer_id) != 36:
            raise Exception(""Invalid Layer ID received"")
        self.layers[name] = WWTLayer(name, layer_id, fields)
          
    def update_layer(self, name, data,
                     no_purge=False, purge_all=False):
        layer_id = self.layers[name].id
        params_dict = {}
        params_dict[""purgeall""] = str(purge_all).lower()
        params_dict[""nopurge""] = str(no_purge).lower()
        params = urllib.urlencode(params_dict)
        my_url = ""http://%s:5050/layerApi.aspx?cmd=update&id=%s&%s"" % (self.ip, 
                                                                       layer_id,
                                                                       params)
        fields = self.layers[name].fields
        nevents = len(data[fields[0]])
        for i in xrange(nevents):
            data_string = ""\t"".join([str(data[k][i]) for k in fields])
            u = urllib2.urlopen(my_url, data=data_string)
            update_str = u.read()
        
    def set_properties(self, name, props_dict):
        layer_id = self.layers[name].id
        props_string = ""<?xml version='1.0' encoding='UTF-8'?><LayerApi><Layer ""
        for key, value in props_dict.items():
            props_string += ""%s=\""%s\"" "" % (key, value)
        props_string += "" /></LayerApi>""
        my_url = ""http://%s:5050/layerApi.aspx?cmd=setprops&id=%s"" % (self.ip, 
                                                                      layer_id)
        u = urllib2.urlopen(my_url, data=props_string)

    def delete_layer(self, name):
        layer_id = self.layers[name].id
        my_url = ""http://%s:5050/layerApi.aspx?cmd=delete&id=%s"" % (self.ip, 
                                                                    layer_id)
        u = urllib2.urlopen(my_url)
        self.layers.pop(""name"")",,delete,"importosimportsocketimporturllibimporturllib2frombs4importBeautifulSoupfromdatetimeimportdatetime,timedeltaclassWWTLayer(object):def__init__(self,name,id,fields):self.name=nameself.id=idself.fields=fieldsclassWWTController(object):def__init__(self,ip=None):ifipisNone:self.ip=socket.gethostbyname(socket.gethostname())else:self.ip=ipifnotself._check_for_server():raiseException(""WWThasnotbeenstartedatthisaddress,""+""orisunreachable,orisnottherequiredversion."")self.layers={}def_check_for_server(self):try:u=urllib2.urlopen(""http://%s:5050/layerapi.aspx?cmd=version""%(self.ip))version_str=u.read()soup=BeautifulSoup(version_str)tag=soup.layerapi.versionversion_numbers=tag.string.split(""."")iffloat(""."".join(version_numbers[:2]))>2.8:returnTrueexcept:passreturnFalsedefchange_mode(self,mode,**kwargs):my_url=""http://%s:5050/layerApi.aspx?cmd=mode&lookat=%s""%(self.ip,mode)u=urllib2.urlopen(my_url)defnew_layer(self,frame,name,fields=None,color=None,start_date=None,end_date=None,fade_type=None,fade_range=None):ifnameinself.layers:raiseException(""Layeralreadyexistswiththisname!Chooseadifferentone."")iffieldsisNone:field_string=""""else:field_string=""\t"".join(fields)params_dict={}params_dict[""name""]=nameparams_dict[""frame""]=frameifstart_dateisnotNone:params_dict[""startdate""]=start_dateifend_dateisnotNone:params_dict[""enddate""]=end_dateiffade_typeisnotNone:params_dict[""fadetype""]=fade_typeiffade_rangeisnotNone:params_dict[""faderange""]=fade_rangeparams=urllib.urlencode(params_dict)my_url=""http://%s:5050/layerApi.aspx?cmd=new&%s""%(self.ip,params)u=urllib2.urlopen(my_url,data=field_string)layer_str=u.read()soup=BeautifulSoup(layer_str)layer_id=soup.layerapi.findChild(name=""newlayerid"").stringiflen(layer_id)!=36:raiseException(""InvalidLayerIDreceived"")self.layers[name]=WWTLayer(name,layer_id,fields)defupdate_layer(self,name,data,no_purge=False,purge_all=False):layer_id=self.layers[name].idparams_dict={}params_dict[""purgeall""]=str(purge_all).lower()params_dict[""nopurge""]=str(no_purge).lower()params=urllib.urlencode(params_dict)my_url=""http://%s:5050/layerApi.aspx?cmd=update&id=%s&%s""%(self.ip,layer_id,params)fields=self.layers[name].fieldsnevents=len(data[fields[0]])foriinxrange(nevents):data_string=""\t"".join([str(data[k][i])forkinfields])u=urllib2.urlopen(my_url,data=data_string)update_str=u.read()defset_properties(self,name,props_dict):layer_id=self.layers[name].idprops_string=""<?xmlversion='1.0'encoding='UTF-8'?><LayerApi><Layer""forkey,valueinprops_dict.items():props_string+=""%s=\""%s\""""%(key,value)props_string+=""/></LayerApi>""my_url=""http://%s:5050/layerApi.aspx?cmd=setprops&id=%s""%(self.ip,layer_id)u=urllib2.urlopen(my_url,data=props_string)defdelete_layer(self,name):layer_id=self.layers[name].idmy_url=""http://%s:5050/layerApi.aspx?cmd=delete&id=%s""%(self.ip,layer_id)u=urllib2.urlopen(my_url)self.layers.pop(""name"")",,pywwt.py
WorldWideTelescope/pywwt,9e00ca03915ffe912f3d2ed0cbaba3e342b812d2,requests,urllib,,"import socket
import requests
from bs4 import BeautifulSoup
import logging
requests_log = logging.getLogger(""requests"")
requests_log.setLevel(logging.WARNING)

class WWTLayer(object):
    def __init__(self, name, id, fields):
        self.name = name
        self.id = id
        self.fields = fields
        
class WWTController(object):
    
    def __init__(self, host=None):
        if host is None:
            self.host = socket.gethostbyname(socket.gethostname())
        else:
            self.host = host
        self.wwt_url = ""http://%s:5050/layerApi.aspx"" % (self.host)
        if not self._check_for_server():
            raise Exception(""WWT has not been started at this address, "" +
                            ""or is unreachable, or is not the required version."")
        self.layers = {}
        
    def _check_for_server(self):
        try:
            params = {""cmd"":""version""}
            u = requests.get(self.wwt_url, params=params)
            version_str = u.text
            soup = BeautifulSoup(version_str)
            tag = soup.layerapi.version
            version_numbers = tag.string.split(""."")
            if float(""."".join(version_numbers[:2])) > 2.8:
                return True
        except:
            pass
        return False

    def _handle_response(self, resp_str):
        soup = BeautifulSoup(resp_str)
        success = soup.layerapi.status.string
        if success != ""Success"":
            print success

    def change_mode(self, mode):
        params = {}
        params[""cmd""] = ""mode""
        params[""lookat""] = mode
        u = requests.get(self.wwt_url, params=params)
        mode_str = u.text
        _handle_response(mode_str)

    def move_view(self, parameter):
        params = {""cmd"":""move"", ""move"":parameter}
        u = requests.get(self.wwt_url, params=params)
        move_str = u.text
        _handle_response(move_str)

    def new_layer(self, frame, name, fields=None,
                  color=None, start_date=None,
                  end_date=None, fade_type=None,
                  fade_range=None):
        if name in self.layers:
            raise Exception(""Layer already exists with this name! Choose a different one."")
        if fields is None:
            field_string = """"
        else:
            field_string = ""\t"".join(fields)
        params = {}
        params[""cmd""] = ""new""
        params[""name""] = name
        params[""frame""] = frame
        params[""startdate""] = start_date
        params[""enddate""] = end_date
        params[""fadetype""] = fade_type
        params[""faderange""] = fade_range
        u = requests.post(self.wwt_url, params=params, data=field_string)
        layer_str = u.text
        _handle_response(layer_str)
        soup = BeautifulSoup(layer_str)
        layer_id = soup.layerapi.findChild(name=""newlayerid"").string
        if len(layer_id) != 36:
            raise Exception(""Invalid Layer ID received"")
        self.layers[name] = WWTLayer(name, layer_id, fields)

    def load(self, filename, frame, name, color=None,
             start_date=None, end_date=None,
             fade_type=None, fade_range=None):
        params = {}
        params[""cmd""] = ""load""
        params[""filename""] = filename
        params[""frame""] = frame
        params[""color""] = color
        params[""startdate""] = start_date
        params[""enddate""] = end_date
        params[""fadetype""] = fade_type
        params[""faderange""] = fade_range
        u = requests.get(self.wwt_url, params=params)
        load_str = u.text
        _handle_response(load_str)

    def update_layer(self, name, data,
                     no_purge=False, purge_all=False):
        layer_id = self.layers[name].id
        params = {}
        params[""cmd""] = ""update""
        params[""id""] = layer_id
        params[""purgeall""] = str(purge_all).lower()
        params[""nopurge""] = str(no_purge).lower()
        fields = self.layers[name].fields
        nevents = len(data[fields[0]])
        for i in xrange(nevents):
            data_string = ""\t"".join([str(data[k][i]) for k in fields])
            u = requests.post(self.wwt_url, params=params, data=data_string)
            update_str = u.text
            _handle_response(update_str)
        
    def set_properties(self, name, props_dict):
        layer_id = self.layers[name].id
        props_string = ""<?xml version='1.0' encoding='UTF-8'?><LayerApi><Layer ""
        for key, value in props_dict.items():
            props_string += ""%s=\""%s\"" "" % (key, value)
        props_string += "" /></LayerApi>""
        params = {}
        params[""cmd""] = ""setprops""
        params[""id""] = layer_id
        u = requests.post(self.wwt_url, params=params, data=props_string)
        props_str = u.text
        _handle_response(props_str)

    def delete_layer(self, name):
        layer_id = self.layers[name].id
        params = {}
        params[""cmd""] = ""delete""
        params[""id""] = layer_id
        u = requests.get(self.wwt_url, params=params)
        layer_str = u.text
        _handle_response(layer_str)
        self.layers.pop(name)

    def uisettings(self, setting_name, setting_val):
        params = {""cmd"":""uisettings"",
                  setting_name:setting_val}
        u = requests.get(self.wwt_url, params=params)
        ui_str = u.text
        _handle_response(ui_str)",insert,,"importsocketimportrequestsfrombs4importBeautifulSoupimportloggingrequests_log=logging.getLogger(""requests"")requests_log.setLevel(logging.WARNING)classWWTLayer(object):def__init__(self,name,id,fields):self.name=nameself.id=idself.fields=fieldsclassWWTController(object):def__init__(self,host=None):ifhostisNone:self.host=socket.gethostbyname(socket.gethostname())else:self.host=hostself.wwt_url=""http://%s:5050/layerApi.aspx""%(self.host)ifnotself._check_for_server():raiseException(""WWThasnotbeenstartedatthisaddress,""+""orisunreachable,orisnottherequiredversion."")self.layers={}def_check_for_server(self):try:params={""cmd"":""version""}u=requests.get(self.wwt_url,params=params)version_str=u.textsoup=BeautifulSoup(version_str)tag=soup.layerapi.versionversion_numbers=tag.string.split(""."")iffloat(""."".join(version_numbers[:2]))>2.8:returnTrueexcept:passreturnFalsedef_handle_response(self,resp_str):soup=BeautifulSoup(resp_str)success=soup.layerapi.status.stringifsuccess!=""Success"":printsuccessdefchange_mode(self,mode):params={}params[""cmd""]=""mode""params[""lookat""]=modeu=requests.get(self.wwt_url,params=params)mode_str=u.text_handle_response(mode_str)defmove_view(self,parameter):params={""cmd"":""move"",""move"":parameter}u=requests.get(self.wwt_url,params=params)move_str=u.text_handle_response(move_str)defnew_layer(self,frame,name,fields=None,color=None,start_date=None,end_date=None,fade_type=None,fade_range=None):ifnameinself.layers:raiseException(""Layeralreadyexistswiththisname!Chooseadifferentone."")iffieldsisNone:field_string=""""else:field_string=""\t"".join(fields)params={}params[""cmd""]=""new""params[""name""]=nameparams[""frame""]=frameparams[""startdate""]=start_dateparams[""enddate""]=end_dateparams[""fadetype""]=fade_typeparams[""faderange""]=fade_rangeu=requests.post(self.wwt_url,params=params,data=field_string)layer_str=u.text_handle_response(layer_str)soup=BeautifulSoup(layer_str)layer_id=soup.layerapi.findChild(name=""newlayerid"").stringiflen(layer_id)!=36:raiseException(""InvalidLayerIDreceived"")self.layers[name]=WWTLayer(name,layer_id,fields)defload(self,filename,frame,name,color=None,start_date=None,end_date=None,fade_type=None,fade_range=None):params={}params[""cmd""]=""load""params[""filename""]=filenameparams[""frame""]=frameparams[""color""]=colorparams[""startdate""]=start_dateparams[""enddate""]=end_dateparams[""fadetype""]=fade_typeparams[""faderange""]=fade_rangeu=requests.get(self.wwt_url,params=params)load_str=u.text_handle_response(load_str)defupdate_layer(self,name,data,no_purge=False,purge_all=False):layer_id=self.layers[name].idparams={}params[""cmd""]=""update""params[""id""]=layer_idparams[""purgeall""]=str(purge_all).lower()params[""nopurge""]=str(no_purge).lower()fields=self.layers[name].fieldsnevents=len(data[fields[0]])foriinxrange(nevents):data_string=""\t"".join([str(data[k][i])forkinfields])u=requests.post(self.wwt_url,params=params,data=data_string)update_str=u.text_handle_response(update_str)defset_properties(self,name,props_dict):layer_id=self.layers[name].idprops_string=""<?xmlversion='1.0'encoding='UTF-8'?><LayerApi><Layer""forkey,valueinprops_dict.items():props_string+=""%s=\""%s\""""%(key,value)props_string+=""/></LayerApi>""params={}params[""cmd""]=""setprops""params[""id""]=layer_idu=requests.post(self.wwt_url,params=params,data=props_string)props_str=u.text_handle_response(props_str)defdelete_layer(self,name):layer_id=self.layers[name].idparams={}params[""cmd""]=""delete""params[""id""]=layer_idu=requests.get(self.wwt_url,params=params)layer_str=u.text_handle_response(layer_str)self.layers.pop(name)defuisettings(self,setting_name,setting_val):params={""cmd"":""uisettings"",setting_name:setting_val}u=requests.get(self.wwt_url,params=params)ui_str=u.text_handle_response(ui_str)",pywwt/pywwt.py
Yinzo/SmartQQBot,c05e1da59980ee1159561a7e517071a02b8ef713,requests,urllib,"def get(self, url, refer=None):
        try:
            req = urllib2.Request(url)
            req.add_header('Referer', refer or SMART_QQ_REFER)
            tmp_req = self._opener.open(req)
            self._cookie.save(COOKIE_FILE, ignore_discard=True,ignore_expires=True)
            return tmp_req.read()
        except urllib2.HTTPError, e:
            return e.read()"," def get(self, url, refer=None):
        try:
            resp = self.session.get(
                url,
                headers=self._get_headers({'Referer': refer or SMART_QQ_REFER}),
            )
        except (excps.ConnectTimeout, excps.HTTPError):
            error_msg = ""Failed to send finish request to `{0}`"".format(
                url
            )
            logger.exception(error_msg)
            return error_msg
        else:
            self._cookies.save(COOKIE_FILE, ignore_discard=True, ignore_expires=True)
            return resp.text",update,"defget(self,url,refer=None):try:req=urllib2.Request(url)req.add_header('Referer',referorSMART_QQ_REFER)tmp_req=self._opener.open(req)self._cookie.save(COOKIE_FILE,ignore_discard=True,ignore_expires=True)returntmp_req.read()excepturllib2.HTTPError,e:returne.read()","defget(self,url,refer=None):try:resp=self.session.get(url,headers=self._get_headers({'Referer':referorSMART_QQ_REFER}),)except(excps.ConnectTimeout,excps.HTTPError):error_msg=""Failedtosendfinishrequestto`{0}`"".format(url)logger.exception(error_msg)returnerror_msgelse:self._cookies.save(COOKIE_FILE,ignore_discard=True,ignore_expires=True)returnresp.text",src/smart_qq_bot/http_client.py
Yinzo/SmartQQBot,c05e1da59980ee1159561a7e517071a02b8ef713,requests,urllib,"def post(self, url, data, refer=None):
        try:
            req = urllib2.Request(url, urllib.urlencode(data))
            req.add_header('Referer', refer or SMART_QQ_REFER)
            try:
                tmp_req = self._opener.open(req, timeout=180)
            except BadStatusLine:
                raise ServerResponseEmpty(""Server response error, check the network connections: %s"" % url)
            self._cookie.save(COOKIE_FILE, ignore_discard=True, ignore_expires=True)
            return tmp_req.read()
        except urllib2.HTTPError, e:
            return e.read()","def post(self, url, data, refer=None):
        try:
            resp = self.session.post(
                url,
                data,
                headers=self._get_headers({'Referer': refer or SMART_QQ_REFER}),
            )
        except Exception:
            error_msg = ""Failed to send request to `{0}`"".format(
                url
            )
            logger.exception(error_msg)
            return error_msg
        else:
            self._cookies.save(COOKIE_FILE, ignore_discard=True, ignore_expires=True)
            return resp.text",update,"defpost(self,url,data,refer=None):try:req=urllib2.Request(url,urllib.urlencode(data))req.add_header('Referer',referorSMART_QQ_REFER)try:tmp_req=self._opener.open(req,timeout=180)exceptBadStatusLine:raiseServerResponseEmpty(""Serverresponseerror,checkthenetworkconnections:%s""%url)self._cookie.save(COOKIE_FILE,ignore_discard=True,ignore_expires=True)returntmp_req.read()excepturllib2.HTTPError,e:returne.read()","defpost(self,url,data,refer=None):try:resp=self.session.post(url,data,headers=self._get_headers({'Referer':referorSMART_QQ_REFER}),)exceptException:error_msg=""Failedtosendrequestto`{0}`"".format(url)logger.exception(error_msg)returnerror_msgelse:self._cookies.save(COOKIE_FILE,ignore_discard=True,ignore_expires=True)returnresp.text",src/smart_qq_bot/http_client.py
airbnb/streamalert,eac5698f031cbe71289dc94ac7286cbebcbbf876,requests,urllib,"def _request_helper(url, data, headers=None, verify=True):
        """"""URL request helper to send a payload to an endpoint

        Args:
            url (str): Endpoint for this request
            data (str): Payload to send with this request
            headers (dict): Dictionary containing request-specific header parameters

            verify (bool): Whether or not SSL should be used for this request
        Returns:
            file handle: Contains the http response to be read
        """"""
        try:
            context = None
            if not verify:
                context = ssl.create_default_context()
                context.check_hostname = False
                context.verify_mode = ssl.CERT_NONE

            http_headers = headers or {}

            # Omitting data means a GET request should occur, not POST
            if not data:
                request = urllib2.Request(url, headers=http_headers)
            else:
                request = urllib2.Request(url, data=data, headers=http_headers)
            resp = urllib2.urlopen(request, context=context)
            return resp
        except urllib2.HTTPError as err:
            raise OutputRequestFailure('Failed to send to {} - [{}]'.format(err.url, err.code))","    def _get_request(url, params=None, headers=None, verify=True):
        """"""Method to return the json loaded response for this GET request

        Args:
            url (str): Endpoint for this request

            headers (dict): Dictionary containing request-specific header parameters
            params (dict): Payload to send with this request
            verify (bool): Whether or not SSL should be used for this request
        Returns:
            dict: Contains the http response object
        """"""
        return requests.get(url, headers=headers, params=params, verify=verify)















",update,"def_request_helper(url,data,headers=None,verify=True):""""""URLrequesthelpertosendapayloadtoanendpointArgs:url(str):Endpointforthisrequestdata(str):Payloadtosendwiththisrequestheaders(dict):Dictionarycontainingrequest-specificheaderparametersverify(bool):WhetherornotSSLshouldbeusedforthisrequestReturns:filehandle:Containsthehttpresponsetoberead""""""try:context=Noneifnotverify:context=ssl.create_default_context()context.check_hostname=Falsecontext.verify_mode=ssl.CERT_NONEhttp_headers=headersor{}#OmittingdatameansaGETrequestshouldoccur,notPOSTifnotdata:request=urllib2.Request(url,headers=http_headers)else:request=urllib2.Request(url,data=data,headers=http_headers)resp=urllib2.urlopen(request,context=context)returnrespexcepturllib2.HTTPErroraserr:raiseOutputRequestFailure('Failedtosendto{}-[{}]'.format(err.url,err.code))","def_get_request(url,params=None,headers=None,verify=True):""""""MethodtoreturnthejsonloadedresponseforthisGETrequestArgs:url(str):Endpointforthisrequestheaders(dict):Dictionarycontainingrequest-specificheaderparametersparams(dict):Payloadtosendwiththisrequestverify(bool):WhetherornotSSLshouldbeusedforthisrequestReturns:dict:Containsthehttpresponseobject""""""returnrequests.get(url,headers=headers,params=params,verify=verify)",stream_alert/alert_processor/output_base.py
airbnb/streamalert,eac5698f031cbe71289dc94ac7286cbebcbbf876,requests,urllib,," def _post_request(url, data=None, headers=None, verify=True):
        """"""Method to return the json loaded response for this POST request

        Args:
            url (str): Endpoint for this request
            headers (dict): Dictionary containing request-specific header parameters
            params (dict): Payload to send with this request
            verify (bool): Whether or not SSL should be used for this request
        Returns:
            dict: Contains the http response object
        """"""
        return requests.post(url, headers=headers, json=data, verify=verify)",insert,,"def_post_request(url,data=None,headers=None,verify=True):""""""MethodtoreturnthejsonloadedresponseforthisPOSTrequestArgs:url(str):Endpointforthisrequestheaders(dict):Dictionarycontainingrequest-specificheaderparametersparams(dict):Payloadtosendwiththisrequestverify(bool):WhetherornotSSLshouldbeusedforthisrequestReturns:dict:Containsthehttpresponseobject""""""returnrequests.post(url,headers=headers,json=data,verify=verify)",stream_alert/alert_processor/output_base.py
airbnb/streamalert,eac5698f031cbe71289dc94ac7286cbebcbbf876,requests,urllib,"    def _check_http_response(resp):
        return resp and (200 <= resp.getcode() <= 299)","   @staticmethod
    def _check_http_response(response):
        """"""Method for checking for a valid HTTP response code

        Args:
            response (requests.Response): Response object from requests

        Returns:
            bool: Indicator of whether or not this request was successful
        """"""
        return response is not None and (200 <= response.status_code <= 299)",update,def_check_http_response(resp):returnrespand(200<=resp.getcode()<=299),"@staticmethoddef_check_http_response(response):""""""MethodforcheckingforavalidHTTPresponsecodeArgs:response(requests.Response):ResponseobjectfromrequestsReturns:bool:Indicatorofwhetherornotthisrequestwassuccessful""""""returnresponseisnotNoneand(200<=response.status_code<=299)",stream_alert/alert_processor/output_base.py
akamhy/waybackpy,60ee8b95a872ef8987908163e85284aeec582435,requests,urllib,"def test_archive_url_parser():
    request_url = ""https://amazon.com""
    hdr = {""User-Agent"": user_agent}  # nosec
    req = Request(request_url, headers=hdr)  # nosec
    header = waybackpy._get_response(req).headers","def test_archive_url_parser():
    endpoint = ""https://amazon.com""
    user_agent = ""Mozilla/5.0 (Windows NT 6.2; rv:20.0) Gecko/20121202 Firefox/20.0""
    headers = {""User-Agent"": ""%s"" % user_agent}
    response = waybackpy._get_response(endpoint, params=None, headers=headers)
    header = response.headers",update,"deftest_archive_url_parser():request_url=""https://amazon.com""hdr={""User-Agent"":user_agent}#nosecreq=Request(request_url,headers=hdr)#nosecheader=waybackpy._get_response(req).headers","deftest_archive_url_parser():endpoint=""https://amazon.com""user_agent=""Mozilla/5.0(WindowsNT6.2;rv:20.0)Gecko/20121202Firefox/20.0""headers={""User-Agent"":""%s""%user_agent}response=waybackpy._get_response(endpoint,params=None,headers=headers)header=response.headers",tests/test_wrapper.py
akamhy/waybackpy,60ee8b95a872ef8987908163e85284aeec582435,requests,urllib,"def test_get_response():
    hdr = {
        ""User-Agent"": ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:78.0) ""
        ""Gecko/20100101 Firefox/78.0""
    }
    req = Request(""https://www.google.com"", headers=hdr)  # nosec
    response = waybackpy._get_response(req)
    assert response.code == 200","def test_get_response():
    endpoint = ""https://www.google.com""
    user_agent = ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:78.0) Gecko/20100101 Firefox/78.0""
    headers = {""User-Agent"": ""%s"" % user_agent}
    response = waybackpy._get_response(endpoint, params=None, headers=headers)
    assert response.status_code == 200",update,"deftest_get_response():hdr={""User-Agent"":""Mozilla/5.0(X11;Ubuntu;Linuxx86_64;rv:78.0)""""Gecko/20100101Firefox/78.0""}req=Request(""https://www.google.com"",headers=hdr)#nosecresponse=waybackpy._get_response(req)assertresponse.code==200","deftest_get_response():endpoint=""https://www.google.com""user_agent=""Mozilla/5.0(X11;Ubuntu;Linuxx86_64;rv:78.0)Gecko/20100101Firefox/78.0""headers={""User-Agent"":""%s""%user_agent}response=waybackpy._get_response(endpoint,params=None,headers=headers)assertresponse.status_code==200",tests/test_wrapper.py
akamhy/waybackpy,60ee8b95a872ef8987908163e85284aeec582435,requests,urllib,"def _get_response(req):
    """"""Get response for the supplied request.""""""

    try:
        response = urlopen(req)  # nosec
    except Exception:
        try:
            response = urlopen(req)  # nosec","def _get_response(endpoint, params=None, headers=None):
    """"""Get response for the supplied request.""""""

    try:
        response = requests.get(endpoint, params=params, headers=headers)
    except Exception:
        try:
            response = requests.get(endpoint, params=params, headers=headers)  # nosec",update,"def_get_response(req):""""""Getresponseforthesuppliedrequest.""""""try:response=urlopen(req)#nosecexceptException:try:response=urlopen(req)#nosec","def_get_response(endpoint,params=None,headers=None):""""""Getresponseforthesuppliedrequest.""""""try:response=requests.get(endpoint,params=params,headers=headers)exceptException:try:response=requests.get(endpoint,params=params,headers=headers)#nosec",waybackpy/wrapper.py
akamhy/waybackpy,60ee8b95a872ef8987908163e85284aeec582435,requests,urllib,"    def _JSON(self):
        request_url = ""https://archive.org/wayback/available?url=%s"" % (
            self._clean_url(),
        )

        hdr = {""User-Agent"": ""%s"" % self.user_agent}
        req = Request(request_url, headers=hdr)  # nosec
        response = _get_response(req)
        data_string = response.read().decode(""UTF-8"")
        data = json.loads(data_string)

        return data","    def _JSON(self):
        endpoint = ""https://archive.org/wayback/available""
        headers = {""User-Agent"": ""%s"" % self.user_agent}
        payload = {""url"": ""%s"" % self._clean_url()}
        response = _get_response(endpoint, params=payload, headers=headers)
        return response.json()





",update,"def_JSON(self):request_url=""https://archive.org/wayback/available?url=%s""%(self._clean_url(),)hdr={""User-Agent"":""%s""%self.user_agent}req=Request(request_url,headers=hdr)#nosecresponse=_get_response(req)data_string=response.read().decode(""UTF-8"")data=json.loads(data_string)returndata","def_JSON(self):endpoint=""https://archive.org/wayback/available""headers={""User-Agent"":""%s""%self.user_agent}payload={""url"":""%s""%self._clean_url()}response=_get_response(endpoint,params=payload,headers=headers)returnresponse.json()",waybackpy/wrapper.py
akamhy/waybackpy,60ee8b95a872ef8987908163e85284aeec582435,requests,urllib,"        hdr = {""User-Agent"": ""%s"" % self.user_agent}  # nosec
        req = Request(request_url, headers=hdr)  # nosec
        header = _get_response(req).headers
        self.archive_url = ""https://"" + _archive_url_parser(header)","        headers = {""User-Agent"": ""%s"" % self.user_agent}
        response = _get_response(request_url, params=None, headers=headers)
        self.archive_url = ""https://"" + _archive_url_parser(response.headers)",update,"hdr={""User-Agent"":""%s""%self.user_agent}#nosecreq=Request(request_url,headers=hdr)#nosecheader=_get_response(req).headersself.archive_url=""https://""+_archive_url_parser(header)","headers={""User-Agent"":""%s""%self.user_agent}response=_get_response(request_url,params=None,headers=headers)self.archive_url=""https://""+_archive_url_parser(response.headers)",waybackpy/wrapper.py
akamhy/waybackpy,60ee8b95a872ef8987908163e85284aeec582435,requests,urllib,"        hdr = {""User-Agent"": ""%s"" % user_agent}
        req = Request(url, headers=hdr)  # nosec
        response = _get_response(req)
        if not encoding:
            try:
                encoding = response.headers[""content-type""].split(""charset="")[-1]
            except AttributeError:
                encoding = ""UTF-8""
        return response.read().decode(encoding.replace(""text/html"", ""UTF-8"", 1))","        headers = {""User-Agent"": ""%s"" % self.user_agent}
        response = _get_response(url, params=None, headers=headers)

        if not encoding:
            try:
                encoding = response.encoding
            except AttributeError:
                encoding = ""UTF-8""

        return response.content.decode(encoding.replace(""text/html"", ""UTF-8"", 1))",update,"hdr={""User-Agent"":""%s""%user_agent}req=Request(url,headers=hdr)#nosecresponse=_get_response(req)ifnotencoding:try:encoding=response.headers[""content-type""].split(""charset="")[-1]exceptAttributeError:encoding=""UTF-8""returnresponse.read().decode(encoding.replace(""text/html"",""UTF-8"",1))","headers={""User-Agent"":""%s""%self.user_agent}response=_get_response(url,params=None,headers=headers)ifnotencoding:try:encoding=response.encodingexceptAttributeError:encoding=""UTF-8""returnresponse.content.decode(encoding.replace(""text/html"",""UTF-8"",1))",waybackpy/wrapper.py
akamhy/waybackpy,60ee8b95a872ef8987908163e85284aeec582435,requests,urllib,"        request_url = ""https://archive.org/wayback/available?url=%s&timestamp=%s"" % (
            self._clean_url(),
            timestamp,
        )
        hdr = {""User-Agent"": ""%s"" % self.user_agent}
        req = Request(request_url, headers=hdr)  # nosec
        response = _get_response(req)
        data = json.loads(response.read().decode(""UTF-8""))","
        endpoint = ""https://archive.org/wayback/available""
        headers = {""User-Agent"": ""%s"" % self.user_agent}
        payload = {""url"": ""%s"" % self._clean_url(), ""timestamp"" : timestamp}
        response = _get_response(endpoint, params=payload, headers=headers)
        print(response.text)
        data = response.json()
",update,"request_url=""https://archive.org/wayback/available?url=%s&timestamp=%s""%(self._clean_url(),timestamp,)hdr={""User-Agent"":""%s""%self.user_agent}req=Request(request_url,headers=hdr)#nosecresponse=_get_response(req)data=json.loads(response.read().decode(""UTF-8""))","endpoint=""https://archive.org/wayback/available""headers={""User-Agent"":""%s""%self.user_agent}payload={""url"":""%s""%self._clean_url(),""timestamp"":timestamp}response=_get_response(endpoint,params=payload,headers=headers)print(response.text)data=response.json()",waybackpy/wrapper.py
akamhy/waybackpy,60ee8b95a872ef8987908163e85284aeec582435,requests,urllib,"def total_archives(self):
        """"""Returns the total number of Wayback Machine archives for this URL.""""""
        hdr = {""User-Agent"": ""%s"" % self.user_agent}
        request_url = (
            ""https://web.archive.org/cdx/search/cdx?url=%s&output=json&fl=statuscode""
            % self._clean_url()
        )
        req = Request(request_url, headers=hdr)  # nosec
        response = _get_response(req)
        # Most efficient method to count number of archives (yet)
        return str(response.read()).count("","")","    def total_archives(self):
        """"""Returns the total number of Wayback Machine archives for this URL.""""""

        endpoint = ""https://web.archive.org/cdx/search/cdx""
        headers = {""User-Agent"": ""%s"" % self.user_agent, ""output"" : ""json"", ""fl"" : ""statuscode""}
        payload = {""url"": ""%s"" % self._clean_url()}
        response = _get_response(endpoint, params=payload, headers=headers)


        # Most efficient method to count number of archives (yet)
        return response.text.count("","")",update,"deftotal_archives(self):""""""ReturnsthetotalnumberofWaybackMachinearchivesforthisURL.""""""hdr={""User-Agent"":""%s""%self.user_agent}request_url=(""https://web.archive.org/cdx/search/cdx?url=%s&output=json&fl=statuscode""%self._clean_url())req=Request(request_url,headers=hdr)#nosecresponse=_get_response(req)#Mostefficientmethodtocountnumberofarchives(yet)returnstr(response.read()).count("","")","deftotal_archives(self):""""""ReturnsthetotalnumberofWaybackMachinearchivesforthisURL.""""""endpoint=""https://web.archive.org/cdx/search/cdx""headers={""User-Agent"":""%s""%self.user_agent,""output"":""json"",""fl"":""statuscode""}payload={""url"":""%s""%self._clean_url()}response=_get_response(endpoint,params=payload,headers=headers)#Mostefficientmethodtocountnumberofarchives(yet)returnresponse.text.count("","")",waybackpy/wrapper.py
akamhy/waybackpy,60ee8b95a872ef8987908163e85284aeec582435,requests,urllib,"        hdr = {""User-Agent"": ""%s"" % self.user_agent}
        req = Request(request_url, headers=hdr)  # nosec
        response = _get_response(req)

        data = json.loads(response.read().decode(""UTF-8""))","        headers = {""User-Agent"": ""%s"" % self.user_agent}
        response = _get_response(request_url, params=None, headers=headers)
        data = response.json()",update,"hdr={""User-Agent"":""%s""%self.user_agent}req=Request(request_url,headers=hdr)#nosecresponse=_get_response(req)data=json.loads(response.read().decode(""UTF-8""))","headers={""User-Agent"":""%s""%self.user_agent}response=_get_response(request_url,params=None,headers=headers)data=response.json()",waybackpy/wrapper.py
alexmojaki/birdseye,afff063fc3de2522e63cc385dabe7865a459a772,requests,urllib,"self.assertEqual(urlopen(Request('http://localhost:5000/kill', '')).read(),
                         'Server shutting down...')","        self.assertEqual(requests.post('http://localhost:5000/kill').text,
                         'Server shutting down...')",update,"self.assertEqual(urlopen(Request('http://localhost:5000/kill','')).read(),'Servershuttingdown...')","self.assertEqual(requests.post('http://localhost:5000/kill').text,'Servershuttingdown...')",tests/test_interface.py
armadillica/flamenco,26a19197d936638163d0e122fede9abeb61fbf31,requests,urllib," if post_params:
        params = urllib.urlencode(post_params)
        f = urllib.urlopen('http://' + ip_address + method, params)

    else:
        f = urllib.urlopen('http://' + ip_address + method)


    print('message sent, reply follows:')
    return f.read()","    if post_params:
        #params = urllib.urlencode(post_params)
        #f = urllib.urlopen('http://' + ip_address + method, params)
        r = requests.post('http://' + ip_address + method, data=post_params)
    else:
        #f = urllib.urlopen('http://' + ip_address + method)
        r = requests.get('http://' + ip_address + method)

    #print('message sent, reply follows:')
    #return f.read()
    return r.json()",update,"ifpost_params:params=urllib.urlencode(post_params)f=urllib.urlopen('http://'+ip_address+method,params)else:f=urllib.urlopen('http://'+ip_address+method)print('messagesent,replyfollows:')returnf.read()","ifpost_params:#params=urllib.urlencode(post_params)#f=urllib.urlopen('http://'+ip_address+method,params)r=requests.post('http://'+ip_address+method,data=post_params)else:#f=urllib.urlopen('http://'+ip_address+method)r=requests.get('http://'+ip_address+method)#print('messagesent,replyfollows:')#returnf.read()returnr.json()",brender/dashboard/__init__.py
at4260/inbestment,0149d4ee8393406303d974310b805fdb38f96e70,requests,urllib,"def load_ticker_data(ticker_url_list, session):
	beginning = time.time()
	for ticker_url in ticker_url_list:	
		u = urllib.urlopen(ticker_url)
		print ""@@@"", time.time() - beginning
		data = u.read()
		print ""###"", time.time() - beginning
		newdata = json.loads(data)

		ticker_symbol = (newdata[""code""].split(""_""))[1]","def load_ticker_data(ticker_url_list, session):

	for ticker_url in ticker_url_list:	
		u = requests.get(ticker_url)
		data = u.text


		newdata = json.loads(data)

		ticker_symbol = (newdata[""code""].split(""_""))[1]",update,"defload_ticker_data(ticker_url_list,session):beginning=time.time()forticker_urlinticker_url_list:u=urllib.urlopen(ticker_url)print""@@@"",time.time()-beginningdata=u.read()print""###"",time.time()-beginningnewdata=json.loads(data)ticker_symbol=(newdata[""code""].split(""_""))[1]","defload_ticker_data(ticker_url_list,session):forticker_urlinticker_url_list:u=requests.get(ticker_url)data=u.textnewdata=json.loads(data)ticker_symbol=(newdata[""code""].split(""_""))[1]",utils.py
aurelg/feedspora,f0a44c53f032610bdcd09514f868d59418543344,requests,urllib,"        except FileNotFoundError:
            logging.info(""File not found."")
            logging.info(""Trying to read %s as a URL."", feed_url)
            req = urllib.request.Request(
                url=feed_url,
                data=b'None',
                method='GET',
                headers={'User-Agent': self._ua})
            feed_content = urllib.request.urlopen(req).read()
        logging.info(""Feed read."")

        return BeautifulSoup(feed_content, 'html.parser')","
        except FileNotFoundError:
            logging.info(""File not found."")
            logging.info(""Trying to read %s as a URL."", feed_url)
            response = requests.get(feed_url, headers={'User-Agent': self._ua})

            if not response.ok:
                raise Exception(feed_content)
            feed_content = response.text

        logging.info(""Feed read."")

        return BeautifulSoup(feed_content, 'html.parser')",update,"exceptFileNotFoundError:logging.info(""Filenotfound."")logging.info(""Tryingtoread%sasaURL."",feed_url)req=urllib.request.Request(url=feed_url,data=b'None',method='GET',headers={'User-Agent':self._ua})feed_content=urllib.request.urlopen(req).read()logging.info(""Feedread."")returnBeautifulSoup(feed_content,'html.parser')","exceptFileNotFoundError:logging.info(""Filenotfound."")logging.info(""Tryingtoread%sasaURL."",feed_url)response=requests.get(feed_url,headers={'User-Agent':self._ua})ifnotresponse.ok:raiseException(feed_content)feed_content=response.textlogging.info(""Feedread."")returnBeautifulSoup(feed_content,'html.parser')",src/feedspora/feedspora_runner.py
aurelg/feedspora,f0a44c53f032610bdcd09514f868d59418543344,requests,urllib,"try:
            soup = self.retrieve_feed_soup(feed_url)
        except (urllib.error.HTTPError, ValueError, OSError,
                urllib.error.URLError) as error:
            logging.error(""Error while reading feed at %s: %s"", feed_url,
                          format(error))




            return","try:
            soup = self.retrieve_feed_soup(feed_url)
        except (requests.exceptions.ConnectionError, ValueError,
                OSError) as error:
            logging.error(
                ""Error while reading feed at %s: %s"",
                feed_url,
                format(error),
                exc_info=True)",update,"try:soup=self.retrieve_feed_soup(feed_url)except(urllib.error.HTTPError,ValueError,OSError,urllib.error.URLError)aserror:logging.error(""Errorwhilereadingfeedat%s:%s"",feed_url,format(error))return","try:soup=self.retrieve_feed_soup(feed_url)except(requests.exceptions.ConnectionError,ValueError,OSError)aserror:logging.error(""Errorwhilereadingfeedat%s:%s"",feed_url,format(error),exc_info=True)",src/feedspora/feedspora_runner.py
avwx-rest/AVWX-Engine,1e7ebbf7b3f229d0869ce78d6e288e2520363385,requests,urllib,"def checkPast2Weeks(station):
	try:
		if sys.version_info[0] == 2:
			response = urllib2.urlopen('http://www.aviationweather.gov/metar/data?ids='+station+'&format=raw&date=0&hours=336')
			html = response.read()
		elif sys.version_info[0] == 3:
			response = urlopen('http://www.aviationweather.gov/metar/data?ids='+station+'&format=raw&date=0&hours=336')
			html = response.read().decode('utf-8')","def checkPast2Weeks(station):
	try:
		url = 'http://www.aviationweather.gov/metar/data?ids='+station+'&format=raw&date=0&hours=336'
		html = requests.get(url).text



",update,defcheckPast2Weeks(station):try:ifsys.version_info[0]==2:response=urllib2.urlopen('http://www.aviationweather.gov/metar/data?ids='+station+'&format=raw&date=0&hours=336')html=response.read()elifsys.version_info[0]==3:response=urlopen('http://www.aviationweather.gov/metar/data?ids='+station+'&format=raw&date=0&hours=336')html=response.read().decode('utf-8'),defcheckPast2Weeks(station):try:url='http://www.aviationweather.gov/metar/data?ids='+station+'&format=raw&date=0&hours=336'html=requests.get(url).text,
avwx-rest/AVWX-Engine,1e7ebbf7b3f229d0869ce78d6e288e2520363385,requests,urllib,"def getMETAR(station):
	try:
		if sys.version_info[0] == 2:
			response = urllib2.urlopen('http://www.aviationweather.gov/metar/data?ids='+station+'&format=raw&date=0&hours=0')
			html = response.read()
		elif sys.version_info[0] == 3:
			response = urlopen('http://www.aviationweather.gov/metar/data?ids='+station+'&format=raw&date=0&hours=0')
			html = response.read().decode('utf-8')","def getMETAR(station):
	try:
		url = 'http://www.aviationweather.gov/metar/data?ids='+station+'&format=raw&date=0&hours=0'
		html = requests.get(url).text",update,defgetMETAR(station):try:ifsys.version_info[0]==2:response=urllib2.urlopen('http://www.aviationweather.gov/metar/data?ids='+station+'&format=raw&date=0&hours=0')html=response.read()elifsys.version_info[0]==3:response=urlopen('http://www.aviationweather.gov/metar/data?ids='+station+'&format=raw&date=0&hours=0')html=response.read().decode('utf-8'),defgetMETAR(station):try:url='http://www.aviationweather.gov/metar/data?ids='+station+'&format=raw&date=0&hours=0'html=requests.get(url).text,avwx.py
avwx-rest/AVWX-Engine,1e7ebbf7b3f229d0869ce78d6e288e2520363385,requests,urllib,"def getTAF(station):
	try:
		if sys.version_info[0] == 2:
			response = urllib2.urlopen('http://www.aviationweather.gov/taf/data?ids=' + station + '&format=raw&submit=Get+TAF+data')
			html = response.read()
		elif sys.version_info[0] == 3:
			response = urlopen('http://www.aviationweather.gov/taf/data?ids=' + station + '&format=raw&submit=Get+TAF+data')
			html = response.read().decode('utf-8')","def getTAF(station):
	try:
		url = 'http://www.aviationweather.gov/taf/data?ids=' + station + '&format=raw&submit=Get+TAF+data'
		html = requests.get(url).text",update,defgetTAF(station):try:ifsys.version_info[0]==2:response=urllib2.urlopen('http://www.aviationweather.gov/taf/data?ids='+station+'&format=raw&submit=Get+TAF+data')html=response.read()elifsys.version_info[0]==3:response=urlopen('http://www.aviationweather.gov/taf/data?ids='+station+'&format=raw&submit=Get+TAF+data')html=response.read().decode('utf-8'),defgetTAF(station):try:url='http://www.aviationweather.gov/taf/data?ids='+station+'&format=raw&submit=Get+TAF+data'html=requests.get(url).text,avwx.py
banbanchs/pan-baidu-download,7bfc0d3d45b47198193d4171fca26e8e6325136b,requests,urllib,"        self.cj = cookielib.LWPCookieJar(cookie_filename)
        self.opener = urllib2.build_opener(
            urllib2.HTTPRedirectHandler(),
            urllib2.HTTPHandler(),
            urllib2.HTTPCookieProcessor(self.cj)
        )","        self.session = Session()




",update,"self.cj=cookielib.LWPCookieJar(cookie_filename)self.opener=urllib2.build_opener(urllib2.HTTPRedirectHandler(),urllib2.HTTPHandler(),urllib2.HTTPCookieProcessor(self.cj))",self.session=Session(),
banbanchs/pan-baidu-download,7bfc0d3d45b47198193d4171fca26e8e6325136b,requests,urllib," def _get_baidu_uid(self):
        """"""Get BAIDUID.""""""
        self.opener.open('http://www.baidu.com')
        for cookie in self.cj:
            if cookie.name == 'BAIDUID':
                self.baiduid = cookie.value
        log_message = {'type': 'baidu uid', 'method': 'GET'}
        logger.debug(self.baiduid, extra=log_message)"," def _get_baidu_uid(self):
        """"""Get BAIDUID.""""""
        self.session.get('http://www.baidu.com')
        self.baiduid = self.session.cookies.get('BAIDUID')


        log_message = {'type': 'baidu uid', 'method': 'GET'}
        logger.debug(self.baiduid, extra=log_message)",update,"def_get_baidu_uid(self):""""""GetBAIDUID.""""""self.opener.open('http://www.baidu.com')forcookieinself.cj:ifcookie.name=='BAIDUID':self.baiduid=cookie.valuelog_message={'type':'baiduuid','method':'GET'}logger.debug(self.baiduid,extra=log_message)","def_get_baidu_uid(self):""""""GetBAIDUID.""""""self.session.get('http://www.baidu.com')self.baiduid=self.session.cookies.get('BAIDUID')log_message={'type':'baiduuid','method':'GET'}logger.debug(self.baiduid,extra=log_message)",command/login.py
banbanchs/pan-baidu-download,7bfc0d3d45b47198193d4171fca26e8e6325136b,requests,urllib," def _check_verify_code(self):
        """"""Check if login need to input verify code.""""""
        r = self.opener.open(self._check_url)
        s = r.read()
        data = json.loads(s[s.index('{'):-1])
        log_message = {'type': 'check loging verify code', 'method': 'GET'}
        logger.debug(data, extra=log_message)","def _check_verify_code(self):
        """"""Check if login need to input verify code.""""""
        r = self.session.get(self._check_url)
        s = r.text
        data = json.loads(s[s.index('{'):-1])
        log_message = {'type': 'check loging verify code', 'method': 'GET'}
        logger.debug(data, extra=log_message)",update,"def_check_verify_code(self):""""""Checkifloginneedtoinputverifycode.""""""r=self.opener.open(self._check_url)s=r.read()data=json.loads(s[s.index('{'):-1])log_message={'type':'checklogingverifycode','method':'GET'}logger.debug(data,extra=log_message)","def_check_verify_code(self):""""""Checkifloginneedtoinputverifycode.""""""r=self.session.get(self._check_url)s=r.textdata=json.loads(s[s.index('{'):-1])log_message={'type':'checklogingverifycode','method':'GET'}logger.debug(data,extra=log_message)",command/login.py
banbanchs/pan-baidu-download,7bfc0d3d45b47198193d4171fca26e8e6325136b,requests,urllib,"def _get_token(self):
        """"""Get bdstoken.""""""
        r = self.opener.open(self._token_url)
        s = r.read()
        try:
            self.token = re.search(""login_token='(\w+)';"", s).group(1)

            log_message = {'type': 'bdstoken', 'method': 'GET'}
            logger.debug(self.token, extra=log_message)
        except:","def _get_token(self):
        """"""Get bdstoken.""""""
        r = self.session.get(self._token_url)
        s = r.text
        try:
            self.token = re.search(""login_token='(\w+)';"", s).group(1)
            # FIXME: if couldn't get the token, we can not get the log message.
            log_message = {'type': 'bdstoken', 'method': 'GET'}
            logger.debug(self.token, extra=log_message)
        except:",update,"def_get_token(self):""""""Getbdstoken.""""""r=self.opener.open(self._token_url)s=r.read()try:self.token=re.search(""login_token='(\w+)';"",s).group(1)log_message={'type':'bdstoken','method':'GET'}logger.debug(self.token,extra=log_message)except:","def_get_token(self):""""""Getbdstoken.""""""r=self.session.get(self._token_url)s=r.texttry:self.token=re.search(""login_token='(\w+)';"",s).group(1)#FIXME:ifcouldn'tgetthetoken,wecannotgetthelogmessage.log_message={'type':'bdstoken','method':'GET'}logger.debug(self.token,extra=log_message)except:",command/login.py
banbanchs/pan-baidu-download,7bfc0d3d45b47198193d4171fca26e8e6325136b,requests,urllib,"def _post_data(self):
        """"""Post login form.""""""
        post_data = {'ppui_logintime': '9379', 'charset': 'utf-8', 'codestring': '', 'token': self.token,
                     'isPhone': 'false', 'index': '0', 'u': '', 'safeflg': 0,
                     'staticpage': 'http://www.baidu.com/cache/user/html/jump.html', 'loginType': '1', 'tpl': 'mn',
                     'callback': 'parent.bdPass.api.login._postCallback', 'username': self.username,
                     'password': self.passwd, 'verifycode': '', 'mem_pass': 'on'}
        post_data = urlencode(post_data)
        log_message = {'type': 'login post data', 'method': 'POST'}
        logger.debug(post_data, extra=log_message)
        response = self.opener.open(self._post_url, data=post_data).read()

        log_message = {'type': 'response', 'method': 'POST'}
        logger.debug(response, extra=log_message)
        for cookie in self.cj:
            if cookie.name == 'BDUSS':
                self.bduss = cookie.value
        log_message = {'type': 'BDUSS', 'method': 'GET'}
        logger.debug(self.bduss, extra=log_message)
        return response"," def _post_data(self):
        """"""Post login form.""""""
        post_data = {'ppui_logintime': '9379', 'charset': 'utf-8', 'codestring': '', 'token': self.token,
                     'isPhone': 'false', 'index': '0', 'u': '', 'safeflg': 0,
                     'staticpage': 'http://www.baidu.com/cache/user/html/jump.html', 'loginType': '1', 'tpl': 'mn',
                     'callback': 'parent.bdPass.api.login._postCallback', 'username': self.username,
                     'password': self.passwd, 'verifycode': '', 'mem_pass': 'on'}
        # post_data = urlencode(post_data)
        log_message = {'type': 'login post data', 'method': 'POST'}
        logger.debug(post_data, extra=log_message)
        response = self.session.post(self._post_url, data=post_data)
        s = response.text
        log_message = {'type': 'response', 'method': 'POST'}
        logger.debug(s, extra=log_message)
        self.bduss = response.cookies.get(""BDUSS"")


        log_message = {'type': 'BDUSS', 'method': 'GET'}
        logger.debug(self.bduss, extra=log_message)
        return s",update,"def_post_data(self):""""""Postloginform.""""""post_data={'ppui_logintime':'9379','charset':'utf-8','codestring':'','token':self.token,'isPhone':'false','index':'0','u':'','safeflg':0,'staticpage':'http://www.baidu.com/cache/user/html/jump.html','loginType':'1','tpl':'mn','callback':'parent.bdPass.api.login._postCallback','username':self.username,'password':self.passwd,'verifycode':'','mem_pass':'on'}post_data=urlencode(post_data)log_message={'type':'loginpostdata','method':'POST'}logger.debug(post_data,extra=log_message)response=self.opener.open(self._post_url,data=post_data).read()log_message={'type':'response','method':'POST'}logger.debug(response,extra=log_message)forcookieinself.cj:ifcookie.name=='BDUSS':self.bduss=cookie.valuelog_message={'type':'BDUSS','method':'GET'}logger.debug(self.bduss,extra=log_message)returnresponse","def_post_data(self):""""""Postloginform.""""""post_data={'ppui_logintime':'9379','charset':'utf-8','codestring':'','token':self.token,'isPhone':'false','index':'0','u':'','safeflg':0,'staticpage':'http://www.baidu.com/cache/user/html/jump.html','loginType':'1','tpl':'mn','callback':'parent.bdPass.api.login._postCallback','username':self.username,'password':self.passwd,'verifycode':'','mem_pass':'on'}#post_data=urlencode(post_data)log_message={'type':'loginpostdata','method':'POST'}logger.debug(post_data,extra=log_message)response=self.session.post(self._post_url,data=post_data)s=response.textlog_message={'type':'response','method':'POST'}logger.debug(s,extra=log_message)self.bduss=response.cookies.get(""BDUSS"")log_message={'type':'BDUSS','method':'GET'}logger.debug(self.bduss,extra=log_message)returns",command/login.py
banbanchs/pan-baidu-download,7bfc0d3d45b47198193d4171fca26e8e6325136b,requests,urllib," def load_cookies_from_file(self):
        """"""Load cookies file if file exist.""""""
        if os.access(self.cookie_filename, os.F_OK):
            self.cj.load()
            for cookie in self.cj:
                if cookie.name == 'BAIDUID':
                    self.baiduid = cookie.value
                elif cookie.name == 'BDUSS':
                    self.bduss = cookie.value","    def load_cookies_from_file(self):
        """"""Load cookies file if file exist.""""""
        if os.access(self.cookie_filename, os.F_OK):
            with open(self.cookie_filename) as f:
                cookies = requests.utils.cookiejar_from_dict(pickle.load(f))
            self.session.cookies = cookies
            # NOT SURE stoken is bdstoken!
            # self.token = self.session.cookies.get('STOKEN')
            self.baiduid = self.session.cookies.get('BAIDUID')
            self.bduss = self.session.cookies.get('BDUSS')",update,"defload_cookies_from_file(self):""""""Loadcookiesfileiffileexist.""""""ifos.access(self.cookie_filename,os.F_OK):self.cj.load()forcookieinself.cj:ifcookie.name=='BAIDUID':self.baiduid=cookie.valueelifcookie.name=='BDUSS':self.bduss=cookie.value","defload_cookies_from_file(self):""""""Loadcookiesfileiffileexist.""""""ifos.access(self.cookie_filename,os.F_OK):withopen(self.cookie_filename)asf:cookies=requests.utils.cookiejar_from_dict(pickle.load(f))self.session.cookies=cookies#NOTSUREstokenisbdstoken!#self.token=self.session.cookies.get('STOKEN')self.baiduid=self.session.cookies.get('BAIDUID')self.bduss=self.session.cookies.get('BDUSS')",command/login.py
braiden/python-ant-downloader,8291c453890ea25c0b5a5d41f7335b42db978064,requests,urllib,,"    # work around old versions of requests
    def get_response_text(self, response):
        return response.text if hasattr(response, ""text"") else response.content",insert,,"#workaroundoldversionsofrequestsdefget_response_text(self,response):returnresponse.textifhasattr(response,""text"")elseresponse.content",antd/connect.py
braiden/python-ant-downloader,8291c453890ea25c0b5a5d41f7335b42db978064,requests,urllib," def login(self):
        if self.logged_in: return
        if self.login_invalid: raise InvalidLogin()
        # get session cookies
        _log.debug(""Fetching cookies from Garmin Connect."")
        self.opener.open(""http://connect.garmin.com/signin"")
        # build the login string
        login_dict = {
            ""login"": ""login"",
            ""login:loginUsernameField"": self.username,
            ""login:password"": self.password,
            ""login:signInButton"": ""Sign In"",
            ""javax.faces.ViewState"": ""j_id1"",
        }
        login_str = urllib.urlencode(login_dict)
        # post login credentials
        _log.debug(""Posting login credentials to Garmin Connect. username=%s"", self.username)
        self.opener.open(""https://connect.garmin.com/signin"", login_str)
        # verify we're logged in
        _log.debug(""Checking if login was successful."")
        reply = self.opener.open(""http://connect.garmin.com/user/username"")
        username = json.loads(reply.read())[""username""]
        if username == """":
            self.login_invalid = True
            raise InvalidLogin()
        elif username != self.username:
            _log.warning(""Username mismatch, probably OK, if upload fails check user/pass. %s != %s"" % (username, self.username))














































































","def login(self):
        if self.logged_in: return
        if self.login_invalid: raise InvalidLogin()
        
        # Use a session, removes the need to manage cookies ourselves
        self.rsession = requests.Session()
        
        _log.debug(""Checking to see what style of login to use for Garmin Connect."")
        #Login code taken almost directly from https://github.com/cpfair/tapiriik/
        self._rate_limit()
        gcPreResp = self.rsession.get(""http://connect.garmin.com/"", allow_redirects=False)
        # New site gets this redirect, old one does not
        if gcPreResp.status_code == 200:
            _log.debug(""Using old login style"")
            params = {""login"": ""login"", ""login:loginUsernameField"": self.username, ""login:password"": self.password, ""login:signInButton"": ""Sign In"", ""javax.faces.ViewState"": ""j_id1""}
            auth_retries = 3 # Did I mention Garmin Connect is silly?
            for retries in range(auth_retries):
                self._rate_limit()
                resp = self.rsession.post(""https://connect.garmin.com/signin"", data=params, allow_redirects=False, cookies=gcPreResp.cookies)
                if resp.status_code >= 500 and resp.status_code < 600:
                    raise APIException(""Remote API failure"")
                if resp.status_code != 302:  # yep
                    if ""errorMessage"" in self.get_response_text(resp):
                        if retries < auth_retries - 1:
                            time.sleep(1)
                            continue
                        else:
                            login_invalid = True
                            raise APIException(""Invalid login"", block=True, user_exception=UserException(UserExceptionType.Authorization, intervention_required=True))
                    else:
                        raise APIException(""Mystery login error %s"" % self.get_response_text(resp))
                _log.debug(""Old style login complete"")
                break
        elif gcPreResp.status_code == 302:
            _log.debug(""Using new style login"")
            # JSIG CAS, cool I guess.
            # Not quite OAuth though, so I'll continue to collect raw credentials.
            # Commented stuff left in case this ever breaks because of missing parameters...
            data = {
                ""username"": self.username,
                ""password"": self.password,
                ""_eventId"": ""submit"",
                ""embed"": ""true"",
                # ""displayNameRequired"": ""false""
            }
            params = {
                ""service"": ""http://connect.garmin.com/post-auth/login"",
                # ""redirectAfterAccountLoginUrl"": ""http://connect.garmin.com/post-auth/login"",
                # ""redirectAfterAccountCreationUrl"": ""http://connect.garmin.com/post-auth/login"",
                # ""webhost"": ""olaxpw-connect00.garmin.com"",
                ""clientId"": ""GarminConnect"",
                # ""gauthHost"": ""https://sso.garmin.com/sso"",
                # ""rememberMeShown"": ""true"",
                # ""rememberMeChecked"": ""false"",
                ""consumeServiceTicket"": ""false"",
                # ""id"": ""gauth-widget"",
                # ""embedWidget"": ""false"",
                # ""cssUrl"": ""https://static.garmincdn.com/com.garmin.connect/ui/src-css/gauth-custom.css"",
                # ""source"": ""http://connect.garmin.com/en-US/signin"",
                # ""createAccountShown"": ""true"",
                # ""openCreateAccount"": ""false"",
                # ""usernameShown"": ""true"",
                # ""displayNameShown"": ""false"",
                # ""initialFocus"": ""true"",
                # ""locale"": ""en""
            }
            _log.debug(""Fetching login variables"")
            
            # I may never understand what motivates people to mangle a perfectly good protocol like HTTP in the ways they do...
            preResp = self.rsession.get(""https://sso.garmin.com/sso/login"", params=params)
            if preResp.status_code != 200:
                raise APIException(""SSO prestart error %s %s"" % (preResp.status_code, self.get_response_text(preResp)))
            data[""lt""] = re.search(""name=\""lt\""\s+value=\""([^\""]+)\"""", self.get_response_text(preResp)).groups(1)[0]
            _log.debug(""lt=%s""%data[""lt""])

            _log.debug(""Posting login credentials to Garmin Connect. username=%s"", self.username)
            ssoResp = self.rsession.post(""https://sso.garmin.com/sso/login"", params=params, data=data, allow_redirects=False)
            if ssoResp.status_code != 200:
                login_invalid = True
                _log.error(""Login failed"")
                raise APIException(""SSO error %s %s"" % (ssoResp.status_code, self.get_response_text(ssoResp)))

            ticket_match = re.search(""ticket=([^']+)'"", self.get_response_text(ssoResp))
            if not ticket_match:
                login_invalid = True
                raise APIException(""Invalid login"", block=True, user_exception=UserException(UserExceptionType.Authorization, intervention_required=True))
            ticket = ticket_match.groups(1)[0]

            # ...AND WE'RE NOT DONE YET!

            _log.debug(""Post login step 1"")
            self._rate_limit()
            gcRedeemResp1 = self.rsession.get(""http://connect.garmin.com/post-auth/login"", params={""ticket"": ticket}, allow_redirects=False)
            if gcRedeemResp1.status_code != 302:
                raise APIException(""GC redeem 1 error %s %s"" % (gcRedeemResp1.status_code, self.get_response_text(gcRedeemResp1)))

            _log.debug(""Post login step 2"")
            self._rate_limit()
            gcRedeemResp2 = self.rsession.get(gcRedeemResp1.headers[""location""], allow_redirects=False)
            if gcRedeemResp2.status_code != 302:
                raise APIException(""GC redeem 2 error %s %s"" % (gcRedeemResp2.status_code, self.get_response_text(gcRedeemResp2)))

        else:
            raise APIException(""Unknown GC prestart response %s %s"" % (gcPreResp.status_code, self.get_response_text(gcPreResp)))",update,"deflogin(self):ifself.logged_in:returnifself.login_invalid:raiseInvalidLogin()#getsessioncookies_log.debug(""FetchingcookiesfromGarminConnect."")self.opener.open(""http://connect.garmin.com/signin"")#buildtheloginstringlogin_dict={""login"":""login"",""login:loginUsernameField"":self.username,""login:password"":self.password,""login:signInButton"":""SignIn"",""javax.faces.ViewState"":""j_id1"",}login_str=urllib.urlencode(login_dict)#postlogincredentials_log.debug(""PostinglogincredentialstoGarminConnect.username=%s"",self.username)self.opener.open(""https://connect.garmin.com/signin"",login_str)#verifywe'reloggedin_log.debug(""Checkingifloginwassuccessful."")reply=self.opener.open(""http://connect.garmin.com/user/username"")username=json.loads(reply.read())[""username""]ifusername=="""":self.login_invalid=TrueraiseInvalidLogin()elifusername!=self.username:_log.warning(""Usernamemismatch,probablyOK,ifuploadfailscheckuser/pass.%s!=%s""%(username,self.username))","deflogin(self):ifself.logged_in:returnifself.login_invalid:raiseInvalidLogin()#Useasession,removestheneedtomanagecookiesourselvesself.rsession=requests.Session()_log.debug(""CheckingtoseewhatstyleoflogintouseforGarminConnect."")#Logincodetakenalmostdirectlyfromhttps://github.com/cpfair/tapiriik/self._rate_limit()gcPreResp=self.rsession.get(""http://connect.garmin.com/"",allow_redirects=False)#Newsitegetsthisredirect,oldonedoesnotifgcPreResp.status_code==200:_log.debug(""Usingoldloginstyle"")params={""login"":""login"",""login:loginUsernameField"":self.username,""login:password"":self.password,""login:signInButton"":""SignIn"",""javax.faces.ViewState"":""j_id1""}auth_retries=3#DidImentionGarminConnectissilly?forretriesinrange(auth_retries):self._rate_limit()resp=self.rsession.post(""https://connect.garmin.com/signin"",data=params,allow_redirects=False,cookies=gcPreResp.cookies)ifresp.status_code>=500andresp.status_code<600:raiseAPIException(""RemoteAPIfailure"")ifresp.status_code!=302:#yepif""errorMessage""inself.get_response_text(resp):ifretries<auth_retries-1:time.sleep(1)continueelse:login_invalid=TrueraiseAPIException(""Invalidlogin"",block=True,user_exception=UserException(UserExceptionType.Authorization,intervention_required=True))else:raiseAPIException(""Mysteryloginerror%s""%self.get_response_text(resp))_log.debug(""Oldstylelogincomplete"")breakelifgcPreResp.status_code==302:_log.debug(""Usingnewstylelogin"")#JSIGCAS,coolIguess.#NotquiteOAuththough,soI'llcontinuetocollectrawcredentials.#Commentedstuffleftincasethiseverbreaksbecauseofmissingparameters...data={""username"":self.username,""password"":self.password,""_eventId"":""submit"",""embed"":""true"",#""displayNameRequired"":""false""}params={""service"":""http://connect.garmin.com/post-auth/login"",#""redirectAfterAccountLoginUrl"":""http://connect.garmin.com/post-auth/login"",#""redirectAfterAccountCreationUrl"":""http://connect.garmin.com/post-auth/login"",#""webhost"":""olaxpw-connect00.garmin.com"",""clientId"":""GarminConnect"",#""gauthHost"":""https://sso.garmin.com/sso"",#""rememberMeShown"":""true"",#""rememberMeChecked"":""false"",""consumeServiceTicket"":""false"",#""id"":""gauth-widget"",#""embedWidget"":""false"",#""cssUrl"":""https://static.garmincdn.com/com.garmin.connect/ui/src-css/gauth-custom.css"",#""source"":""http://connect.garmin.com/en-US/signin"",#""createAccountShown"":""true"",#""openCreateAccount"":""false"",#""usernameShown"":""true"",#""displayNameShown"":""false"",#""initialFocus"":""true"",#""locale"":""en""}_log.debug(""Fetchingloginvariables"")#ImayneverunderstandwhatmotivatespeopletomangleaperfectlygoodprotocollikeHTTPinthewaystheydo...preResp=self.rsession.get(""https://sso.garmin.com/sso/login"",params=params)ifpreResp.status_code!=200:raiseAPIException(""SSOprestarterror%s%s""%(preResp.status_code,self.get_response_text(preResp)))data[""lt""]=re.search(""name=\""lt\""\s+value=\""([^\""]+)\"""",self.get_response_text(preResp)).groups(1)[0]_log.debug(""lt=%s""%data[""lt""])_log.debug(""PostinglogincredentialstoGarminConnect.username=%s"",self.username)ssoResp=self.rsession.post(""https://sso.garmin.com/sso/login"",params=params,data=data,allow_redirects=False)ifssoResp.status_code!=200:login_invalid=True_log.error(""Loginfailed"")raiseAPIException(""SSOerror%s%s""%(ssoResp.status_code,self.get_response_text(ssoResp)))ticket_match=re.search(""ticket=([^']+)'"",self.get_response_text(ssoResp))ifnotticket_match:login_invalid=TrueraiseAPIException(""Invalidlogin"",block=True,user_exception=UserException(UserExceptionType.Authorization,intervention_required=True))ticket=ticket_match.groups(1)[0]#...ANDWE'RENOTDONEYET!_log.debug(""Postloginstep1"")self._rate_limit()gcRedeemResp1=self.rsession.get(""http://connect.garmin.com/post-auth/login"",params={""ticket"":ticket},allow_redirects=False)ifgcRedeemResp1.status_code!=302:raiseAPIException(""GCredeem1error%s%s""%(gcRedeemResp1.status_code,self.get_response_text(gcRedeemResp1)))_log.debug(""Postloginstep2"")self._rate_limit()gcRedeemResp2=self.rsession.get(gcRedeemResp1.headers[""location""],allow_redirects=False)ifgcRedeemResp2.status_code!=302:raiseAPIException(""GCredeem2error%s%s""%(gcRedeemResp2.status_code,self.get_response_text(gcRedeemResp2)))else:raiseAPIException(""UnknownGCprestartresponse%s%s""%(gcPreResp.status_code,self.get_response_text(gcPreResp)))",antd/connect.py
braiden/python-ant-downloader,8291c453890ea25c0b5a5d41f7335b42db978064,requests,urllib," def upload(self, format, file_name):
        import poster.encode
        with open(file_name) as file:
            upload_dict = {
                ""responseContentType"": ""text/html"",
                ""data"": file,
            }
            data, headers = poster.encode.multipart_encode(upload_dict)
            _log.info(""Uploading %s to Garmin Connect."", file_name) 
            request = urllib2.Request(""http://connect.garmin.com/proxy/upload-service-1.1/json/upload/.%s"" % format, data, headers)
            self.opener.open(request)","def upload(self, format, file_name):
        #TODO: Restore streaming for upload
        with open(file_name) as file:
            files = {'file': file}




            _log.info(""Uploading %s to Garmin Connect."", file_name) 
            r = self.rsession.post(""http://connect.garmin.com/proxy/upload-service-1.1/json/upload/.%s"" % format, files=files)",update,"defupload(self,format,file_name):importposter.encodewithopen(file_name)asfile:upload_dict={""responseContentType"":""text/html"",""data"":file,}data,headers=poster.encode.multipart_encode(upload_dict)_log.info(""Uploading%stoGarminConnect."",file_name)request=urllib2.Request(""http://connect.garmin.com/proxy/upload-service-1.1/json/upload/.%s""%format,data,headers)self.opener.open(request)","defupload(self,format,file_name):#TODO:Restorestreamingforuploadwithopen(file_name)asfile:files={'file':file}_log.info(""Uploading%stoGarminConnect."",file_name)r=self.rsession.post(""http://connect.garmin.com/proxy/upload-service-1.1/json/upload/.%s""%format,files=files)",antd/connect.py
bvanheu/pytoutv,b8372496f000675dbf20e811ce1b1fadf89e68ac,requests,urllib,"def get_episode_playlist_url(self, episode):
        url = toutv.config.TOUTV_PLAYLIST_URL_TMPL.format(episode.PID)
        headers = {'User-Agent': toutv.config.USER_AGENT}
        req = urllib.request.Request(url, None, headers)
        json_string = urllib.request.urlopen(req).read().decode('utf-8')
        response = json.loads(json_string)


        if response['errorCode']:
            raise RuntimeError(response['message'])

        return response['url']","def get_episode_playlist_url(self, episode):
        url = toutv.config.TOUTV_PLAYLIST_URL
        headers = {
            'User-Agent': toutv.config.USER_AGENT
        }
        params = dict(toutv.config.TOUTV_PLAYLIST_PARAMS)
        params['idMedia'] = episode.PID

        r = requests.get(url, params=params, headers=headers)
        response_obj = r.json()

        if response_obj['errorCode']:
            raise RuntimeError(response_obj['message'])

        return response_obj['url']",update,"defget_episode_playlist_url(self,episode):url=toutv.config.TOUTV_PLAYLIST_URL_TMPL.format(episode.PID)headers={'User-Agent':toutv.config.USER_AGENT}req=urllib.request.Request(url,None,headers)json_string=urllib.request.urlopen(req).read().decode('utf-8')response=json.loads(json_string)ifresponse['errorCode']:raiseRuntimeError(response['message'])returnresponse['url']","defget_episode_playlist_url(self,episode):url=toutv.config.TOUTV_PLAYLIST_URLheaders={'User-Agent':toutv.config.USER_AGENT}params=dict(toutv.config.TOUTV_PLAYLIST_PARAMS)params['idMedia']=episode.PIDr=requests.get(url,params=params,headers=headers)response_obj=r.json()ifresponse_obj['errorCode']:raiseRuntimeError(response_obj['message'])returnresponse_obj['url']",toutv/client.py
bvanheu/pytoutv,b8372496f000675dbf20e811ce1b1fadf89e68ac,requests,urllib,"def _do_query(self, method, parameters={}):
        parameters_str = urllib.parse.urlencode(parameters)
        url = ''.join([
            toutv.config.TOUTV_JSON_URL,
            method,
            '?',
        parameters_str])
        headers = {'User-Agent': toutv.config.USER_AGENT}
        request = urllib.request.Request(url, None, headers)
        json_string = urllib.request.urlopen(request).read().decode('utf-8')
        json_decoded = self.json_decoder.decode(json_string)
        return json_decoded['d']","    def _do_query(self, endpoint, params={}):
        url = '{}{}'.format(toutv.config.TOUTV_JSON_URL, endpoint)
        headers = {
            'User-Agent': toutv.config.USER_AGENT
        }

        r = requests.get(url, params=params, headers=headers)
        response_obj = r.json()

        return response_obj['d']

",update,"def_do_query(self,method,parameters={}):parameters_str=urllib.parse.urlencode(parameters)url=''.join([toutv.config.TOUTV_JSON_URL,method,'?',parameters_str])headers={'User-Agent':toutv.config.USER_AGENT}request=urllib.request.Request(url,None,headers)json_string=urllib.request.urlopen(request).read().decode('utf-8')json_decoded=self.json_decoder.decode(json_string)returnjson_decoded['d']","def_do_query(self,endpoint,params={}):url='{}{}'.format(toutv.config.TOUTV_JSON_URL,endpoint)headers={'User-Agent':toutv.config.USER_AGENT}r=requests.get(url,params=params,headers=headers)response_obj=r.json()returnresponse_obj['d']",toutv/transport.py
callahantiff/PheKnowLator,77a902e5ca8426ef17ee454f13952c260127aa3f,requests,urllib,"def gets_json_results_from_api_call(url):
    """"""Function makes API requests and returns results as a json file. API documentation can be found here:
    http://data.bioontology.org/documentation.


    Args:
        url (str): A string containing a URL to be run against an API.


    Return:
        A json-formatted file of API results.

    Raises:
        An exception is raised if a 500 HTTP server-side code is raised.

    """"""

    try:
        # to ease rate limiting sleep for random amount of time between 10-60 seconds
        opener = urllib.request.build_opener()

    except HTTPError:
        # pause for 1 minutes and try again
        time.sleep(30)
        opener = urllib.request.build_opener()

    # fetch data
    opener.addheaders = [('Authorization', 'apikey token=' + open('resources/bioportal_api_key.txt').read())]

    return json.loads(opener.open(url).read())","def gets_json_results_from_api_call(url: str, api_key: str):
    """"""Function makes API requests and returns results as a json file. API documentation can be found here:
    http://data.bioontology.org/documentation. If a 500 HTTP server-side code is return from ""status_code"" then the
    algorithm pauses for 30 seconds before trying the request again.

    Args:
        url: A string containing a URL to be run against an API.
        api_key: A string containing an API key.

    Return:
        A json-formatted file containing API results.

    Raises:
        An exception is raised if a 500 HTTP server-side code is raised.

    """"""

    response = requests.get(url, headers={'Authorization': 'apikey token=' + api_key})



    if response.status_code == 500:
        # to ease rate limiting sleep for random amount of time between 10-60 seconds
        time.sleep(30)
        response = requests.get(url, headers={'Authorization': 'apikey token=' + api_key})

    return json.loads(response.text)",update,"defgets_json_results_from_api_call(url):""""""FunctionmakesAPIrequestsandreturnsresultsasajsonfile.APIdocumentationcanbefoundhere:http://data.bioontology.org/documentation.Args:url(str):AstringcontainingaURLtoberunagainstanAPI.Return:Ajson-formattedfileofAPIresults.Raises:Anexceptionisraisedifa500HTTPserver-sidecodeisraised.""""""try:#toeaseratelimitingsleepforrandomamountoftimebetween10-60secondsopener=urllib.request.build_opener()exceptHTTPError:#pausefor1minutesandtryagaintime.sleep(30)opener=urllib.request.build_opener()#fetchdataopener.addheaders=[('Authorization','apikeytoken='+open('resources/bioportal_api_key.txt').read())]returnjson.loads(opener.open(url).read())","defgets_json_results_from_api_call(url:str,api_key:str):""""""FunctionmakesAPIrequestsandreturnsresultsasajsonfile.APIdocumentationcanbefoundhere:http://data.bioontology.org/documentation.Ifa500HTTPserver-sidecodeisreturnfrom""status_code""thenthealgorithmpausesfor30secondsbeforetryingtherequestagain.Args:url:AstringcontainingaURLtoberunagainstanAPI.api_key:AstringcontaininganAPIkey.Return:Ajson-formattedfilecontainingAPIresults.Raises:Anexceptionisraisedifa500HTTPserver-sidecodeisraised.""""""response=requests.get(url,headers={'Authorization':'apikeytoken='+api_key})ifresponse.status_code==500:#toeaseratelimitingsleepforrandomamountoftimebetween10-60secondstime.sleep(30)response=requests.get(url,headers={'Authorization':'apikeytoken='+api_key})returnjson.loads(response.text)",scripts/python/ncbo_rest_api.py
canonical/cloud-init,0fc887d97626132e9024490b271888bed162c867,requests,urllib,"def readurl(url, data=None, timeout=None, retries=0,
            headers=None, ssl_details=None):
    req_args = {}
    p_url = util.parse_url(url)
    if p_url.scheme == 'https' and ssl_details:
        for k in ['key_file', 'cert_file', 'cert_reqs', 'ca_certs']:
            if k in ssl_details:
                req_args[k] = ssl_details[k]
    with closing(connectionpool.connection_from_url(url, **req_args)) as req_p:
        retries = max(int(retries), 0)
        attempts = retries + 1
        LOG.debug((""Attempting to open '%s' with %s attempts""
                   "" (%s retries, timeout=%s) to be performed""),
                  url, attempts, retries, timeout)
        open_args = {
            'method': 'GET',
            'retries': retries,
            'redirect': False,
            'url': p_url.request_uri,
        }
        if data is not None:
            if isinstance(data, (str, basestring)):
                open_args['body'] = data
            else:
                open_args['body'] = urllib.urlencode(data)
            open_args['method'] = 'POST'
        if not headers:
            headers = {
                'User-Agent': 'Cloud-Init/%s' % (version.version_string()),
            }
        open_args['headers'] = headers
        if timeout is not None:
            open_args['timeout'] = max(int(timeout), 0)
        r = req_p.urlopen(**open_args)
        return UrlResponse(r.status, r.data, r.headers)","def readurl(url, data=None, timeout=None, retries=0,
            headers=None, ssl_details=None, check_status=True):
    req_args = {
        'url': url,
    }
    if urlparse(url).scheme == 'https' and ssl_details:
        if not SSL_ENABLED:
            LOG.warn(""SSL is not enabled, cert. verification can not occur!"")
        else:
            if 'ca_certs' in ssl_details and ssl_details['ca_certs']:
                req_args['verify'] = ssl_details['ca_certs']












            else:
                req_args['verify'] = True
            if 'cert_file' in ssl_details and 'key_file' in ssl_details:
                req_args['cert'] = [ssl_details['cert_file'],
                                    ssl_details['key_file']]
    req_args['allow_redirects'] = False
    req_args['method'] = 'GET'
    if timeout is not None:
        req_args['timeout'] = max(float(timeout), 0)
    if data:
        req_args['method'] = 'POST'
    # It doesn't seem like config
    # was added in older library versions, thus we
    # need to manually do the retries if it wasn't
    manual_tries = 1
    if CONFIG_ENABLED:
        req_config = {}
        req_config['store_cookies'] = False
        if retries:
            req_config['max_retries'] = max(int(retries), 0)
        req_args['config'] = req_config
    else:
        if retries:
            manual_tries = max(int(retries) + 1, 1)
    if not headers:
        headers = {
            'User-Agent': 'Cloud-Init/%s' % (version.version_string()),
        }
    req_args['headers'] = headers
    LOG.debug(""Attempting to open '%s' with %s configuration"", url, req_args)
    if data:
        # Do this after the log (it might be large)
        req_args['data'] = data
    last_excp = []
    for _i in range(0, manual_tries):
        try:
            r = requests.request(**req_args)
        except exceptions.RequestException as e:
            last_excp = [e]
    if last_excp:
        raise last_excp[-1]
    if check_status:
        r.raise_for_status()
    return UrlResponse(r.status_code, r.content, r.headers)",update,"defreadurl(url,data=None,timeout=None,retries=0,headers=None,ssl_details=None):req_args={}p_url=util.parse_url(url)ifp_url.scheme=='https'andssl_details:forkin['key_file','cert_file','cert_reqs','ca_certs']:ifkinssl_details:req_args[k]=ssl_details[k]withclosing(connectionpool.connection_from_url(url,**req_args))asreq_p:retries=max(int(retries),0)attempts=retries+1LOG.debug((""Attemptingtoopen'%s'with%sattempts""""(%sretries,timeout=%s)tobeperformed""),url,attempts,retries,timeout)open_args={'method':'GET','retries':retries,'redirect':False,'url':p_url.request_uri,}ifdataisnotNone:ifisinstance(data,(str,basestring)):open_args['body']=dataelse:open_args['body']=urllib.urlencode(data)open_args['method']='POST'ifnotheaders:headers={'User-Agent':'Cloud-Init/%s'%(version.version_string()),}open_args['headers']=headersiftimeoutisnotNone:open_args['timeout']=max(int(timeout),0)r=req_p.urlopen(**open_args)returnUrlResponse(r.status,r.data,r.headers)","defreadurl(url,data=None,timeout=None,retries=0,headers=None,ssl_details=None,check_status=True):req_args={'url':url,}ifurlparse(url).scheme=='https'andssl_details:ifnotSSL_ENABLED:LOG.warn(""SSLisnotenabled,cert.verificationcannotoccur!"")else:if'ca_certs'inssl_detailsandssl_details['ca_certs']:req_args['verify']=ssl_details['ca_certs']else:req_args['verify']=Trueif'cert_file'inssl_detailsand'key_file'inssl_details:req_args['cert']=[ssl_details['cert_file'],ssl_details['key_file']]req_args['allow_redirects']=Falsereq_args['method']='GET'iftimeoutisnotNone:req_args['timeout']=max(float(timeout),0)ifdata:req_args['method']='POST'#Itdoesn'tseemlikeconfig#wasaddedinolderlibraryversions,thuswe#needtomanuallydotheretriesifitwasn'tmanual_tries=1ifCONFIG_ENABLED:req_config={}req_config['store_cookies']=Falseifretries:req_config['max_retries']=max(int(retries),0)req_args['config']=req_configelse:ifretries:manual_tries=max(int(retries)+1,1)ifnotheaders:headers={'User-Agent':'Cloud-Init/%s'%(version.version_string()),}req_args['headers']=headersLOG.debug(""Attemptingtoopen'%s'with%sconfiguration"",url,req_args)ifdata:#Dothisafterthelog(itmightbelarge)req_args['data']=datalast_excp=[]for_iinrange(0,manual_tries):try:r=requests.request(**req_args)exceptexceptions.RequestExceptionase:last_excp=[e]iflast_excp:raiselast_excp[-1]ifcheck_status:r.raise_for_status()returnUrlResponse(r.status_code,r.content,r.headers)",cloudinit/url_helper.py
canonical/cloud-init,0fc887d97626132e9024490b271888bed162c867,requests,urllib,"            except exceptions.HTTPError as e:
                reason = ""http error [%s]"" % e.code","            except exceptions.RequestException as e:
                reason = ""request error [%s]"" % e",update,"exceptexceptions.HTTPErrorase:reason=""httperror[%s]""%e.code","exceptexceptions.RequestExceptionase:reason=""requesterror[%s]""%e",cloudinit/url_helper.py
caseymrm/drivesink,c8c1050fdfe56a088593e6ed41746247260b1217,requests,urllib,"def upload_child_file(self, local_path):
        logging.info(""Uploading %s in %r"", local_path, self.node[""name""])"," def upload_child_file(self, name, local_path):
        logging.info(""Uploading %s in %r"", local_path, self.node[""name""])
        m = requests_toolbelt.MultipartEncoder([
            (""metadata"", json.dumps({
                ""name"": name,
                ""kind"": ""FILE"",
                ""parents"": [self.node[""id""]]})),
            (""content"", (name, open(local_path, 'rb')))])
        node = CloudNode(DriveSink.instance().request_content(
            ""%snodes"", method=""post"", data=m,
            headers={'Content-Type': m.content_type}))
        self._children[name] = node",update,"defupload_child_file(self,local_path):logging.info(""Uploading%sin%r"",local_path,self.node[""name""])","defupload_child_file(self,name,local_path):logging.info(""Uploading%sin%r"",local_path,self.node[""name""])m=requests_toolbelt.MultipartEncoder([(""metadata"",json.dumps({""name"":name,""kind"":""FILE"",""parents"":[self.node[""id""]]})),(""content"",(name,open(local_path,'rb')))])node=CloudNode(DriveSink.instance().request_content(""%snodes"",method=""post"",data=m,headers={'Content-Type':m.content_type}))self._children[name]=node",drivesink.py
caseymrm/drivesink,c8c1050fdfe56a088593e6ed41746247260b1217,requests,urllib," def fetch_metadata(self, path, data=None):
        return self._fetch(path % self._config()[""metadataUrl""], data)

    def _fetch(self, url, data=None, refresh=True):
        try:
            headers = {
                ""Authorization"": ""Bearer %s"" % self._config()[""access_token""],
            }
            if data:
                req_data = json.dumps(data)
            else:
                req_data = None
            req = urllib2.Request(url, req_data, headers)
            return json.loads(urllib2.urlopen(req).read())
        except urllib2.HTTPError, e:
            if e.code == 401 and refresh:
                # Have to proxy to get the client id and secret
                refresh = urllib.urlencode({
                    ""refresh_token"": self._config()[""refresh_token""],
                })
                req = urllib2.Request(""%s/refresh"" % self.drivesink, refresh)
                new_config = json.loads(urllib2.urlopen(req).read())
                self.config.update(new_config)
                with open(self._config_file(), 'w') as f:
                    f.write(json.dumps(self.config, sort_keys=True, indent=4))
                return self._fetch(url, data, refresh=False)
            else:
                logging.error(e.read())
                raise","def request_metadata(self, path, json_data=None, **kwargs):
        args = {}
        if json_data:
            args[""method""] = ""post""
            args[""data""] = json.dumps(json_data)
        else:
            args[""method""] = ""get""

        args.update(kwargs)

        return self._request(
            path % self._config()[""metadataUrl""], **args)

    def request_content(self, path, **kwargs):
        return self._request(
            path % self._config()[""contentUrl""], **kwargs)

    def _request(self, url, refresh=True, **kwargs):
        headers = {
            ""Authorization"": ""Bearer %s"" % self._config()[""access_token""],
        }
        headers.update(kwargs.pop(""headers"", {}))
        req = requests.request(url=url, headers=headers, **kwargs)
        if req.status_code == 401 and refresh:
            # Have to proxy to get the client id and secret
            req = requests.post(""%s/refresh"" % self.args.drivesink, data={
                ""refresh_token"": self._config()[""refresh_token""],
            })
            req.raise_for_status()
            new_config = req.json()
            self.config.update(new_config)
            with open(self._config_file(), 'w') as f:
                f.write(json.dumps(self.config, sort_keys=True, indent=4))
            return self._request(url, refresh=False, **kwargs)
        req.raise_for_status()
        return req.json()",update,"deffetch_metadata(self,path,data=None):returnself._fetch(path%self._config()[""metadataUrl""],data)def_fetch(self,url,data=None,refresh=True):try:headers={""Authorization"":""Bearer%s""%self._config()[""access_token""],}ifdata:req_data=json.dumps(data)else:req_data=Nonereq=urllib2.Request(url,req_data,headers)returnjson.loads(urllib2.urlopen(req).read())excepturllib2.HTTPError,e:ife.code==401andrefresh:#Havetoproxytogettheclientidandsecretrefresh=urllib.urlencode({""refresh_token"":self._config()[""refresh_token""],})req=urllib2.Request(""%s/refresh""%self.drivesink,refresh)new_config=json.loads(urllib2.urlopen(req).read())self.config.update(new_config)withopen(self._config_file(),'w')asf:f.write(json.dumps(self.config,sort_keys=True,indent=4))returnself._fetch(url,data,refresh=False)else:logging.error(e.read())raise","defrequest_metadata(self,path,json_data=None,**kwargs):args={}ifjson_data:args[""method""]=""post""args[""data""]=json.dumps(json_data)else:args[""method""]=""get""args.update(kwargs)returnself._request(path%self._config()[""metadataUrl""],**args)defrequest_content(self,path,**kwargs):returnself._request(path%self._config()[""contentUrl""],**kwargs)def_request(self,url,refresh=True,**kwargs):headers={""Authorization"":""Bearer%s""%self._config()[""access_token""],}headers.update(kwargs.pop(""headers"",{}))req=requests.request(url=url,headers=headers,**kwargs)ifreq.status_code==401andrefresh:#Havetoproxytogettheclientidandsecretreq=requests.post(""%s/refresh""%self.args.drivesink,data={""refresh_token"":self._config()[""refresh_token""],})req.raise_for_status()new_config=req.json()self.config.update(new_config)withopen(self._config_file(),'w')asf:f.write(json.dumps(self.config,sort_keys=True,indent=4))returnself._request(url,refresh=False,**kwargs)req.raise_for_status()returnreq.json()",drivesink.py
ckan/ckanext-harvest,6b6458f2eadd4eb022f5dd5d76993e9abbd39e7b,requests,urllib," def _get_content(self, url):
        http_request = urllib2.Request(url=url)


        api_key = self.config.get('api_key')
        if api_key:
            http_request.add_header('Authorization', api_key)



        try:
            http_response = urllib2.urlopen(http_request)
        except urllib2.HTTPError, e:
            if e.getcode() == 404:
                raise ContentNotFoundError('HTTP error: %s' % e.code)
            else:
                raise ContentFetchError('HTTP error: %s' % e.code)
        except urllib2.URLError, e:
            raise ContentFetchError('URL error: %s' % e.reason)
        except httplib.HTTPException, e:
            raise ContentFetchError('HTTP Exception: %s' % e)
        except socket.error, e:
            raise ContentFetchError('HTTP socket error: %s' % e)
        except Exception, e:
            raise ContentFetchError('HTTP general exception: %s' % e)
        return http_response.read()"," def _get_content(self, url):


        headers = {}
        api_key = self.config.get('api_key')
        if api_key:
            headers['Authorization'] = api_key

        pyopenssl.inject_into_urllib3()

        try:
            http_request = requests.get(url, headers=headers)
        except HTTPError as e:
            if e.response.status_code == 404:
                raise ContentNotFoundError('HTTP error: %s' % e.code)
            else:
                raise ContentFetchError('HTTP error: %s' % e.code)
        except InvalidURL as e:
            raise ContentFetchError('URL error: %s' % e.reason)
        except httplib.HTTPException as e:
            raise ContentFetchError('HTTP Exception: %s' % e)
        except socket.error as e:
            raise ContentFetchError('HTTP socket error: %s' % e)
        except Exception as e:
            raise ContentFetchError('HTTP general exception: %s' % e)
        return http_request.text",update,"def_get_content(self,url):http_request=urllib2.Request(url=url)api_key=self.config.get('api_key')ifapi_key:http_request.add_header('Authorization',api_key)try:http_response=urllib2.urlopen(http_request)excepturllib2.HTTPError,e:ife.getcode()==404:raiseContentNotFoundError('HTTPerror:%s'%e.code)else:raiseContentFetchError('HTTPerror:%s'%e.code)excepturllib2.URLError,e:raiseContentFetchError('URLerror:%s'%e.reason)excepthttplib.HTTPException,e:raiseContentFetchError('HTTPException:%s'%e)exceptsocket.error,e:raiseContentFetchError('HTTPsocketerror:%s'%e)exceptException,e:raiseContentFetchError('HTTPgeneralexception:%s'%e)returnhttp_response.read()","def_get_content(self,url):headers={}api_key=self.config.get('api_key')ifapi_key:headers['Authorization']=api_keypyopenssl.inject_into_urllib3()try:http_request=requests.get(url,headers=headers)exceptHTTPErrorase:ife.response.status_code==404:raiseContentNotFoundError('HTTPerror:%s'%e.code)else:raiseContentFetchError('HTTPerror:%s'%e.code)exceptInvalidURLase:raiseContentFetchError('URLerror:%s'%e.reason)excepthttplib.HTTPExceptionase:raiseContentFetchError('HTTPException:%s'%e)exceptsocket.errorase:raiseContentFetchError('HTTPsocketerror:%s'%e)exceptExceptionase:raiseContentFetchError('HTTPgeneralexception:%s'%e)returnhttp_request.text",ckanext/harvest/harvesters/ckanharvester.py
cloudify-cosmo/cloudify-cli,7a56c5357a2c3ef10ef3f7fe546f6b9dc5c1aace,requests,urllib,"def download_file(url, destination=None):
    if not destination:
        fd, destination = tempfile.mkstemp()
        os.close(fd)
    logger = get_logger()
    logger.info('Downloading {0} to {1}...'.format(url, destination))
    final_url = urllib.urlopen(url).geturl()
    if final_url != url:
        logger.debug('Redirected to {0}'.format(final_url))
    f = urllib.URLopener()
    try:
        f.retrieve(final_url, destination)
    except IOError as ex:
        raise CloudifyCliError(
            'Failed to download {0}. ({1})'.format(url, str(ex)))

    return destination"," if not destination:
        fd, destination = tempfile.mkstemp()
        os.close(fd)
    logger = get_logger()
    logger.info('Downloading {0} to {1}...'.format(url, destination))

    try:
        response = requests.get(url, stream=True)
    except requests.exceptions.RequestException as ex:
        raise CloudifyCliError(
            'Failed to download {0}. ({1})'.format(url, str(ex)))

    final_url = response.url
    if final_url != url:
        logger.debug('Redirected to {0}'.format(final_url))

    try:
        with open(destination, 'wb') as destination_file:
            for chunk in response.iter_content(CHUNK_SIZE):
                destination_file.write(chunk)
    except IOError as ex:
        raise CloudifyCliError(
            'Failed to download {0}. ({1})'.format(url, str(ex)))

    return destination",update,"defdownload_file(url,destination=None):ifnotdestination:fd,destination=tempfile.mkstemp()os.close(fd)logger=get_logger()logger.info('Downloading{0}to{1}...'.format(url,destination))final_url=urllib.urlopen(url).geturl()iffinal_url!=url:logger.debug('Redirectedto{0}'.format(final_url))f=urllib.URLopener()try:f.retrieve(final_url,destination)exceptIOErrorasex:raiseCloudifyCliError('Failedtodownload{0}.({1})'.format(url,str(ex)))returndestination","ifnotdestination:fd,destination=tempfile.mkstemp()os.close(fd)logger=get_logger()logger.info('Downloading{0}to{1}...'.format(url,destination))try:response=requests.get(url,stream=True)exceptrequests.exceptions.RequestExceptionasex:raiseCloudifyCliError('Failedtodownload{0}.({1})'.format(url,str(ex)))final_url=response.urliffinal_url!=url:logger.debug('Redirectedto{0}'.format(final_url))try:withopen(destination,'wb')asdestination_file:forchunkinresponse.iter_content(CHUNK_SIZE):destination_file.write(chunk)exceptIOErrorasex:raiseCloudifyCliError('Failedtodownload{0}.({1})'.format(url,str(ex)))returndestination",cloudify_cli/utils.py
collinsmuriuki/flask-movie-app,290859c10d3d69134bfc30949b76ad148f507217,requests,urllib,"def get_movies(category):
    """"""
    Function that gets the json response to our url request
    """"""
    get_movies_url = base_url.format(category, api_key)
    with urllib.request.urlopen(get_movies_url) as url:
        get_movies_data = url.read()
        get_movies_response = json.loads(get_movies_data)

        movie_results = None

        if get_movies_response['results']:
            movie_results_list = get_movies_response['results']
            movie_results = process_results(movie_results_list)

    return movie_results","def get_movies(category):
    """"""
    Function that gets the json response to our url request
    """"""
    get_movies_url = base_url.format(category, api_key)
    get_movies_response = requests.get(get_movies_url).json()
    
    if get_movies_response['results']:
        movie_results_list = get_movies_response['results']
        movie_results = process_results(movie_results_list)





    return movie_results",update,"defget_movies(category):""""""Functionthatgetsthejsonresponsetooururlrequest""""""get_movies_url=base_url.format(category,api_key)withurllib.request.urlopen(get_movies_url)asurl:get_movies_data=url.read()get_movies_response=json.loads(get_movies_data)movie_results=Noneifget_movies_response['results']:movie_results_list=get_movies_response['results']movie_results=process_results(movie_results_list)returnmovie_results","defget_movies(category):""""""Functionthatgetsthejsonresponsetooururlrequest""""""get_movies_url=base_url.format(category,api_key)get_movies_response=requests.get(get_movies_url).json()ifget_movies_response['results']:movie_results_list=get_movies_response['results']movie_results=process_results(movie_results_list)returnmovie_results",app/request.py
collinsmuriuki/flask-movie-app,290859c10d3d69134bfc30949b76ad148f507217,requests,urllib,"def get_movie(id):
    get_movie_details_url = base_url.format(id, api_key)
    with urllib.request.urlopen(get_movie_details_url) as url:
        movie_details_data = url.read()
        movie_details_response = json.loads(movie_details_data)

        movie_object = None
        if movie_details_response:
            id = movie_details_response.get('id')
            title = movie_details_response.get('original_title')
            overview = movie_details_response.get('overview')
            poster = movie_details_response.get('poster_path')
            vote_average = movie_details_response.get('vote_average')
            vote_count = movie_details_response.get('vote_count')

            movie_object = Movie(id, title, overview, poster, vote_average, vote_count)

    return movie_object","def get_movie(id):
    get_movie_details_url = base_url.format(id, api_key)
    movie_details_response = requests.get(get_movie_details_url).json()

    if movie_details_response:
        id = movie_details_response.get('id')
        title = movie_details_response.get('original_title')
        overview = movie_details_response.get('overview')
        poster = movie_details_response.get('poster_path')
        vote_average = movie_details_response.get('vote_average')
        vote_count = movie_details_response.get('vote_count')

        movie_object = Movie(id, title, overview, poster, vote_average, vote_count)

    return movie_object",update,"defget_movie(id):get_movie_details_url=base_url.format(id,api_key)withurllib.request.urlopen(get_movie_details_url)asurl:movie_details_data=url.read()movie_details_response=json.loads(movie_details_data)movie_object=Noneifmovie_details_response:id=movie_details_response.get('id')title=movie_details_response.get('original_title')overview=movie_details_response.get('overview')poster=movie_details_response.get('poster_path')vote_average=movie_details_response.get('vote_average')vote_count=movie_details_response.get('vote_count')movie_object=Movie(id,title,overview,poster,vote_average,vote_count)returnmovie_object","defget_movie(id):get_movie_details_url=base_url.format(id,api_key)movie_details_response=requests.get(get_movie_details_url).json()ifmovie_details_response:id=movie_details_response.get('id')title=movie_details_response.get('original_title')overview=movie_details_response.get('overview')poster=movie_details_response.get('poster_path')vote_average=movie_details_response.get('vote_average')vote_count=movie_details_response.get('vote_count')movie_object=Movie(id,title,overview,poster,vote_average,vote_count)returnmovie_object",app/request.py
collinsmuriuki/flask-movie-app,290859c10d3d69134bfc30949b76ad148f507217,requests,urllib,"def search_movie(movie_name):
    search_movie_url = 'https://api.themoviedb.org/3/search/movie?api_key={}&query={}'.format(api_key, movie_name)
    with urllib.request.urlopen(search_movie_url) as url:
        search_movie_data = url.read()
        search_movie_response = json.loads(search_movie_data)

        search_movie_results = None

        if search_movie_response['results']:
            search_movie_list = search_movie_response['results']
            search_movie_results = process_results(search_movie_list)","def search_movie(movie_name):
    search_movie_url = 'https://api.themoviedb.org/3/search/movie?api_key={}&query={}'.format(api_key, movie_name)
    search_movie_response = requests.get(search_movie_url).json()





    if search_movie_response['results']:
        search_movie_list = search_movie_response['results']
        search_movie_results = process_results(search_movie_list)",update,"defsearch_movie(movie_name):search_movie_url='https://api.themoviedb.org/3/search/movie?api_key={}&query={}'.format(api_key,movie_name)withurllib.request.urlopen(search_movie_url)asurl:search_movie_data=url.read()search_movie_response=json.loads(search_movie_data)search_movie_results=Noneifsearch_movie_response['results']:search_movie_list=search_movie_response['results']search_movie_results=process_results(search_movie_list)","defsearch_movie(movie_name):search_movie_url='https://api.themoviedb.org/3/search/movie?api_key={}&query={}'.format(api_key,movie_name)search_movie_response=requests.get(search_movie_url).json()ifsearch_movie_response['results']:search_movie_list=search_movie_response['results']search_movie_results=process_results(search_movie_list)",app/request.py
conix-security/BTG,ffd739cb34268f954ae0357ed1e39e181ec7552d,requests,urllib,"def search(self):
        mod.display(self.module_name, """", ""INFO"", ""Search in VirusTotal ..."")
        if ""proxy_host"" in self.config:
            if len(self.config[""proxy_host""][""https""]) > 0:
                proxy = urllib2.ProxyHandler({'https': self.config[""proxy_host""][""https""]})
                opener = urllib2.build_opener(proxy)
            else:
                opener = urllib2.build_opener()
        else:
            mod.display(self.module_name,
                        message_type=""ERROR"",
                        string=""Please check if you have proxy_host field in config.ini"")
        urllib2.install_opener(opener)
        try:
            if ""virustotal_api_keys"" in self.config:
                self.key = choice(self.config[""virustotal_api_keys""])
            else:
                mod.display(self.module_name,
                            message_type=""ERROR"",
                            string=""Check if you have virustotal_api_keys field in config.ini"")
        except:
            mod.display(self.module_name, self.ioc, ""ERROR"", ""Please provide your authkey."")
            return

        if self.type in [""URL"", ""domain"", ""IPv4""]:
            self.searchURL()
        else:
            self.searchReport()

    def searchReport(self):
        self.url = ""https://www.virustotal.com/vtapi/v2/file/report""
        parameters = {""resource"": self.ioc,
                      ""apikey"": self.key,
                      ""allinfo"": 1}
        data = urllib.urlencode(parameters)
        req = urllib2.Request(self.url, data)
        response = urllib2.urlopen(req)
        if response.getcode() == 200 :
            response_content = response.read()
            try:
                import simplejson
                json_content = simplejson.loads(response_content)
            except :
                mod.display(self.module_name, self.ioc, ""ERROR"", ""Virustotal json decode fail. Blacklisted/Bad API key? (Sleep 10sec)."")
                sleep(randint(5, 10))
            try:
                if json_content[""positives""]:
                    mod.display(self.module_name,
                                self.ioc,
                                ""FOUND"",
                                ""Score: %s/%s | %s""%(json_content[""positives""],
                                                     json_content[""total""],
                                                     json_content[""permalink""]))
            except:
                pass
        else :
            mod.display(self.module_name, self.ioc, ""ERROR"", ""VirusTotal returned ""+ str(response.getcode()))","def search(self):
        mod.display(self.module_name, """", ""INFO"", ""Search in VirusTotal ..."")











        try:
            if ""virustotal_api_keys"" in self.config:
                self.key = choice(self.config[""virustotal_api_keys""])
            else:
                mod.display(self.module_name,
                            message_type=""ERROR"",
                            string=""Check if you have virustotal_api_keys field in config.ini"")
        except:
            mod.display(self.module_name, self.ioc, ""ERROR"", ""Please provide your authkey."")
            return

        if self.type in [""URL"", ""domain"", ""IPv4""]:
            self.searchURL()
        else:
            self.searchReport()

    def searchReport(self):
        self.url = ""https://www.virustotal.com/vtapi/v2/file/report""
        parameters = {""resource"": self.ioc,
                      ""apikey"": self.key,
                      ""allinfo"": 1}
        while True:
            req = post(
                        self.url,
                        headers=self.config[""user_agent""],
                        proxies=self.config[""proxy_host""],
                        timeout=self.config[""requests_timeout""],
                        data = parameters
                    )
            if req.status_code == 200 :
                response_content = req.text
                try:
                    json_content = json.loads(response_content)
                    break
                except :
                    mod.display(self.module_name, self.ioc, ""WARNING"", ""Virustotal json decode fail. Blacklisted/Bad API key? (Sleep 10sec)."")
                    sleep(randint(5, 10))
            else :
                mod.display(self.module_name, self.ioc, ""ERROR"", ""VirusTotal returned ""+ str(response.getcode()))
                return
        try:
            if json_content[""positives""]:
                mod.display(self.module_name,
                            self.ioc,
                            ""FOUND"",
                            ""Score: %s/%s | %s""%(json_content[""positives""],
                                                 json_content[""total""],
                                                 json_content[""permalink""]))
        except:
            pass",update,"defsearch(self):mod.display(self.module_name,"""",""INFO"",""SearchinVirusTotal..."")if""proxy_host""inself.config:iflen(self.config[""proxy_host""][""https""])>0:proxy=urllib2.ProxyHandler({'https':self.config[""proxy_host""][""https""]})opener=urllib2.build_opener(proxy)else:opener=urllib2.build_opener()else:mod.display(self.module_name,message_type=""ERROR"",string=""Pleasecheckifyouhaveproxy_hostfieldinconfig.ini"")urllib2.install_opener(opener)try:if""virustotal_api_keys""inself.config:self.key=choice(self.config[""virustotal_api_keys""])else:mod.display(self.module_name,message_type=""ERROR"",string=""Checkifyouhavevirustotal_api_keysfieldinconfig.ini"")except:mod.display(self.module_name,self.ioc,""ERROR"",""Pleaseprovideyourauthkey."")returnifself.typein[""URL"",""domain"",""IPv4""]:self.searchURL()else:self.searchReport()defsearchReport(self):self.url=""https://www.virustotal.com/vtapi/v2/file/report""parameters={""resource"":self.ioc,""apikey"":self.key,""allinfo"":1}data=urllib.urlencode(parameters)req=urllib2.Request(self.url,data)response=urllib2.urlopen(req)ifresponse.getcode()==200:response_content=response.read()try:importsimplejsonjson_content=simplejson.loads(response_content)except:mod.display(self.module_name,self.ioc,""ERROR"",""Virustotaljsondecodefail.Blacklisted/BadAPIkey?(Sleep10sec)."")sleep(randint(5,10))try:ifjson_content[""positives""]:mod.display(self.module_name,self.ioc,""FOUND"",""Score:%s/%s|%s""%(json_content[""positives""],json_content[""total""],json_content[""permalink""]))except:passelse:mod.display(self.module_name,self.ioc,""ERROR"",""VirusTotalreturned""+str(response.getcode()))","defsearch(self):mod.display(self.module_name,"""",""INFO"",""SearchinVirusTotal..."")try:if""virustotal_api_keys""inself.config:self.key=choice(self.config[""virustotal_api_keys""])else:mod.display(self.module_name,message_type=""ERROR"",string=""Checkifyouhavevirustotal_api_keysfieldinconfig.ini"")except:mod.display(self.module_name,self.ioc,""ERROR"",""Pleaseprovideyourauthkey."")returnifself.typein[""URL"",""domain"",""IPv4""]:self.searchURL()else:self.searchReport()defsearchReport(self):self.url=""https://www.virustotal.com/vtapi/v2/file/report""parameters={""resource"":self.ioc,""apikey"":self.key,""allinfo"":1}whileTrue:req=post(self.url,headers=self.config[""user_agent""],proxies=self.config[""proxy_host""],timeout=self.config[""requests_timeout""],data=parameters)ifreq.status_code==200:response_content=req.texttry:json_content=json.loads(response_content)breakexcept:mod.display(self.module_name,self.ioc,""WARNING"",""Virustotaljsondecodefail.Blacklisted/BadAPIkey?(Sleep10sec)."")sleep(randint(5,10))else:mod.display(self.module_name,self.ioc,""ERROR"",""VirusTotalreturned""+str(response.getcode()))returntry:ifjson_content[""positives""]:mod.display(self.module_name,self.ioc,""FOUND"",""Score:%s/%s|%s""%(json_content[""positives""],json_content[""total""],json_content[""permalink""]))except:pass",modules/virustotal.py
conix-security/BTG,ffd739cb34268f954ae0357ed1e39e181ec7552d,requests,urllib,"def searchURL(self):
        self.url = ""http://www.virustotal.com/vtapi/v2/url/report""
        parameters = {""resource"": self.ioc,
                      ""apikey"": self.key}
        data = urllib.urlencode(parameters)
        req = urllib2.Request(self.url, data)
        while True:







            try:
                response = urllib2.urlopen(req).read()
                json_content = loads(response)
                break
            except:
                mod.display(self.module_name,
                            self.ioc,
                            ""INFO"",
                            ""Virustotal json decode fail. Blacklisted/Bad API key? (Sleep 10sec)."")
                sleep(randint(5, 10))
                pass"," def searchURL(self):
        self.url = ""http://www.virustotal.com/vtapi/v2/url/report""
        parameters = {""resource"": self.ioc,
                      ""apikey"": self.key}


        while True:
            req = post(
                self.url,
                headers=self.config[""user_agent""],
                proxies=self.config[""proxy_host""],
                timeout=self.config[""requests_timeout""],
                data = parameters
            )
            try:
                json_content = json.loads(req.text)

                break
            except:
                mod.display(self.module_name,
                            self.ioc,
                            ""WARNING"",
                            ""Virustotal json decode fail. Blacklisted/Bad API key? (Sleep 10sec)."")
                sleep(randint(5, 10))
                pass",update,"defsearchURL(self):self.url=""http://www.virustotal.com/vtapi/v2/url/report""parameters={""resource"":self.ioc,""apikey"":self.key}data=urllib.urlencode(parameters)req=urllib2.Request(self.url,data)whileTrue:try:response=urllib2.urlopen(req).read()json_content=loads(response)breakexcept:mod.display(self.module_name,self.ioc,""INFO"",""Virustotaljsondecodefail.Blacklisted/BadAPIkey?(Sleep10sec)."")sleep(randint(5,10))pass","defsearchURL(self):self.url=""http://www.virustotal.com/vtapi/v2/url/report""parameters={""resource"":self.ioc,""apikey"":self.key}whileTrue:req=post(self.url,headers=self.config[""user_agent""],proxies=self.config[""proxy_host""],timeout=self.config[""requests_timeout""],data=parameters)try:json_content=json.loads(req.text)breakexcept:mod.display(self.module_name,self.ioc,""WARNING"",""Virustotaljsondecodefail.Blacklisted/BadAPIkey?(Sleep10sec)."")sleep(randint(5,10))pass",modules/virustotal.py
dansan/spring-replay-site,7090aa14404897a72ca07b3de03d4632c6d65938,requests,urllib," def fetch_img(self):
        """"""
        fetches map image from api.springfiles.com
        """"""
        if not self.map_info:
            self.fetch_info()
        if self.map_info:
            map_url = self.map_info[0]['mapimages'][0]
            urllib.urlretrieve(map_url, self.full_image_filepath)
            logger.info(""Downloaded map image from %r into %r."", map_url, self.full_image_filepath)

        else:
            logger.warn(""We have no map-info, setting image to 'map_img_not_avail.jpg'."")
            copyfile(path_join(settings.IMG_PATH, ""map_img_not_avail.jpg""), self.full_image_filepath)","def fetch_img(self):
        """"""
        fetches map image from api.springfiles.com
        """"""
        if not self.map_info:
            self.fetch_info()
        if self.map_info:
            map_url = self.map_info[0]['mapimages'][0]
            logger.info(""Downloaded map image from %r into %r..."", map_url, self.full_image_filepath)
            response = requests.get(map_url, stream=True)
            if response.status_code == 200:
                with open(self.full_image_filepath, 'wb') as fp:
                    for chunk in response:
                        fp.write(chunk)
            else:
                logger.error('Could not download image %r. Setting image to ""map_img_not_avail.jpg"".')
                copyfile(path_join(settings.IMG_PATH, 'map_img_not_avail.jpg'), self.full_image_filepath)
        else:
            logger.warn(""We have no map-info, setting image to 'map_img_not_avail.jpg'."")
            copyfile(path_join(settings.IMG_PATH, ""map_img_not_avail.jpg""), self.full_image_filepath)",update,"deffetch_img(self):""""""fetchesmapimagefromapi.springfiles.com""""""ifnotself.map_info:self.fetch_info()ifself.map_info:map_url=self.map_info[0]['mapimages'][0]urllib.urlretrieve(map_url,self.full_image_filepath)logger.info(""Downloadedmapimagefrom%rinto%r."",map_url,self.full_image_filepath)else:logger.warn(""Wehavenomap-info,settingimageto'map_img_not_avail.jpg'."")copyfile(path_join(settings.IMG_PATH,""map_img_not_avail.jpg""),self.full_image_filepath)","deffetch_img(self):""""""fetchesmapimagefromapi.springfiles.com""""""ifnotself.map_info:self.fetch_info()ifself.map_info:map_url=self.map_info[0]['mapimages'][0]logger.info(""Downloadedmapimagefrom%rinto%r..."",map_url,self.full_image_filepath)response=requests.get(map_url,stream=True)ifresponse.status_code==200:withopen(self.full_image_filepath,'wb')asfp:forchunkinresponse:fp.write(chunk)else:logger.error('Couldnotdownloadimage%r.Settingimageto""map_img_not_avail.jpg"".')copyfile(path_join(settings.IMG_PATH,'map_img_not_avail.jpg'),self.full_image_filepath)else:logger.warn(""Wehavenomap-info,settingimageto'map_img_not_avail.jpg'."")copyfile(path_join(settings.IMG_PATH,""map_img_not_avail.jpg""),self.full_image_filepath)",srs/springmaps.py
datawire/ambassador,f192b6b45ce30995b60f465f6028840145e4f593,requests,urllib,"def get_code_with_retry(req):
    for attempts in range(10):
        try:
            conn = request.urlopen(req, timeout=10)
            conn.close()
            return 200
        except HTTPError as e:
            if int(e.code) < 500:
                return e.code
            print(f""get_code_with_retry: HTTPError code {e.code}, attempt {attempts+1}"")
        except socket.timeout as e:
            print(f""get_code_with_retry: socket.timeout {e}, attempt {attempts+1}"")


        time.sleep(5)
    return 503","def get_code_with_retry(req, headers={}):
    for attempts in range(10):
        try:
            resp = requests.get(req, headers=headers, timeout=10)
            if resp.status_code < 500:
                return resp.status_code
            print(f""get_code_with_retry: 5xx code {resp.status_code}, retrying..."")
        except requests.exceptions.ConnectionError as e:
            print(f""get_code_with_retry: ConnectionError {e}, attempt {attempts+1}"")

        except socket.timeout as e:
            print(f""get_code_with_retry: socket.timeout {e}, attempt {attempts+1}"")
        except Exception as e:
            print(f""get_code_with_retry: generic exception {e}, attempt {attempts+1}"")
        time.sleep(5)
    return 503",update,"defget_code_with_retry(req):forattemptsinrange(10):try:conn=request.urlopen(req,timeout=10)conn.close()return200exceptHTTPErrorase:ifint(e.code)<500:returne.codeprint(f""get_code_with_retry:HTTPErrorcode{e.code},attempt{attempts+1}"")exceptsocket.timeoutase:print(f""get_code_with_retry:socket.timeout{e},attempt{attempts+1}"")time.sleep(5)return503","defget_code_with_retry(req,headers={}):forattemptsinrange(10):try:resp=requests.get(req,headers=headers,timeout=10)ifresp.status_code<500:returnresp.status_codeprint(f""get_code_with_retry:5xxcode{resp.status_code},retrying..."")exceptrequests.exceptions.ConnectionErrorase:print(f""get_code_with_retry:ConnectionError{e},attempt{attempts+1}"")exceptsocket.timeoutase:print(f""get_code_with_retry:socket.timeout{e},attempt{attempts+1}"")exceptExceptionase:print(f""get_code_with_retry:genericexception{e},attempt{attempts+1}"")time.sleep(5)return503",python/tests/utils.py
davidhalter/depl,e21cc5321e53dbb42502e5a6b2c13528005a6b74,requests,urllib,"def django_basic_test(tmpdir):
    copy_to_temp(tmpdir)
    main_run(['depl', 'deploy', 'localhost'])
    assert urllib.urlopen(""http://localhost:8887/"").read() == ""django rocks\n""

    txt = urllib.urlopen(""http://localhost:8887/static/something.txt"").read()
    assert txt == ""static files\n""
    # django plays with the db
    assert urllib.urlopen(""http://localhost:8887/db_add.html"").read() == ""saved\n""","def django_basic_test(tmpdir):
    copy_to_temp(tmpdir)
    main_run(['depl', 'deploy', 'localhost'])
    assert requests.get(""http://localhost:8887/"").text == ""django rocks\n""

    txt = requests.get(""http://localhost:8887/static/something.txt"").text
    assert txt == ""static files\n""
    # django plays with the db
    assert requests.get(""http://localhost:8887/db_add.html"").text == ""saved\n""",update,"defdjango_basic_test(tmpdir):copy_to_temp(tmpdir)main_run(['depl','deploy','localhost'])asserturllib.urlopen(""http://localhost:8887/"").read()==""djangorocks\n""txt=urllib.urlopen(""http://localhost:8887/static/something.txt"").read()asserttxt==""staticfiles\n""#djangoplayswiththedbasserturllib.urlopen(""http://localhost:8887/db_add.html"").read()==""saved\n""","defdjango_basic_test(tmpdir):copy_to_temp(tmpdir)main_run(['depl','deploy','localhost'])assertrequests.get(""http://localhost:8887/"").text==""djangorocks\n""txt=requests.get(""http://localhost:8887/static/something.txt"").textasserttxt==""staticfiles\n""#djangoplayswiththedbassertrequests.get(""http://localhost:8887/db_add.html"").text==""saved\n""",test/deploy/test_django.py
davidhalter/depl,e21cc5321e53dbb42502e5a6b2c13528005a6b74,requests,urllib,"def django_pg_test(tmpdir):
    django_basic_test(tmpdir)
    # django plays with the db
    content = urllib.urlopen(""http://localhost:8887/db_show.html"").read()
    assert content == 'django.db.backends.postgresql_psycopg2: 1\n'
    delete_pg_connection()","def django_pg_test(tmpdir):
    django_basic_test(tmpdir)
    # django plays with the db
    content = requests.get(""http://localhost:8887/db_show.html"").text
    assert content == 'django.db.backends.postgresql_psycopg2: 1\n'
    delete_pg_connection()",update,"defdjango_pg_test(tmpdir):django_basic_test(tmpdir)#djangoplayswiththedbcontent=urllib.urlopen(""http://localhost:8887/db_show.html"").read()assertcontent=='django.db.backends.postgresql_psycopg2:1\n'delete_pg_connection()","defdjango_pg_test(tmpdir):django_basic_test(tmpdir)#djangoplayswiththedbcontent=requests.get(""http://localhost:8887/db_show.html"").textassertcontent=='django.db.backends.postgresql_psycopg2:1\n'delete_pg_connection()",test/deploy/test_django.py
davidhalter/depl,e21cc5321e53dbb42502e5a6b2c13528005a6b74,requests,urllib,"def test_django_sqlite(tmpdir):
    django_basic_test(tmpdir)
    content =  urllib.urlopen(""http://localhost:8887/db_show.html"").read()
    assert content == 'django.db.backends.sqlite3: 1\n'","def test_django_sqlite(tmpdir):
    django_basic_test(tmpdir)
    content = requests.get(""http://localhost:8887/db_show.html"").text
    assert content == 'django.db.backends.sqlite3: 1\n'",update,"deftest_django_sqlite(tmpdir):django_basic_test(tmpdir)content=urllib.urlopen(""http://localhost:8887/db_show.html"").read()assertcontent=='django.db.backends.sqlite3:1\n'","deftest_django_sqlite(tmpdir):django_basic_test(tmpdir)content=requests.get(""http://localhost:8887/db_show.html"").textassertcontent=='django.db.backends.sqlite3:1\n'",test/deploy/test_django.py
dcos/dcos,c3012f9f331fe06ff1ab038293d4cad2212354f2,requests,urllib,"try:
    response = urllib.request.urlopen(EXHIBITOR_STATUS_URL)
except urllib.error.URLError:
    print('Could not get exhibitor status: {}'.format(
        EXHIBITOR_STATUS_URL), file=sys.stderr)
    sys.exit(1)
reader = codecs.getreader(""utf-8"")
data = json.load(reader(response))","resp = requests.get(EXHIBITOR_STATUS_URL)
if resp.status_code != 200:
    print('Could not get exhibitor status: {}, Status code: {}'.format(
          EXHIBITOR_STATUS_URL, resp.status_code), file=sys.stderr)

    sys.exit(1)

data = resp.json()",update,"try:response=urllib.request.urlopen(EXHIBITOR_STATUS_URL)excepturllib.error.URLError:print('Couldnotgetexhibitorstatus:{}'.format(EXHIBITOR_STATUS_URL),file=sys.stderr)sys.exit(1)reader=codecs.getreader(""utf-8"")data=json.load(reader(response))","resp=requests.get(EXHIBITOR_STATUS_URL)ifresp.status_code!=200:print('Couldnotgetexhibitorstatus:{},Statuscode:{}'.format(EXHIBITOR_STATUS_URL,resp.status_code),file=sys.stderr)sys.exit(1)data=resp.json()",packages/exhibitor/extra/exhibitor_wait.py
dissemin/dissemin,bb03947562f044fd5887b964223a1822a3813d75,requests,urllib," request = base_url+'?'+urlencode(query_args)
            f = urlopen(request)
            response = f.read()
            root = ET.fromstring(response)
            result_list = list(root.findall('./result'))","request = base_url+'?'+urlencode(query_args)
            f = requests.get(base_url, params=query_args)
            response = f.text
            root = ET.fromstring(response)
            result_list = list(root.findall('./result'))",update,request=base_url+'?'+urlencode(query_args)f=urlopen(request)response=f.read()root=ET.fromstring(response)result_list=list(root.findall('./result')),"request=base_url+'?'+urlencode(query_args)f=requests.get(base_url,params=query_args)response=f.textroot=ET.fromstring(response)result_list=list(root.findall('./result'))",backend/base.py
dissemin/dissemin,bb03947562f044fd5887b964223a1822a3813d75,requests,urllib,except URLError as e:,except requests.exceptions.RequestException as e:,update,exceptURLErrorase:,exceptrequests.exceptions.RequestExceptionase:,backend/base.py
dissemin/dissemin,bb03947562f044fd5887b964223a1822a3813d75,requests,urllib,"while offset < nb_results and no_match < max_no_match_before_give_up:
            query_args = {'api_key':core_api_key,
                    'offset':str(offset)}
            request = base_url+search_terms+'?'+urlencode(query_args)
            f = urlopen(request)
            response = f.read()
            root = etree.fromstring(response)
            result_list = list(root.iter('record'))","while offset < nb_results and no_match < max_no_match_before_give_up:
            query_args = {'api_key':core_api_key,
                    'offset':str(offset)}
            f = requests.get(base_url+search_terms, params=query_args)
            response = f.text

            root = etree.fromstring(response)
            result_list = list(root.iter('record'))",update,"whileoffset<nb_resultsandno_match<max_no_match_before_give_up:query_args={'api_key':core_api_key,'offset':str(offset)}request=base_url+search_terms+'?'+urlencode(query_args)f=urlopen(request)response=f.read()root=etree.fromstring(response)result_list=list(root.iter('record'))","whileoffset<nb_resultsandno_match<max_no_match_before_give_up:query_args={'api_key':core_api_key,'offset':str(offset)}f=requests.get(base_url+search_terms,params=query_args)response=f.textroot=etree.fromstring(response)result_list=list(root.iter('record'))",backend/core.py
dissemin/dissemin,bb03947562f044fd5887b964223a1822a3813d75,requests,urllib,"  try:
        f = urlopen_retry(request, timeout=crossref_timeout, headers={'Connection':'Keep-Alive'})
        response = f.read()
        parsed = json.loads(response)
        result = []
        for dct in parsed:
            if citationToken and not ('fullCitation' in dct and citationToken in iunaccent(dct['fullCitation'])):
                continue
            if 'doi' in dct and 'title' in dct:
                parsed = to_doi(dct['doi'])
                if parsed and dct['title']:
                    result.append(parsed)
        return result
    except ValueError as e:
        raise MetadataSourceException('Error while fetching metadata:\nInvalid response.\n'+
                'URL was: %s\nJSON parser error was: %s' % (request,unicode(e))) 
    except MetadataSourceException as e:
        raise MetadataSourceException('Error while fetching metadata:\nUnable to open the URL: '+
                request+'\nError was: '+str(e))","try:
        response = urlopen_retry(request, timeout=crossref_timeout)

        parsed = json.loads(response)
        result = []
        for dct in parsed:
            if citationToken and not ('fullCitation' in dct and citationToken in iunaccent(dct['fullCitation'])):
                continue
            if 'doi' in dct and 'title' in dct:
                parsed = to_doi(dct['doi'])
                if parsed and dct['title']:
                    result.append(parsed)
        return result
    except ValueError as e:
        raise MetadataSourceException('Error while fetching metadata:\nInvalid response.\n'+
                'URL was: %s\nJSON parser error was: %s' % (request,unicode(e))) 
    except MetadataSourceException as e:
        raise MetadataSourceException('Error while fetching metadata:\nUnable to open the URL: '+
                request+'\nError was: '+str(e))",update,"try:f=urlopen_retry(request,timeout=crossref_timeout,headers={'Connection':'Keep-Alive'})response=f.read()parsed=json.loads(response)result=[]fordctinparsed:ifcitationTokenandnot('fullCitation'indctandcitationTokeniniunaccent(dct['fullCitation'])):continueif'doi'indctand'title'indct:parsed=to_doi(dct['doi'])ifparsedanddct['title']:result.append(parsed)returnresultexceptValueErrorase:raiseMetadataSourceException('Errorwhilefetchingmetadata:\nInvalidresponse.\n'+'URLwas:%s\nJSONparsererrorwas:%s'%(request,unicode(e)))exceptMetadataSourceExceptionase:raiseMetadataSourceException('Errorwhilefetchingmetadata:\nUnabletoopentheURL:'+request+'\nErrorwas:'+str(e))","try:response=urlopen_retry(request,timeout=crossref_timeout)parsed=json.loads(response)result=[]fordctinparsed:ifcitationTokenandnot('fullCitation'indctandcitationTokeniniunaccent(dct['fullCitation'])):continueif'doi'indctand'title'indct:parsed=to_doi(dct['doi'])ifparsedanddct['title']:result.append(parsed)returnresultexceptValueErrorase:raiseMetadataSourceException('Errorwhilefetchingmetadata:\nInvalidresponse.\n'+'URLwas:%s\nJSONparsererrorwas:%s'%(request,unicode(e)))exceptMetadataSourceExceptionase:raiseMetadataSourceException('Errorwhilefetchingmetadata:\nUnabletoopentheURL:'+request+'\nErrorwas:'+str(e))",backend/crossref.py
dissemin/dissemin,bb03947562f044fd5887b964223a1822a3813d75,requests,urllib,"    try:
        response = urlopen(request).read()
    except URLError as e:
        raise MetadataSourceException('Error while querying RoMEO.\n'+
                'URL was: '+request+'\n'
                'Error is: '+str(e))"," try:
        response = requests.get(base_url, params=search_terms).text
    except requests.exceptions.RequestException as e:
        raise MetadataSourceException('Error while querying RoMEO.\n'+
                'URL was: '+request+'\n'
                'Error is: '+str(e))",update,try:response=urlopen(request).read()exceptURLErrorase:raiseMetadataSourceException('ErrorwhilequeryingRoMEO.\n'+'URLwas:'+request+'\n''Erroris:'+str(e)),"try:response=requests.get(base_url,params=search_terms).textexceptrequests.exceptions.RequestExceptionase:raiseMetadataSourceException('ErrorwhilequeryingRoMEO.\n'+'URLwas:'+request+'\n''Erroris:'+str(e))",backend/romeo.py
egirault/googleplay-api,7d278c6702f3b3dba271c86da7ebfee50ba343d3,requests,urllib,"try:
                data = urllib2.urlopen(self.URL_LOGIN, urllib.urlencode(params)).read()
                data = data.split()
                params = {}
                for d in data:
                    if not ""="" in d: continue
                    k, v = d.split(""="")
                    params[k.strip().lower()] = v.strip()
                if ""auth"" in params:
                    self.setAuthSubToken(params[""auth""])
                else:
                    raise LoginError(""Auth token not found."")
            except urllib2.HTTPError, e:
                if e.code == 403:
                    data = e.fp.read().split()
                    params = {}
                    for d in data:
                        k, v = d.split(""="", 1)
                        params[k.strip().lower()] = v.strip()
                    if ""error"" in params:
                        raise LoginError(params[""error""])
                    else:
                        raise LoginError(""Login failed."")
                else:
                    raise e","            headers = {
                ""Accept-Encoding"": """",
            }
            response = requests.post(self.URL_LOGIN, data=params, headers=headers, verify=False)
            data = response.text.split()
            params = {}
            for d in data:
                if not ""="" in d: continue
                k, v = d.split(""="")
                params[k.strip().lower()] = v.strip()
            if ""auth"" in params:
                self.setAuthSubToken(params[""auth""])
            elif ""error"" in params:
                raise LoginError(""server says: "" + params[""error""])
            else:
                raise LoginError(""Auth token not found."")",update,"try:data=urllib2.urlopen(self.URL_LOGIN,urllib.urlencode(params)).read()data=data.split()params={}fordindata:ifnot""=""ind:continuek,v=d.split(""="")params[k.strip().lower()]=v.strip()if""auth""inparams:self.setAuthSubToken(params[""auth""])else:raiseLoginError(""Authtokennotfound."")excepturllib2.HTTPError,e:ife.code==403:data=e.fp.read().split()params={}fordindata:k,v=d.split(""="",1)params[k.strip().lower()]=v.strip()if""error""inparams:raiseLoginError(params[""error""])else:raiseLoginError(""Loginfailed."")else:raisee","headers={""Accept-Encoding"":"""",}response=requests.post(self.URL_LOGIN,data=params,headers=headers,verify=False)data=response.text.split()params={}fordindata:ifnot""=""ind:continuek,v=d.split(""="")params[k.strip().lower()]=v.strip()if""auth""inparams:self.setAuthSubToken(params[""auth""])elif""error""inparams:raiseLoginError(""serversays:""+params[""error""])else:raiseLoginError(""Authtokennotfound."")",googleplay.py
egirault/googleplay-api,7d278c6702f3b3dba271c86da7ebfee50ba343d3,requests,urllib," def search(self, query, nb_results=None, offset=None):
        """"""Search for apps.""""""
        path = ""search?c=3&q=%s"" % urllib.quote_plus(query)     # TODO handle categories
        if (nb_results is not None):
            path += ""&n=%d"" % int(nb_results)
        if (offset is not None):","    def search(self, query, nb_results=None, offset=None):
        """"""Search for apps.""""""
        path = ""search?c=3&q=%s"" % requests.utils.quote(query) # TODO handle categories
        if (nb_results is not None):
            path += ""&n=%d"" % int(nb_results)
        if (offset is not None):",update,"defsearch(self,query,nb_results=None,offset=None):""""""Searchforapps.""""""path=""search?c=3&q=%s""%urllib.quote_plus(query)#TODOhandlecategoriesif(nb_resultsisnotNone):path+=""&n=%d""%int(nb_results)if(offsetisnotNone):","defsearch(self,query,nb_results=None,offset=None):""""""Searchforapps.""""""path=""search?c=3&q=%s""%requests.utils.quote(query)#TODOhandlecategoriesif(nb_resultsisnotNone):path+=""&n=%d""%int(nb_results)if(offsetisnotNone):",googleplay.py
egirault/googleplay-api,7d278c6702f3b3dba271c86da7ebfee50ba343d3,requests,urllib,"def details(self, packageName):
        """"""Get app details from a package name.
        packageName is the app unique ID (usually starting with 'com.').""""""
        path = ""details?doc=%s"" % urllib.quote_plus(packageName)
        message = self.executeRequestApi2(path)
        return message.payload.detailsResponse"," def details(self, packageName):
        """"""Get app details from a package name.
        packageName is the app unique ID (usually starting with 'com.').""""""
        path = ""details?doc=%s"" % requests.utils.quote(packageName)
        message = self.executeRequestApi2(path)
        return message.payload.detailsResponse",update,"defdetails(self,packageName):""""""Getappdetailsfromapackagename.packageNameistheappuniqueID(usuallystartingwith'com.').""""""path=""details?doc=%s""%urllib.quote_plus(packageName)message=self.executeRequestApi2(path)returnmessage.payload.detailsResponse","defdetails(self,packageName):""""""Getappdetailsfromapackagename.packageNameistheappuniqueID(usuallystartingwith'com.').""""""path=""details?doc=%s""%requests.utils.quote(packageName)message=self.executeRequestApi2(path)returnmessage.payload.detailsResponse",googleplay.py
egirault/googleplay-api,7d278c6702f3b3dba271c86da7ebfee50ba343d3,requests,urllib,"if (cat != None):
            path += ""&cat=%s"" % urllib.quote_plus(cat)
        if (ctr != None):
            path += ""&ctr=%s"" % urllib.quote_plus(ctr)
        message = self.executeRequestApi2(path)
        return message.payload.browseResponse","        if (cat != None):
            path += ""&cat=%s"" % requests.utils.quote(cat)
        if (ctr != None):
            path += ""&ctr=%s"" % requests.utils.quote(ctr)
        message = self.executeRequestApi2(path)
        return message.payload.browseResponse",update,"if(cat!=None):path+=""&cat=%s""%urllib.quote_plus(cat)if(ctr!=None):path+=""&ctr=%s""%urllib.quote_plus(ctr)message=self.executeRequestApi2(path)returnmessage.payload.browseResponse","if(cat!=None):path+=""&cat=%s""%requests.utils.quote(cat)if(ctr!=None):path+=""&ctr=%s""%requests.utils.quote(ctr)message=self.executeRequestApi2(path)returnmessage.payload.browseResponse",googleplay.py
egirault/googleplay-api,7d278c6702f3b3dba271c86da7ebfee50ba343d3,requests,urllib," path = ""list?c=3&cat=%s"" % urllib.quote_plus(cat)
        if (ctr != None):
            path += ""&ctr=%s"" % urllib.quote_plus(ctr)
        if (nb_results != None):
            path += ""&n=%s"" % urllib.quote_plus(nb_results)
        if (offset != None):
            path += ""&o=%s"" % urllib.quote_plus(offset)
        message = self.executeRequestApi2(path)
        return message.payload.listResponse"," path = ""list?c=3&cat=%s"" % requests.utls.quote(cat)
        if (ctr != None):
            path += ""&ctr=%s"" % requests.utils.quote(ctr)
        if (nb_results != None):
            path += ""&n=%s"" % requests.utils.quote(nb_results)
        if (offset != None):
            path += ""&o=%s"" % requests.utils.quote(offset)
        message = self.executeRequestApi2(path)
        return message.payload.listResponse",update,"path=""list?c=3&cat=%s""%urllib.quote_plus(cat)if(ctr!=None):path+=""&ctr=%s""%urllib.quote_plus(ctr)if(nb_results!=None):path+=""&n=%s""%urllib.quote_plus(nb_results)if(offset!=None):path+=""&o=%s""%urllib.quote_plus(offset)message=self.executeRequestApi2(path)returnmessage.payload.listResponse","path=""list?c=3&cat=%s""%requests.utls.quote(cat)if(ctr!=None):path+=""&ctr=%s""%requests.utils.quote(ctr)if(nb_results!=None):path+=""&n=%s""%requests.utils.quote(nb_results)if(offset!=None):path+=""&o=%s""%requests.utils.quote(offset)message=self.executeRequestApi2(path)returnmessage.payload.listResponse",googleplay.py
egirault/googleplay-api,7d278c6702f3b3dba271c86da7ebfee50ba343d3,requests,urllib,"headers = {""Cookie"" : ""%s=%s"" % (cookie.name, cookie.value),
                             ""User-Agent"" : ""AndroidDownloadManager/4.1.1 (Linux; U; Android 4.1.1; Nexus S Build/JRO03E)""}
        request = urllib2.Request(url, None, headers)
        data = urllib2.urlopen(request).read()
        return data","headers = {""Cookie"" : ""%s=%s"" % (cookie.name, cookie.value),
                   ""User-Agent"" : ""AndroidDownloadManager/4.1.1 (Linux; U; Android 4.1.1; Nexus S Build/JRO03E)"",
                   ""Accept-Encoding"": """",
                  }
        response = requests.get(url, headers=headers) # TODO might be POST ?
        return response.content",update,"headers={""Cookie"":""%s=%s""%(cookie.name,cookie.value),""User-Agent"":""AndroidDownloadManager/4.1.1(Linux;U;Android4.1.1;NexusSBuild/JRO03E)""}request=urllib2.Request(url,None,headers)data=urllib2.urlopen(request).read()returndata","headers={""Cookie"":""%s=%s""%(cookie.name,cookie.value),""User-Agent"":""AndroidDownloadManager/4.1.1(Linux;U;Android4.1.1;NexusSBuild/JRO03E)"",""Accept-Encoding"":"""",}response=requests.get(url,headers=headers)#TODOmightbePOST?returnresponse.content",googleplay.py
ericmandel/pyjs9,054c33e4a451b8b516baf81b9faa9ccc510090c5,requests,urllib," def send(self, obj, msg='msg'):
        """"""
        :obj: dictionary containing command and args keys

        :rtype: returned data or info (in format specified by public api)

        examples:
        >>> js9 = pyjs9.JS9()
        >>> js9.send({'cmd': 'GetColormap'})
        {u'bias': 0.5, u'colormap': u'cool', u'contrast': 1}
        >>> js9.send({'cmd': 'SetColormap', 'args': ['red']})
        'OK'
        """"""
        if obj is None:
            obj = {}
        obj['id'] = self.id
        jstr = json.dumps(obj)
        try:
            url = urllib.urlopen(self.host + '/' + msg, jstr)
        except IOError as e:
            raise IOError(""{0}: {1}"".format(self.host, e.strerror))
        urtn = url.read()
        if urtn[0:6] == 'ERROR:':

            raise ValueError(urtn)
        try:



            res = json.loads(urtn, object_hook=_decode_dict)
        except ValueError:   # not json
            res = urtn","def send(self, obj, msg='msg'):
        """"""
        :obj: dictionary containing command and args keys

        :rtype: returned data or info (in format specified by public api)

        examples:
        >>> js9 = pyjs9.JS9()
        >>> js9.send({'cmd': 'GetColormap'})
        {u'bias': 0.5, u'colormap': u'cool', u'contrast': 1}
        >>> js9.send({'cmd': 'SetColormap', 'args': ['red']})
        'OK'
        """"""
        if obj is None:
            obj = {}
        obj['id'] = self.id
        jstr = json.dumps(obj)
        try:
            url = requests.get(self.host + '/' + msg, params=jstr)
        except IOError as e:
            raise IOError(""Cannot connect to {0}: {1}"".format(self.host,
                                                              e.strerror))
        urtn = url.text
        if 'ERROR:' in urtn:
            raise ValueError(urtn)
        try:
            # TODO: url.json() decode the json for us:
            # http://www.python-requests.org/en/latest/user/quickstart/#json-response-content
            # res = url.json()
            res = json.loads(urtn, object_hook=_decode_dict)
        except ValueError:   # not json
            res = urtn",update,"defsend(self,obj,msg='msg'):"""""":obj:dictionarycontainingcommandandargskeys:rtype:returneddataorinfo(informatspecifiedbypublicapi)examples:>>>js9=pyjs9.JS9()>>>js9.send({'cmd':'GetColormap'}){u'bias':0.5,u'colormap':u'cool',u'contrast':1}>>>js9.send({'cmd':'SetColormap','args':['red']})'OK'""""""ifobjisNone:obj={}obj['id']=self.idjstr=json.dumps(obj)try:url=urllib.urlopen(self.host+'/'+msg,jstr)exceptIOErrorase:raiseIOError(""{0}:{1}"".format(self.host,e.strerror))urtn=url.read()ifurtn[0:6]=='ERROR:':raiseValueError(urtn)try:res=json.loads(urtn,object_hook=_decode_dict)exceptValueError:#notjsonres=urtn","defsend(self,obj,msg='msg'):"""""":obj:dictionarycontainingcommandandargskeys:rtype:returneddataorinfo(informatspecifiedbypublicapi)examples:>>>js9=pyjs9.JS9()>>>js9.send({'cmd':'GetColormap'}){u'bias':0.5,u'colormap':u'cool',u'contrast':1}>>>js9.send({'cmd':'SetColormap','args':['red']})'OK'""""""ifobjisNone:obj={}obj['id']=self.idjstr=json.dumps(obj)try:url=requests.get(self.host+'/'+msg,params=jstr)exceptIOErrorase:raiseIOError(""Cannotconnectto{0}:{1}"".format(self.host,e.strerror))urtn=url.textif'ERROR:'inurtn:raiseValueError(urtn)try:#TODO:url.json()decodethejsonforus:#http://www.python-requests.org/en/latest/user/quickstart/#json-response-content#res=url.json()res=json.loads(urtn,object_hook=_decode_dict)exceptValueError:#notjsonres=urtn",pyjs9/__init__.py
evilhero/mylar,e971620b43a984fbae483ed1bec60acd03962563,requests,urllib,"def _get_token(self):
        url = urlparse.urljoin(self.base_url, 'gui/token.html')

        try:
            response = self.opener.open(url)
        except urllib2.HTTPError as err:
            logger.debug('URL: ' + str(url))
            logger.debug('Error getting Token. uTorrent responded with error: ' + str(err))
            return
        match = re.search(utorrentclient.TOKEN_REGEX, response.read())
        return match.group(1)","def _get_token(self):
        TOKEN_REGEX = r'<div[^>]*id=[\""\']token[\""\'][^>]*>([^<]*)</div>'
        utorrent_url_token = '%stoken.html' % self.utorrent_url
        try:
            r = requests.get(utorrent_url_token, auth=self.auth)
        except requests.exceptions.RequestException as err:
            logger.debug('URL: ' + str(utorrent_url_token))
            logger.debug('Error getting Token. uTorrent responded with error: ' + str(err))
            return 'fail'",update,"def_get_token(self):url=urlparse.urljoin(self.base_url,'gui/token.html')try:response=self.opener.open(url)excepturllib2.HTTPErroraserr:logger.debug('URL:'+str(url))logger.debug('ErrorgettingToken.uTorrentrespondedwitherror:'+str(err))returnmatch=re.search(utorrentclient.TOKEN_REGEX,response.read())returnmatch.group(1)","def_get_token(self):TOKEN_REGEX=r'<div[^>]*id=[\""\']token[\""\'][^>]*>([^<]*)</div>'utorrent_url_token='%stoken.html'%self.utorrent_urltry:r=requests.get(utorrent_url_token,auth=self.auth)exceptrequests.exceptions.RequestExceptionaserr:logger.debug('URL:'+str(utorrent_url_token))logger.debug('ErrorgettingToken.uTorrentrespondedwitherror:'+str(err))return'fail'",mylar/utorrent.py
explosion/spaCy,d4cc736b7c8f042e2385a16816aa3f9316478f8c,requests,urllib,"def get_json(url, desc, ssl_verify):
    try:
        data = url_read(url, verify=ssl_verify)
    except HTTPError as e:
        prints(Messages.M004.format(desc, about.__version__),
               title=Messages.M003.format(e.code, e.reason), exits=1)
    return ujson.loads(data)","def get_json(url, desc):
    r = requests.get(url)
    if r.status_code != 200:
        prints(Messages.M004.format(desc=desc, version=about.__version__),
               title=Messages.M003.format(code=r.status_code), exits=1)
    return r.json()",update,"defget_json(url,desc,ssl_verify):try:data=url_read(url,verify=ssl_verify)exceptHTTPErrorase:prints(Messages.M004.format(desc,about.__version__),title=Messages.M003.format(e.code,e.reason),exits=1)returnujson.loads(data)","defget_json(url,desc):r=requests.get(url)ifr.status_code!=200:prints(Messages.M004.format(desc=desc,version=about.__version__),title=Messages.M003.format(code=r.status_code),exits=1)returnr.json()",spacy/cli/download.py
explosion/spaCy,d4cc736b7c8f042e2385a16816aa3f9316478f8c,requests,urllib," try:
        data = url_read(about.__compatibility__)
    except HTTPError as e:
        title = Messages.M003.format(code=e.code, desc=e.reason)
        prints(Messages.M021, title=title, exits=1)
    compat = ujson.loads(data)['spacy']","r = requests.get(about.__compatibility__)
    if r.status_code != 200:
        prints(Messages.M021, title=Messages.M003.format(code=r.status_code),
               exits=1)
    compat = r.json()['spacy']",update,"try:data=url_read(about.__compatibility__)exceptHTTPErrorase:title=Messages.M003.format(code=e.code,desc=e.reason)prints(Messages.M021,title=title,exits=1)compat=ujson.loads(data)['spacy']","r=requests.get(about.__compatibility__)ifr.status_code!=200:prints(Messages.M021,title=Messages.M003.format(code=r.status_code),exits=1)compat=r.json()['spacy']",spacy/cli/validate.py
explosion/spaCy,d4cc736b7c8f042e2385a16816aa3f9316478f8c,requests,urllib,"    url_open = urllib.request.urlopen


def url_read(url):
    file_ = url_open(url)
    code = file_.getcode()
    if code != 200:
        raise HTTPError(url, code, ""Cannot GET url"", [], file_)
    data = file_.read()
    return data",,delete,"url_open=urllib.request.urlopendefurl_read(url):file_=url_open(url)code=file_.getcode()ifcode!=200:raiseHTTPError(url,code,""CannotGETurl"",[],file_)data=file_.read()returndata",,spacy/cli/_messages.py
fboender/sec-tools,9108f28398f37b0cca4026d6efa30989dcf3655a,requests,urllib,"def get_url_headers(url):
    ctx = ssl.create_default_context()
    # Turn off SSL validation, because we don't care for headers
    ctx.check_hostname = False
    ctx.verify_mode = ssl.CERT_NONE
    response = urllib.request.urlopen(url, context=ctx, timeout=4)

    result = {}
    for header in headers:
        result[header] = response.getheader(header)
    return result","def get_url_headers(url):
    r = requests.get(url, headers={""User-Agent"": ""curl/7.58.0""})





    result = {}
    for header in headers:
        result[header] = r.headers.get(header)
    return result",update,"defget_url_headers(url):ctx=ssl.create_default_context()#TurnoffSSLvalidation,becausewedon'tcareforheadersctx.check_hostname=Falsectx.verify_mode=ssl.CERT_NONEresponse=urllib.request.urlopen(url,context=ctx,timeout=4)result={}forheaderinheaders:result[header]=response.getheader(header)returnresult","defget_url_headers(url):r=requests.get(url,headers={""User-Agent"":""curl/7.58.0""})result={}forheaderinheaders:result[header]=r.headers.get(header)returnresult",src/bins/gather_http_headers.py
ffreitasalves/python-address-to-coordinates,3bcee5bc763bcfc4ac9cd565967052e74b6593bc,requests,urllib,"def coordinates(address):
    url = 'http://maps.google.com/maps/api/geocode/json?address=%s&sensor=false' % address
    coord = simplejson.load(
        urllib.urlopen(url)
    )

    if coord['status'] == 'OK':
        return {
            'lat': coord['results'][0]['geometry']['location']['lat'],
            'lng': coord['results'][0]['geometry']['location']['lng'],
        }
    else:
        return {
            'lat': '',
            'lng': '',
            }","def coordinates(address, api_key = None):
    url = 'https://maps.googleapis.com/maps/api/geocode/json'

    params = {
        'address' : address.encode('ascii', 'xmlcharrefreplace'),
        'sensor' : 'false'
    }

    if api_key:
        params['key'] = api_key

    response = requests.get(url, params=params)
    coord = response.json()

    if coord['status'] == 'OVER_QUERY_LIMIT':
        raise RuntimeError(coord['error_message'])

    if coord['status'] == 'OK':
        return {
            'lat': coord['results'][0]['geometry']['location']['lat'],
            'lng': coord['results'][0]['geometry']['location']['lng'],
        }
    else:
        return {
            'lat': '',
            'lng': '',
            }",update,"defcoordinates(address):url='http://maps.google.com/maps/api/geocode/json?address=%s&sensor=false'%addresscoord=simplejson.load(urllib.urlopen(url))ifcoord['status']=='OK':return{'lat':coord['results'][0]['geometry']['location']['lat'],'lng':coord['results'][0]['geometry']['location']['lng'],}else:return{'lat':'','lng':'',}","defcoordinates(address,api_key=None):url='https://maps.googleapis.com/maps/api/geocode/json'params={'address':address.encode('ascii','xmlcharrefreplace'),'sensor':'false'}ifapi_key:params['key']=api_keyresponse=requests.get(url,params=params)coord=response.json()ifcoord['status']=='OVER_QUERY_LIMIT':raiseRuntimeError(coord['error_message'])ifcoord['status']=='OK':return{'lat':coord['results'][0]['geometry']['location']['lat'],'lng':coord['results'][0]['geometry']['location']['lng'],}else:return{'lat':'','lng':'',}",script.py
finish06/pyunifi,3e534821de914f023995719060a8723f6186bef6,requests,urllib,,"        self.ssl_verify = ssl_verify
        # Disable the console warnings about an insecure connection
        if ssl_verify == False:
            requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)

        self.session = requests.Session()
        self.session.verify = ssl_verify",insert,,self.ssl_verify=ssl_verify#Disabletheconsolewarningsaboutaninsecureconnectionifssl_verify==False:requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)self.session=requests.Session()self.session.verify=ssl_verify,pyunifi/controller.py
finish06/pyunifi,3e534821de914f023995719060a8723f6186bef6,requests,urllib,"cj = cookielib.CookieJar()
        if PYTHON_VERSION == 2:
            self.opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))
        elif PYTHON_VERSION == 3:
            self.opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj))",,delete,cj=cookielib.CookieJar()ifPYTHON_VERSION==2:self.opener=urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))elifPYTHON_VERSION==3:self.opener=urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj)),,
finish06/pyunifi,3e534821de914f023995719060a8723f6186bef6,requests,urllib,"def _read(self, url, params=None):
        if PYTHON_VERSION == 3:
            if params is not None:
                params = ast.literal_eval(params)
                #print (params)
                params = urllib.parse.urlencode(params)
                params = params.encode('utf-8')
                res = self.opener.open(url, params)
            else:
                res = self.opener.open(url)
        elif PYTHON_VERSION == 2:
            res = self.opener.open(url, params)
        return self._jsondec(res.read())","    def _read(self, url, params=None):
        r = self.session.get(url, params=params)
        return self._jsondec(r.text)",update,"def_read(self,url,params=None):ifPYTHON_VERSION==3:ifparamsisnotNone:params=ast.literal_eval(params)#print(params)params=urllib.parse.urlencode(params)params=params.encode('utf-8')res=self.opener.open(url,params)else:res=self.opener.open(url)elifPYTHON_VERSION==2:res=self.opener.open(url,params)returnself._jsondec(res.read())","def_read(self,url,params=None):r=self.session.get(url,params=params)returnself._jsondec(r.text)",pyunifi/controller.py
finish06/pyunifi,3e534821de914f023995719060a8723f6186bef6,requests,urllib,,"def _write(self, url, json=None):
        r = self.session.post(url, json=json)
        return self._jsondec(r.text)",insert,,"def_write(self,url,json=None):r=self.session.post(url,json=json)returnself._jsondec(r.text)",pyunifi/controller.py
finish06/pyunifi,3e534821de914f023995719060a8723f6186bef6,requests,urllib,"if version == 'v4' or version == 'v5':
            login_url += 'api/login'
            params = json.dumps(params)

        else:
            login_url += 'login'
            params.update({'login': 'login'})
            if PYTHON_VERSION is 2:
                params = urllib.urlencode(params)
            elif PYTHON_VERSION is 3:
                params = urllib.parse.urlencode(params)

        if PYTHON_VERSION is 3:
            params = params.encode(""UTF-8"")

        self.opener.open(login_url, params).read()","if version == 'v4' or version == 'v5':
            login_url += 'api/login'
            # XXX Why doesn't passing in the dict work?
            params = ""{'username':'"" + self.username + ""', 'password':'"" + self.password + ""'}""
        else:
            login_url += 'login'
            params.update({'login': 'login'})





        r = self.session.post(login_url, params)


        if r.status_code != 200:
            errorstr = ""Failed to login: %d %s"" % (r.status_code, r.reason) 
            errorstr += '\n' + r.raw.read()
            log.error(errorstr)
            raise APIError(errorstr)",update,"ifversion=='v4'orversion=='v5':login_url+='api/login'params=json.dumps(params)else:login_url+='login'params.update({'login':'login'})ifPYTHON_VERSIONis2:params=urllib.urlencode(params)elifPYTHON_VERSIONis3:params=urllib.parse.urlencode(params)ifPYTHON_VERSIONis3:params=params.encode(""UTF-8"")self.opener.open(login_url,params).read()","ifversion=='v4'orversion=='v5':login_url+='api/login'#XXXWhydoesn'tpassinginthedictwork?params=""{'username':'""+self.username+""','password':'""+self.password+""'}""else:login_url+='login'params.update({'login':'login'})r=self.session.post(login_url,params)ifr.status_code!=200:errorstr=""Failedtologin:%d%s""%(r.status_code,r.reason)errorstr+='\n'+r.raw.read()log.error(errorstr)raiseAPIError(errorstr)",pyunifi/controller.py
finish06/pyunifi,3e534821de914f023995719060a8723f6186bef6,requests,urllib,"    def _logout(self):
        log.debug('logout()')

        self.opener.open(self.url + 'logout').read()","    def _logout(self):
        log.debug('logout()')
        self._write(self.url + 'logout')",update,def_logout(self):log.debug('logout()')self.opener.open(self.url+'logout').read(),def_logout(self):log.debug('logout()')self._write(self.url+'logout'),pyunifi/controller.py
finish06/pyunifi,3e534821de914f023995719060a8723f6186bef6,requests,urllib,"    def get_statistics_24h(self, endtime):
        """"""Return statistical data last 24h from time""""""

        js = json.dumps(
            {'attrs': [""bytes"", ""num_sta"", ""time""], 'start': int(endtime - 86400) * 1000, 'end': int(endtime - 3600) * 1000})
        params = urllib.urlencode({'json': js})
        return self._read(self.api_url + 'stat/report/hourly.system', params)","    def get_statistics_24h(self, endtime):
        """"""Return statistical data last 24h from time""""""

        js = { 'attrs': [""bytes"", ""num_sta"", ""time""], 
               'start': int(endtime - 86400) * 1000,
               'end': int(endtime - 3600) * 1000 }
        return self._write(self.api_url + 'stat/report/hourly.site', params)",update,"defget_statistics_24h(self,endtime):""""""Returnstatisticaldatalast24hfromtime""""""js=json.dumps({'attrs':[""bytes"",""num_sta"",""time""],'start':int(endtime-86400)*1000,'end':int(endtime-3600)*1000})params=urllib.urlencode({'json':js})returnself._read(self.api_url+'stat/report/hourly.system',params)","defget_statistics_24h(self,endtime):""""""Returnstatisticaldatalast24hfromtime""""""js={'attrs':[""bytes"",""num_sta"",""time""],'start':int(endtime-86400)*1000,'end':int(endtime-3600)*1000}returnself._write(self.api_url+'stat/report/hourly.site',params)",pyunifi/controller.py
finish06/pyunifi,3e534821de914f023995719060a8723f6186bef6,requests,urllib," def get_client(self, mac):
        """"""Get details about a specific client""""""
        try:
            return self._read(self.api_url + 'stat/sta/' + mac)[0]
        except urllib.error.HTTPError as err:
            if err.code == 401:
                try:
                    self._login('v4')
                except urllib.error.HTTPError as err2:
                    log.debug('Failed to scan client(s) -> auth issues: %s', err2)
            else:
                log.debug('Failed to scan client(s): %s', err)"," def get_client(self, mac):
        """"""Get details about a specific client""""""

        return self._read(self.api_url + 'stat/sta/' + mac)[0]








",update,"defget_client(self,mac):""""""Getdetailsaboutaspecificclient""""""try:returnself._read(self.api_url+'stat/sta/'+mac)[0]excepturllib.error.HTTPErroraserr:iferr.code==401:try:self._login('v4')excepturllib.error.HTTPErroraserr2:log.debug('Failedtoscanclient(s)->authissues:%s',err2)else:log.debug('Failedtoscanclient(s):%s',err)","defget_client(self,mac):""""""Getdetailsaboutaspecificclient""""""returnself._read(self.api_url+'stat/sta/'+mac)[0]",pyunifi/controller.py
finish06/pyunifi,3e534821de914f023995719060a8723f6186bef6,requests,urllib,"def get_clients(self):
        """"""Return a list of all active clients, with significant information about each.""""""
        try:
            return self._read(self.api_url + 'stat/sta')
        except urllib.error.HTTPError as err:
            if err.code == 401:
                try:
                    self._login('v4')
                except urllib.error.HTTPError as err2:
                    log.debug('Failed to scan client(s) -> auth issues: %s', err2)
            else:
                log.debug('Failed to scan client(s): %s', err)"," def get_clients(self):
        """"""Return a list of all active clients, with significant information about each.""""""

        return self._read(self.api_url + 'stat/sta')",update,"defget_clients(self):""""""Returnalistofallactiveclients,withsignificantinformationabouteach.""""""try:returnself._read(self.api_url+'stat/sta')excepturllib.error.HTTPErroraserr:iferr.code==401:try:self._login('v4')excepturllib.error.HTTPErroraserr2:log.debug('Failedtoscanclient(s)->authissues:%s',err2)else:log.debug('Failedtoscanclient(s):%s',err)","defget_clients(self):""""""Returnalistofallactiveclients,withsignificantinformationabouteach.""""""returnself._read(self.api_url+'stat/sta')",pyunifi/controller.py
finish06/pyunifi,3e534821de914f023995719060a8723f6186bef6,requests,urllib,"def _run_command(self, command, params={}, mgr='stamgr'):
        log.debug('_run_command(%s)', command)
        params.update({'cmd': command})
        if PYTHON_VERSION == 2:
            return self._read(self.api_url + 'cmd/' + mgr, urllib.urlencode({'json': json.dumps(params)}))
        elif PYTHON_VERSION == 3:
            return self._read(self.api_url + 'cmd/' + mgr, urllib.parse.urlencode({'json': json.dumps(params)}))","    def _run_command(self, command, params={}, mgr='stamgr'):
        log.debug('_run_command(%s)', command)
        params.update({'cmd': command})
        return self._write(self.api_url + 'cmd/' + mgr, json=params)


",update,"def_run_command(self,command,params={},mgr='stamgr'):log.debug('_run_command(%s)',command)params.update({'cmd':command})ifPYTHON_VERSION==2:returnself._read(self.api_url+'cmd/'+mgr,urllib.urlencode({'json':json.dumps(params)}))elifPYTHON_VERSION==3:returnself._read(self.api_url+'cmd/'+mgr,urllib.parse.urlencode({'json':json.dumps(params)}))","def_run_command(self,command,params={},mgr='stamgr'):log.debug('_run_command(%s)',command)params.update({'cmd':command})returnself._write(self.api_url+'cmd/'+mgr,json=params)",pyunifi/controller.py
finish06/pyunifi,3e534821de914f023995719060a8723f6186bef6,requests,urllib,"    def archive_all_alerts(self):
        """"""Archive all Alerts
        """"""
        js = json.dumps({'cmd': 'archive-all-alarms'})
        params = urllib.urlencode({'json': js})
        answer = self._read(self.api_url + 'cmd/evtmgr', params)","    def archive_all_alerts(self):
        """"""Archive all Alerts
        """"""
        js = {'cmd': 'archive-all-alarms'}
        answer = self._read(self.api_url + 'cmd/evtmgr', json=js)",update,"defarchive_all_alerts(self):""""""ArchiveallAlerts""""""js=json.dumps({'cmd':'archive-all-alarms'})params=urllib.urlencode({'json':js})answer=self._read(self.api_url+'cmd/evtmgr',params)","defarchive_all_alerts(self):""""""ArchiveallAlerts""""""js={'cmd':'archive-all-alarms'}answer=self._read(self.api_url+'cmd/evtmgr',json=js)",pyunifi/controller.py
finish06/pyunifi,3e534821de914f023995719060a8723f6186bef6,requests,urllib,"    def create_backup(self):
        """"""Ask controller to create a backup archive file, response contains the path to the backup file.

        Warning: This process puts significant load on the controller may
                 render it partially unresponsive for other requests.
        """"""

        js = json.dumps({'cmd': 'backup'})
        params = urllib.urlencode({'json': js})
        answer = self._read(self.api_url + 'cmd/system', params)

        return answer[0].get('url')","def create_backup(self):
        """"""Ask controller to create a backup archive file, response contains the path to the backup file.

        Warning: This process puts significant load on the controller may
                 render it partially unresponsive for other requests.
        """"""

        js = {'cmd': 'backup'}
        r = self.session.post(self.api_url + 'cmd/system', json=js)


        data = self._jsondec(r.text)
        return data[0]['url']",update,"defcreate_backup(self):""""""Askcontrollertocreateabackuparchivefile,responsecontainsthepathtothebackupfile.Warning:Thisprocessputssignificantloadonthecontrollermayrenderitpartiallyunresponsiveforotherrequests.""""""js=json.dumps({'cmd':'backup'})params=urllib.urlencode({'json':js})answer=self._read(self.api_url+'cmd/system',params)returnanswer[0].get('url')","defcreate_backup(self):""""""Askcontrollertocreateabackuparchivefile,responsecontainsthepathtothebackupfile.Warning:Thisprocessputssignificantloadonthecontrollermayrenderitpartiallyunresponsiveforotherrequests.""""""js={'cmd':'backup'}r=self.session.post(self.api_url+'cmd/system',json=js)data=self._jsondec(r.text)returndata[0]['url']",pyunifi/controller.py
FCP-INDI/C-PAC,a0b38702cab01112752ba1870cb945946eef0388,boto3,boto,"import boto
    import boto.s3.connection

    # Init variables
    cf = boto.s3.connection.OrdinaryCallingFormat()



    # Get AWS credentials if a creds_path is specified
    if creds_path:
        aws_access_key_id, aws_secret_access_key = return_aws_keys(creds_path)







        # Init connection
        s3_conn = boto.connect_s3(aws_access_key_id, aws_secret_access_key,
                                  calling_format=cf)








    else:
        s3_conn= boto.connect_s3(anon=True, calling_format=cf)








    # And fetch the bucket with the name argument
    try:
        bucket = s3_conn.get_bucket(bucket_name)
    except S3ResponseError as exc:
        err_msg = 'Unable to connect to bucket: %s; check credentials or ' \
                  'bucket name spelling and try again. Error message: %s' \












                  % (bucket_name, exc)
        raise Exception(err_msg)

    # Return bucket
    return bucket","    try:
        import boto3
        import botocore
    except ImportError as exc:
        err_msg = 'Boto3 package is not installed - install boto3 and '\
                  'try again.'
        raise Exception(err_msg)

    # Try and get AWS credentials if a creds_path is specified
    if creds_path:
        try:
            aws_access_key_id, aws_secret_access_key = \
                return_aws_keys(creds_path)
        except Exception as exc:
            err_msg = 'There was a problem extracting the AWS credentials '\
                      'from the credentials file provided: %s. Error:\n%s'\
                      % (creds_path, exc)
            raise Exception(err_msg)
        # Init connection
        print 'Connecting to S3 bucket: %s with credentials from %s ...'\
              % (bucket_name, creds_path)
        # Use individual session for each instance of DataSink
        # Better when datasinks are being used in multi-threading, see:
        # http://boto3.readthedocs.org/en/latest/guide/resources.html#multithreading
        session = boto3.session.Session(aws_access_key_id=aws_access_key_id,
                                        aws_secret_access_key=aws_secret_access_key)
        s3_resource = session.resource('s3', use_ssl=True)

    # Otherwise, connect anonymously
    else:
        print 'Connecting to AWS: %s anonymously...' % bucket_name
        session = boto3.session.Session()
        s3_resource = session.resource('s3', use_ssl=True)
        s3_resource.meta.client.meta.events.register('choose-signer.s3.*',
                                                     botocore.handlers.disable_signing)

    # Explicitly declare a secure SSL connection for bucket object
    bucket = s3_resource.Bucket(bucket_name)

    # And try fetch the bucket with the name argument
    try:
        s3_resource.meta.client.head_bucket(Bucket=bucket_name)
    except botocore.exceptions.ClientError as exc:
        error_code = int(exc.response['Error']['Code'])
        if error_code == 403:
            err_msg = 'Access to bucket: %s is denied; check credentials'\
                      % bucket_name
            raise Exception(err_msg)
        elif error_code == 404:
            err_msg = 'Bucket: %s does not exist; check spelling and try '\
                      'again' % bucket_name
            raise Exception(err_msg)
        else:
            err_msg = 'Unable to connect to bucket: %s. Error message:\n%s'\
                      % (bucket_name, exc)
    except Exception as exc:
        err_msg = 'Unable to connect to bucket: %s. Error message:\n%s'\
                  % (bucket_name, exc)
        raise Exception(err_msg)

    # Return the bucket
    return bucket",update,"importbotoimportboto.s3.connection#Initvariablescf=boto.s3.connection.OrdinaryCallingFormat()#GetAWScredentialsifacreds_pathisspecifiedifcreds_path:aws_access_key_id,aws_secret_access_key=return_aws_keys(creds_path)#Initconnections3_conn=boto.connect_s3(aws_access_key_id,aws_secret_access_key,calling_format=cf)else:s3_conn=boto.connect_s3(anon=True,calling_format=cf)#Andfetchthebucketwiththenameargumenttry:bucket=s3_conn.get_bucket(bucket_name)exceptS3ResponseErrorasexc:err_msg='Unabletoconnecttobucket:%s;checkcredentialsor'\'bucketnamespellingandtryagain.Errormessage:%s'\%(bucket_name,exc)raiseException(err_msg)#Returnbucketreturnbucket","try:importboto3importbotocoreexceptImportErrorasexc:err_msg='Boto3packageisnotinstalled-installboto3and'\'tryagain.'raiseException(err_msg)#TryandgetAWScredentialsifacreds_pathisspecifiedifcreds_path:try:aws_access_key_id,aws_secret_access_key=\return_aws_keys(creds_path)exceptExceptionasexc:err_msg='TherewasaproblemextractingtheAWScredentials'\'fromthecredentialsfileprovided:%s.Error:\n%s'\%(creds_path,exc)raiseException(err_msg)#Initconnectionprint'ConnectingtoS3bucket:%swithcredentialsfrom%s...'\%(bucket_name,creds_path)#UseindividualsessionforeachinstanceofDataSink#Betterwhendatasinksarebeingusedinmulti-threading,see:#http://boto3.readthedocs.org/en/latest/guide/resources.html#multithreadingsession=boto3.session.Session(aws_access_key_id=aws_access_key_id,aws_secret_access_key=aws_secret_access_key)s3_resource=session.resource('s3',use_ssl=True)#Otherwise,connectanonymouslyelse:print'ConnectingtoAWS:%sanonymously...'%bucket_namesession=boto3.session.Session()s3_resource=session.resource('s3',use_ssl=True)s3_resource.meta.client.meta.events.register('choose-signer.s3.*',botocore.handlers.disable_signing)#ExplicitlydeclareasecureSSLconnectionforbucketobjectbucket=s3_resource.Bucket(bucket_name)#Andtryfetchthebucketwiththenameargumenttry:s3_resource.meta.client.head_bucket(Bucket=bucket_name)exceptbotocore.exceptions.ClientErrorasexc:error_code=int(exc.response['Error']['Code'])iferror_code==403:err_msg='Accesstobucket:%sisdenied;checkcredentials'\%bucket_nameraiseException(err_msg)eliferror_code==404:err_msg='Bucket:%sdoesnotexist;checkspellingandtry'\'again'%bucket_nameraiseException(err_msg)else:err_msg='Unabletoconnecttobucket:%s.Errormessage:\n%s'\%(bucket_name,exc)exceptExceptionasexc:err_msg='Unabletoconnecttobucket:%s.Errormessage:\n%s'\%(bucket_name,exc)raiseException(err_msg)#Returnthebucketreturnbucket",CPAC/AWS/fetch_creds.py
GeoNode/geonode,e9658b4ef958b97b916091fea01e161f2c8d59f3,boto3,boto,"def set_alpha_ip():
    config = readconfig()
    conn = boto.connect_ec2()
    # Assign elastic ip to instance
    instance_id = config.get('ec2', 'INSTANCE')
    conn.associate_address(instance_id=instance_id, public_ip=ALPHA_ELASTIC_IP)","def set_alpha_ip():
    config = readconfig()


    instance_id = config.get('ec2', 'INSTANCE')
    ec2 = boto3.client('ec2')
    try:
        allocation = ec2.allocate_address(Domain='vpc')
        response = ec2.associate_address(
            AllocationId=allocation['AllocationId'],
            InstanceId=instance_id,
            PublicIp=ALPHA_ELASTIC_IP)
        return response
    except botocore.exceptions.ClientError as e:
        print(e)
        return None",update,"defset_alpha_ip():config=readconfig()conn=boto.connect_ec2()#Assignelasticiptoinstanceinstance_id=config.get('ec2','INSTANCE')conn.associate_address(instance_id=instance_id,public_ip=ALPHA_ELASTIC_IP)","defset_alpha_ip():config=readconfig()instance_id=config.get('ec2','INSTANCE')ec2=boto3.client('ec2')try:allocation=ec2.allocate_address(Domain='vpc')response=ec2.associate_address(AllocationId=allocation['AllocationId'],InstanceId=instance_id,PublicIp=ALPHA_ELASTIC_IP)returnresponseexceptbotocore.exceptions.ClientErrorase:print(e)returnNone",scripts/cloud/ec2.py
GeoNode/geonode,e9658b4ef958b97b916091fea01e161f2c8d59f3,boto3,boto," 
    if launch:
        conn = boto.connect_ec2()
        image = conn.get_image(MY_AMI)
        security_groups = conn.get_all_security_groups()

        try:
            [geonode_group] = [x for x in security_groups if x.name == SECURITY_GROUP]
        except ValueError:
            # this probably means the security group is not defined
            # create the rules programatically to add access to ports 21, 22, 80, 2300-2400, 8000, 8001, 8021 and 8080
            geonode_group = conn.create_security_group(SECURITY_GROUP, 'Cool GeoNode rules')
            geonode_group.authorize('tcp', 21, 21, '0.0.0.0/0') # Batch Upload FTP
            geonode_group.authorize('tcp', 22, 22, '0.0.0.0/0') # SSH
            geonode_group.authorize('tcp', 80, 80, '0.0.0.0/0') # Apache
            geonode_group.authorize('tcp', 2300, 2400, '0.0.0.0/0') # Passive FTP 
            geonode_group.authorize('tcp', 8000, 8001, '0.0.0.0/0') # Dev Django and Jetty
            geonode_group.authorize('tcp', 8021, 8021, '0.0.0.0/0' ) # Batch Upload FTP
            geonode_group.authorize('tcp', 8080, 8080, '0.0.0.0/0' ) # Tomcat

        try:
            [geonode_key] = [x for x in conn.get_all_key_pairs() if x.name == 'geonode']
        except ValueError:
            # this probably means the key is not defined
            # get the first one in the belt for now:
            print ""GeoNode file not found in the server""
            geonode_key = conn.get_all_key_pairs()[0]

        reservation = image.run(security_groups=[geonode_group,], key_name=geonode_key.name, instance_type=INSTANCE_TYPE)
        instance = reservation.instances[0]

        print ""Firing up instance""

        # Give it 10 minutes to appear online
        for i in range(120):
            time.sleep(5)
            instance.update()
            print instance.state
            if instance.state == ""running"":
                break

        if instance.state == ""running"":
            dns = instance.dns_name
            print ""Instance up and running at %s"" % dns

        config.set('ec2', 'HOST', dns)
        config.set('ec2', 'INSTANCE', instance.id)
        writeconfig(config)
        
        print ""ssh -i %s ubuntu@%s"" % (KEY_PATH, dns)
        print ""Terminate the instance via the web interface %s"" % instance

        time.sleep(20)","if not launch:
        return

    ec2 = boto3.client('ec2')
    vpc_id = ec2.describe_vpcs().get('Vpcs', [{}])[0].get('VpcId', '')

    try:
        geonode_group = ec2.describe_security_groups(
            GroupNames=[group_name])['SecurityGroups'][0]
    except botocore.exceptions.ClientError:
        # Create the security group
        geonode_group = ec2.create_security_group(
            GroupName=group_name, Description='GeoNode rules.', VpcId=vpc_id)
        port_ranges = [
            (21, 21),  # Batch upload FTP
            (22, 22),  # SSH
            (80, 80),  # Apache
            (2300, 2400),  # Passive FTP
            (8000, 8001),  # Dev Django and Jetty
            (8021, 8021),  # Batch upload FTP
            (8080, 8080),  # Tomcat
        ]

        for from_port, to_port in port_ranges:
            ec2.authorize_security_group_ingress(
                GroupId=geonode_group['GroupId'],
                IpProtocol='tcp',
                FromPort=from_port,
                ToPort=to_port,
                CidrIp='0.0.0.0/0')

    try:
        key_pairs = ec2.describe_key_pairs(KeyNames=[key_name])['KeyPairs']
    except botocore.exceptions.ClientError:
        # Key is not likely not defined
        print(""GeoNode file not found in server."")
        key_pairs = ec2.describe_key_pairs()['KeyPairs']

    key = key_pairs[0] if len(
        key_pairs) > 0 else ec2.create_key_pair(KeyName=key_name)
    reservation = ec2.run_instances(
        ImageId=ami,
        InstanceType=instance_type,
        KeyName=key['KeyName'],
        MaxCount=1,
        MinCount=1,
        SecurityGroupIds=[
            geonode_group['GroupId']])
    instance_id = [instance['InstanceId']
                   for instance in reservation['Instances']
                   if instance['ImageId'] == ami][0]

    print(""Firing up instance..."")
    instance = wait_for_state(ec2, instance_id, 'running')
    dns = instance['PublicDnsName']
    print(""Instance running at %s"" % dns)

    config.set('ec2', 'HOST', dns)
    config.set('ec2', 'INSTANCE', instance_id)
    writeconfig(config)

    print(""ssh -i %s ubuntu@%s"" % (key_path, dns))
    print(""Terminate the instance via the web interface."")

    time.sleep(20)",update,"iflaunch:conn=boto.connect_ec2()image=conn.get_image(MY_AMI)security_groups=conn.get_all_security_groups()try:[geonode_group]=[xforxinsecurity_groupsifx.name==SECURITY_GROUP]exceptValueError:#thisprobablymeansthesecuritygroupisnotdefined#createtherulesprogramaticallytoaddaccesstoports21,22,80,2300-2400,8000,8001,8021and8080geonode_group=conn.create_security_group(SECURITY_GROUP,'CoolGeoNoderules')geonode_group.authorize('tcp',21,21,'0.0.0.0/0')#BatchUploadFTPgeonode_group.authorize('tcp',22,22,'0.0.0.0/0')#SSHgeonode_group.authorize('tcp',80,80,'0.0.0.0/0')#Apachegeonode_group.authorize('tcp',2300,2400,'0.0.0.0/0')#PassiveFTPgeonode_group.authorize('tcp',8000,8001,'0.0.0.0/0')#DevDjangoandJettygeonode_group.authorize('tcp',8021,8021,'0.0.0.0/0')#BatchUploadFTPgeonode_group.authorize('tcp',8080,8080,'0.0.0.0/0')#Tomcattry:[geonode_key]=[xforxinconn.get_all_key_pairs()ifx.name=='geonode']exceptValueError:#thisprobablymeansthekeyisnotdefined#getthefirstoneinthebeltfornow:print""GeoNodefilenotfoundintheserver""geonode_key=conn.get_all_key_pairs()[0]reservation=image.run(security_groups=[geonode_group,],key_name=geonode_key.name,instance_type=INSTANCE_TYPE)instance=reservation.instances[0]print""Firingupinstance""#Giveit10minutestoappearonlineforiinrange(120):time.sleep(5)instance.update()printinstance.stateifinstance.state==""running"":breakifinstance.state==""running"":dns=instance.dns_nameprint""Instanceupandrunningat%s""%dnsconfig.set('ec2','HOST',dns)config.set('ec2','INSTANCE',instance.id)writeconfig(config)print""ssh-i%subuntu@%s""%(KEY_PATH,dns)print""Terminatetheinstanceviathewebinterface%s""%instancetime.sleep(20)","ifnotlaunch:returnec2=boto3.client('ec2')vpc_id=ec2.describe_vpcs().get('Vpcs',[{}])[0].get('VpcId','')try:geonode_group=ec2.describe_security_groups(GroupNames=[group_name])['SecurityGroups'][0]exceptbotocore.exceptions.ClientError:#Createthesecuritygroupgeonode_group=ec2.create_security_group(GroupName=group_name,Description='GeoNoderules.',VpcId=vpc_id)port_ranges=[(21,21),#BatchuploadFTP(22,22),#SSH(80,80),#Apache(2300,2400),#PassiveFTP(8000,8001),#DevDjangoandJetty(8021,8021),#BatchuploadFTP(8080,8080),#Tomcat]forfrom_port,to_portinport_ranges:ec2.authorize_security_group_ingress(GroupId=geonode_group['GroupId'],IpProtocol='tcp',FromPort=from_port,ToPort=to_port,CidrIp='0.0.0.0/0')try:key_pairs=ec2.describe_key_pairs(KeyNames=[key_name])['KeyPairs']exceptbotocore.exceptions.ClientError:#Keyisnotlikelynotdefinedprint(""GeoNodefilenotfoundinserver."")key_pairs=ec2.describe_key_pairs()['KeyPairs']key=key_pairs[0]iflen(key_pairs)>0elseec2.create_key_pair(KeyName=key_name)reservation=ec2.run_instances(ImageId=ami,InstanceType=instance_type,KeyName=key['KeyName'],MaxCount=1,MinCount=1,SecurityGroupIds=[geonode_group['GroupId']])instance_id=[instance['InstanceId']forinstanceinreservation['Instances']ifinstance['ImageId']==ami][0]print(""Firingupinstance..."")instance=wait_for_state(ec2,instance_id,'running')dns=instance['PublicDnsName']print(""Instancerunningat%s""%dns)config.set('ec2','HOST',dns)config.set('ec2','INSTANCE',instance_id)writeconfig(config)print(""ssh-i%subuntu@%s""%(key_path,dns))print(""Terminatetheinstanceviathewebinterface."")time.sleep(20)",scripts/cloud/ec2.py
GeoNode/geonode,e9658b4ef958b97b916091fea01e161f2c8d59f3,boto3,boto,"def terminate():
    config = readconfig()
    instance_id = config.get('ec2', 'INSTANCE')
    conn = boto.connect_ec2()
    conn.get_all_instances()
    instance = None
    for reservation in conn.get_all_instances():
        for ins in reservation.instances:
            if ins.id == instance_id:
                instance = ins

    print 'Terminating instance'
    instance.terminate()
    # Give it 10 minutes to terminate
    for i in range(120):
        time.sleep(5)
        instance.update()
        print instance.state
        if instance.state == ""terminated"":
            config.set('ec2', 'HOST', '')
            config.set('ec2', 'INSTANCE', '')
            configfile = open(CONFIG_FILE, 'wb')
            config.write(configfile)
            configfile.close()
            break

if sys.argv[1] == ""launch_geonode"":
    launch_geonode()
elif sys.argv[1] == ""launch_base"":
    launch_base()
elif sys.argv[1] == ""set_alpha_ip"":
    set_alpha_ip()
elif sys.argv[1] == ""terminate"":
    terminate()
elif sys.argv[1] == ""host"":
    config = readconfig()
    print config.get('ec2', 'HOST')
elif sys.argv[1] == ""key"":
    config = readconfig()
    print config.get('ec2', 'KEY_PATH')
else:
    print ""Usage:\n    python %s launch_base\n     python %s launch_geonode\n    python %s terminate"" % (sys.argv[0], sys.argv[0], sys.argv[0])","def terminate():
    config = readconfig()
    instance_id = config.get('ec2', 'INSTANCE')

    ec2 = boto3.client('ec2')
    ec2.terminate_instances(InstanceIds=[instance_id])

    print(""Terminating instance..."")
    wait_for_state(ec2, instance_id, 'terminated')
    print(""Instance terminated."")

    config.set('ec2', 'HOST', '')
    config.set('ec2', 'INSTANCE', '')
    configfile = open(CONFIG_FILE, 'wb')
    config.write(configfile)
    configfile.close()


if __name__ == '__main__':
    if sys.argv[1] == ""launch_geonode"":
        launch_geonode()
    elif sys.argv[1] == ""launch_base"":
        launch_base()
    elif sys.argv[1] == ""set_alpha_ip"":
        set_alpha_ip()
    elif sys.argv[1] == ""terminate"":
        terminate()
    elif sys.argv[1] == ""host"":
        config = readconfig()
        print config.get('ec2', 'HOST')
    elif sys.argv[1] == ""key"":
        config = readconfig()
        print config.get('ec2', 'KEY_PATH')
    else:
        print(""Usage:\n    "" + 
              ""python %s launch_base\n     "" +
              ""python %s launch_geonode\n    "" +
              ""python %s terminate"" % (sys.argv[0], sys.argv[0], sys.argv[0]))


",update,"defterminate():config=readconfig()instance_id=config.get('ec2','INSTANCE')conn=boto.connect_ec2()conn.get_all_instances()instance=Noneforreservationinconn.get_all_instances():forinsinreservation.instances:ifins.id==instance_id:instance=insprint'Terminatinginstance'instance.terminate()#Giveit10minutestoterminateforiinrange(120):time.sleep(5)instance.update()printinstance.stateifinstance.state==""terminated"":config.set('ec2','HOST','')config.set('ec2','INSTANCE','')configfile=open(CONFIG_FILE,'wb')config.write(configfile)configfile.close()breakifsys.argv[1]==""launch_geonode"":launch_geonode()elifsys.argv[1]==""launch_base"":launch_base()elifsys.argv[1]==""set_alpha_ip"":set_alpha_ip()elifsys.argv[1]==""terminate"":terminate()elifsys.argv[1]==""host"":config=readconfig()printconfig.get('ec2','HOST')elifsys.argv[1]==""key"":config=readconfig()printconfig.get('ec2','KEY_PATH')else:print""Usage:\npython%slaunch_base\npython%slaunch_geonode\npython%sterminate""%(sys.argv[0],sys.argv[0],sys.argv[0])","defterminate():config=readconfig()instance_id=config.get('ec2','INSTANCE')ec2=boto3.client('ec2')ec2.terminate_instances(InstanceIds=[instance_id])print(""Terminatinginstance..."")wait_for_state(ec2,instance_id,'terminated')print(""Instanceterminated."")config.set('ec2','HOST','')config.set('ec2','INSTANCE','')configfile=open(CONFIG_FILE,'wb')config.write(configfile)configfile.close()if__name__=='__main__':ifsys.argv[1]==""launch_geonode"":launch_geonode()elifsys.argv[1]==""launch_base"":launch_base()elifsys.argv[1]==""set_alpha_ip"":set_alpha_ip()elifsys.argv[1]==""terminate"":terminate()elifsys.argv[1]==""host"":config=readconfig()printconfig.get('ec2','HOST')elifsys.argv[1]==""key"":config=readconfig()printconfig.get('ec2','KEY_PATH')else:print(""Usage:\n""+""python%slaunch_base\n""+""python%slaunch_geonode\n""+""python%sterminate""%(sys.argv[0],sys.argv[0],sys.argv[0]))",scripts/cloud/ec2.py
GeoNode/geonode,e9658b4ef958b97b916091fea01e161f2c8d59f3,boto3,boto,"import os, sys
import boto
from boto.s3.key import Key


bucket_name = sys.argv[1] 
file_name = sys.argv[2] 

conn = boto.connect_s3(os.environ['AWS_ACCESS_KEY_ID'], os.environ['AWS_SECRET_ACCESS_KEY'])
bucket = conn.get_bucket(bucket_name)

k = Key(bucket)
k.key = file_name.split('/')[-1]
k.set_contents_from_filename(file_name)
k.set_acl('public-read')


print file_name + "" uploaded to "" + bucket_name","import os.path
import sys
import boto3
import botocore




def upload_file_s3(filename, bucket, obj_name=None):
    """"""Upload a file to an S3 bucket

    :param filename: File to upload
    :param bucket: Bucket to upload to
    :param object_name: S3 object name. If not specified, filename is used
    :return None if upload was successful, otherwise the associated error code
    """"""

    if obj_name is None:
        obj_name = filename

    s3_client = boto3.client('s3')
    try:
        s3_client.upload_file(filename, bucket, obj_name)
    except botocore.exceptions.ClientError as e:
        error_code = e.response['Error']['Code']
        return error_code

    return None


if __name__ == '__main__':
    try:
        _, bucket_name, filepath = sys.argv
    except ValueError:
        print(""Usage:\n    python %s bucket_name filepath"" % sys.argv[0])

    filename = os.path.basename(filepath)
    error = upload_file_s3(filepath, bucket_name)

    if error is not None:
        print(filename + "" failed uploading to "" +
              bucket_name + "" with error "" + error)
    else:
        print(filename + "" uploaded to "" + bucket_name)",update,"importos,sysimportbotofromboto.s3.keyimportKeybucket_name=sys.argv[1]file_name=sys.argv[2]conn=boto.connect_s3(os.environ['AWS_ACCESS_KEY_ID'],os.environ['AWS_SECRET_ACCESS_KEY'])bucket=conn.get_bucket(bucket_name)k=Key(bucket)k.key=file_name.split('/')[-1]k.set_contents_from_filename(file_name)k.set_acl('public-read')printfile_name+""uploadedto""+bucket_name","importos.pathimportsysimportboto3importbotocoredefupload_file_s3(filename,bucket,obj_name=None):""""""UploadafiletoanS3bucket:paramfilename:Filetoupload:parambucket:Buckettouploadto:paramobject_name:S3objectname.Ifnotspecified,filenameisused:returnNoneifuploadwassuccessful,otherwisetheassociatederrorcode""""""ifobj_nameisNone:obj_name=filenames3_client=boto3.client('s3')try:s3_client.upload_file(filename,bucket,obj_name)exceptbotocore.exceptions.ClientErrorase:error_code=e.response['Error']['Code']returnerror_codereturnNoneif__name__=='__main__':try:_,bucket_name,filepath=sys.argvexceptValueError:print(""Usage:\npython%sbucket_namefilepath""%sys.argv[0])filename=os.path.basename(filepath)error=upload_file_s3(filepath,bucket_name)iferrorisnotNone:print(filename+""faileduploadingto""+bucket_name+""witherror""+error)else:print(filename+""uploadedto""+bucket_name)",scripts/misc/upload.py
Scout24/aws-monocyte,fcdf87503957aeb6976ada1682f4194f652fe280,boto3,boto,"def send_email(self):
        conn = ses.connect_to_region(region_name=self.region)

        self.logger.info(""Sending Email to %s"", "", "".join(self.recipients))

        conn.send_email(
            source=self.sender,
            subject=self.subject,
            body=self.body,
            to_addresses=self.recipients)







","def send_email(self):
        conn = boto3.client('ses', region_name=self.region)

        self.logger.info(""Sending Email to %s"", "", "".join(self.recipients))

        conn.send_email(
            Source=self.sender,
            Destination={'ToAddresses': [self.sender]},
            Message={
                'Subject': {
                    'Data': self.subject
                },
                'Body': {
                    'Text': {
                        'Data': self.body
                    }
                }
            })",update,"defsend_email(self):conn=ses.connect_to_region(region_name=self.region)self.logger.info(""SendingEmailto%s"","","".join(self.recipients))conn.send_email(source=self.sender,subject=self.subject,body=self.body,to_addresses=self.recipients)","defsend_email(self):conn=boto3.client('ses',region_name=self.region)self.logger.info(""SendingEmailto%s"","","".join(self.recipients))conn.send_email(Source=self.sender,Destination={'ToAddresses':[self.sender]},Message={'Subject':{'Data':self.subject},'Body':{'Text':{'Data':self.body}}})",src/main/python/monocyte/plugins/ses_plugin.py
Scout24/aws-monocyte,fcdf87503957aeb6976ada1682f4194f652fe280,boto3,boto,"mock_ses
    def test_send_mail_ok(self):
        conn = boto.connect_ses('the_key', 'the_secret')
        conn.verify_email_identity(self.sender)

        self.aws_ses_plugin.send_email()

        send_quota = conn.get_send_quota()
        sent_count = int(
            send_quota['GetSendQuotaResponse']['GetSendQuotaResult'][
                'SentLast24Hours'])
        self.assertEqual(sent_count, 1)","@mock_ses
    def test_send_mail_ok(self):
        conn = boto3.client('ses')
        conn.verify_email_identity(EmailAddress=self.sender)

        self.aws_ses_plugin.send_email()

        send_quota = conn.get_send_quota()
        sent_count = int(send_quota['SentLast24Hours'])


        self.assertEqual(sent_count, 1)",update,"mock_sesdeftest_send_mail_ok(self):conn=boto.connect_ses('the_key','the_secret')conn.verify_email_identity(self.sender)self.aws_ses_plugin.send_email()send_quota=conn.get_send_quota()sent_count=int(send_quota['GetSendQuotaResponse']['GetSendQuotaResult']['SentLast24Hours'])self.assertEqual(sent_count,1)","@mock_sesdeftest_send_mail_ok(self):conn=boto3.client('ses')conn.verify_email_identity(EmailAddress=self.sender)self.aws_ses_plugin.send_email()send_quota=conn.get_send_quota()sent_count=int(send_quota['SentLast24Hours'])self.assertEqual(sent_count,1)",src/unittest/python/plugins/ses_plugin_tests.py
Scout24/aws-monocyte,fcdf87503957aeb6976ada1682f4194f652fe280,boto3,boto,"self.assertRaises(boto.exception.BotoServerError,
                          aws_ses_plugin.send_email)","self.assertRaises(ClientError,
                          aws_ses_plugin.send_email)",update,"self.assertRaises(boto.exception.BotoServerError,aws_ses_plugin.send_email)","self.assertRaises(ClientError,aws_ses_plugin.send_email)",src/unittest/python/plugins/ses_plugin_tests.py
NixOS/nixops,9e925b6f92498f51aaa3f2bc2c4b500e26e456fd,boto3,boto,"self._conn = boto.s3.connection.S3Connection(aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key)"," self._conn = boto3.session.Session(region_name=self.region,
                                           aws_access_key_id=access_key_id,
                                           aws_secret_access_key=secret_access_key)",update,"self._conn=boto.s3.connection.S3Connection(aws_access_key_id=access_key_id,aws_secret_access_key=secret_access_key)","self._conn=boto3.session.Session(region_name=self.region,aws_access_key_id=access_key_id,aws_secret_access_key=secret_access_key)",nixops/resources/s3_bucket.py
NixOS/nixops,9e925b6f92498f51aaa3f2bc2c4b500e26e456fd,boto3,boto,"try:
                self._conn.create_bucket(defn.bucket_name, location=region_to_s3_location(defn.region))
            except boto.exception.S3CreateError as e:
                if e.error_code != ""BucketAlreadyOwnedByYou"": raise

            bucket = self._conn.get_bucket(defn.bucket_name)





            if defn.policy:
                self.log(""setting S3 bucket policy on ���{0}���..."".format(bucket))
                bucket.set_policy(defn.policy.strip())

            else:
                try:
                    bucket.delete_policy()
                except boto.exception.S3ResponseError as e:
                    # This seems not to happen - despite docs indicating it should:
                    # [http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEpolicy.html]
                    if e.status != 204: raise # (204 : Bucket didn't have any policy to delete)","try:
                s3client = self._conn.client('s3')
                ACL = 'private' # ..or: public-read, public-read-write, authenticated-read
                s3client.create_bucket(ACL = ACL,
                                       Bucket = defn.bucket_name,
                                       CreateBucketConfiguration = {
                                           'LocationConstraint': region_to_s3_location(defn.region)
                                       })
                # self._conn.create_bucket(defn.bucket_name, location=region_to_s3_location(defn.region))
            except botocore.exceptions.ClientError as e:
                if e.response['Error']['Code'] != ""BucketAlreadyOwnedByYou"": raise
            if defn.policy:
                self.log(""setting S3 bucket policy on ���{0}���..."".format(bucket))
                s3client.put_bucket_policy(Bucket = defn.bucket_name,
                                           Policy = defn.policy.strip())
            else:
                try:
                    s3client.delete_bucket_policy(Bucket = defn.bucket_name)
                except botocore.exceptions.ClientError as e:
                    # This seems not to happen - despite docs indicating it should:
                    # [http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEpolicy.html]
                    if e.response['ResponseMetadata']['HTTPStatusCode'] != 204: raise # (204 : Bucket didn't have any policy to delete)",update,"try:self._conn.create_bucket(defn.bucket_name,location=region_to_s3_location(defn.region))exceptboto.exception.S3CreateErrorase:ife.error_code!=""BucketAlreadyOwnedByYou"":raisebucket=self._conn.get_bucket(defn.bucket_name)ifdefn.policy:self.log(""settingS3bucketpolicyon���{0}���..."".format(bucket))bucket.set_policy(defn.policy.strip())else:try:bucket.delete_policy()exceptboto.exception.S3ResponseErrorase:#Thisseemsnottohappen-despitedocsindicatingitshould:#[http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEpolicy.html]ife.status!=204:raise#(204:Bucketdidn'thaveanypolicytodelete)","try:s3client=self._conn.client('s3')ACL='private'#..or:public-read,public-read-write,authenticated-reads3client.create_bucket(ACL=ACL,Bucket=defn.bucket_name,CreateBucketConfiguration={'LocationConstraint':region_to_s3_location(defn.region)})#self._conn.create_bucket(defn.bucket_name,location=region_to_s3_location(defn.region))exceptbotocore.exceptions.ClientErrorase:ife.response['Error']['Code']!=""BucketAlreadyOwnedByYou"":raiseifdefn.policy:self.log(""settingS3bucketpolicyon���{0}���..."".format(bucket))s3client.put_bucket_policy(Bucket=defn.bucket_name,Policy=defn.policy.strip())else:try:s3client.delete_bucket_policy(Bucket=defn.bucket_name)exceptbotocore.exceptions.ClientErrorase:#Thisseemsnottohappen-despitedocsindicatingitshould:#[http://docs.aws.amazon.com/AmazonS3/latest/API/RESTBucketDELETEpolicy.html]ife.response['ResponseMetadata']['HTTPStatusCode']!=204:raise#(204:Bucketdidn'thaveanypolicytodelete)",nixops/resources/s3_bucket.py
NixOS/nixops,9e925b6f92498f51aaa3f2bc2c4b500e26e456fd,boto3,boto,"bucket = self._conn.get_bucket(self.bucket_name)
                try:
                    bucket.delete()
                except boto.exception.S3ResponseError as e:
                    if e.error_code != ""BucketNotEmpty"": raise
                    if not self.depl.logger.confirm(""are you sure you want to destroy S3 bucket ���{0}���?"".format(self.bucket_name)): return False
                    keys = bucket.list()
                    bucket.delete_keys(keys)
                    bucket.delete()
            except boto.exception.S3ResponseError as e:
                if e.error_code != ""NoSuchBucket"": raise
        return True","bucket = self._conn.resource('s3').Bucket(self.bucket_name)
                try:
                    bucket.delete()
                except botocore.exceptions.ClientError as e:
                    if e.response['Error']['Code'] != ""BucketNotEmpty"": raise
                    if not self.depl.logger.confirm(""are you sure you want to destroy S3 bucket ���{0}���?"".format(self.bucket_name)): return False
                    bucket.objects.all().delete()

                    bucket.delete()
            except botocore.exceptions.ClientError as e:
                if e.response['Error']['Code'] != ""NoSuchBucket"": raise
        return True",update,"bucket=self._conn.get_bucket(self.bucket_name)try:bucket.delete()exceptboto.exception.S3ResponseErrorase:ife.error_code!=""BucketNotEmpty"":raiseifnotself.depl.logger.confirm(""areyousureyouwanttodestroyS3bucket���{0}���?"".format(self.bucket_name)):returnFalsekeys=bucket.list()bucket.delete_keys(keys)bucket.delete()exceptboto.exception.S3ResponseErrorase:ife.error_code!=""NoSuchBucket"":raisereturnTrue","bucket=self._conn.resource('s3').Bucket(self.bucket_name)try:bucket.delete()exceptbotocore.exceptions.ClientErrorase:ife.response['Error']['Code']!=""BucketNotEmpty"":raiseifnotself.depl.logger.confirm(""areyousureyouwanttodestroyS3bucket���{0}���?"".format(self.bucket_name)):returnFalsebucket.objects.all().delete()bucket.delete()exceptbotocore.exceptions.ClientErrorase:ife.response['Error']['Code']!=""NoSuchBucket"":raisereturnTrue",nixops/resources/s3_bucket.py
cfn-sphere/cfn-sphere,6c5433eb8fffa67db161f8014299d7dece78466b,boto3,boto,"class Ec2Api(object):
    def __init__(self, region=""eu-west-1""):
        self.conn = ec2.connect_to_region(region)
        self.logger = get_logger()

    @with_boto_retry()
    def get_taupage_images(self):
        filters = {'name': ['Taupage-AMI-*'],
                   'is-public': ['false'],
                   'state': ['available'],
                   'root-device-type': ['ebs']
                   }
        try:
            response = self.conn.get_all_images(executable_by=[""self""], filters=filters)
        except BotoServerError as e:
            raise CfnSphereBotoError(e)

        if not response:
            raise CfnSphereException(""Could not find any private and available Taupage AMI"")

        self.logger.debug(""Found Taupage-AMI-* images:\n{0}"".format(pprint.pformat([vars(x) for x in response])))
        return response

    def get_latest_taupage_image_id(self):
        images = {image.creationDate: image.id for image in self.get_taupage_images()}

        creation_dates = list(images.keys())
        creation_dates.sort(reverse=True)
        return images[creation_dates[0]]","class Ec2Api(object):
    def __init__(self, region=""eu-west-1""):
        self.client = boto3.client('ec2', region_name=region)


    @with_boto_retry()
    def get_taupage_images(self):
        filters = [{'Name': 'name', 'Values': ['Taupage-AMI-*']},
                   {'Name': 'is-public', 'Values': ['false']},
                   {'Name': 'state', 'Values': ['available']},
                   {'Name': 'root-device-type', 'Values': ['ebs']}
                   ]
        try:
            response = self.client.describe_images(ExecutableUsers=[""self""], Filters=filters)
        except (BotoCoreError, ClientError) as e:
            raise CfnSphereBotoError(e)

        if not response:
            raise CfnSphereException(""Could not find any private and available Taupage AMI"")

        return response['Images']


    def get_latest_taupage_image_id(self):
        images = {image['CreationDate']: image['ImageId'] for image in self.get_taupage_images()}

        creation_dates = list(images.keys())
        creation_dates.sort(reverse=True)
        return images[creation_dates[0]]


if __name__ == ""__main__"":
    ec2 = Ec2Api()
    print(ec2.get_latest_taupage_image_id())",update,"classEc2Api(object):def__init__(self,region=""eu-west-1""):self.conn=ec2.connect_to_region(region)self.logger=get_logger()@with_boto_retry()defget_taupage_images(self):filters={'name':['Taupage-AMI-*'],'is-public':['false'],'state':['available'],'root-device-type':['ebs']}try:response=self.conn.get_all_images(executable_by=[""self""],filters=filters)exceptBotoServerErrorase:raiseCfnSphereBotoError(e)ifnotresponse:raiseCfnSphereException(""CouldnotfindanyprivateandavailableTaupageAMI"")self.logger.debug(""FoundTaupage-AMI-*images:\n{0}"".format(pprint.pformat([vars(x)forxinresponse])))returnresponsedefget_latest_taupage_image_id(self):images={image.creationDate:image.idforimageinself.get_taupage_images()}creation_dates=list(images.keys())creation_dates.sort(reverse=True)returnimages[creation_dates[0]]","classEc2Api(object):def__init__(self,region=""eu-west-1""):self.client=boto3.client('ec2',region_name=region)@with_boto_retry()defget_taupage_images(self):filters=[{'Name':'name','Values':['Taupage-AMI-*']},{'Name':'is-public','Values':['false']},{'Name':'state','Values':['available']},{'Name':'root-device-type','Values':['ebs']}]try:response=self.client.describe_images(ExecutableUsers=[""self""],Filters=filters)except(BotoCoreError,ClientError)ase:raiseCfnSphereBotoError(e)ifnotresponse:raiseCfnSphereException(""CouldnotfindanyprivateandavailableTaupageAMI"")returnresponse['Images']defget_latest_taupage_image_id(self):images={image['CreationDate']:image['ImageId']forimageinself.get_taupage_images()}creation_dates=list(images.keys())creation_dates.sort(reverse=True)returnimages[creation_dates[0]]if__name__==""__main__"":ec2=Ec2Api()print(ec2.get_latest_taupage_image_id())",src/main/python/cfn_sphere/aws/ec2.py
cloudtools/stacker,f7a250072d1d8af6352f49044ec5570ac47378f2,boto3,boto,"@property
    def s3_conn(self):
        """"""The boto s3 connection object used for communication with S3.""""""
        if not hasattr(self, ""_s3_conn""):
            self._s3_conn = boto.connect_s3()

        return self._s3_conn","@property
    def s3_conn(self):
        """"""The boto s3 connection object used for communication with S3.""""""
        if not hasattr(self, ""_s3_conn""):
            session = boto3.Session(profile_name=self.provider.profile, region_name=self.provider.region)
            self._s3_conn = session.client('s3')
        return self._s3_conn",update,"@propertydefs3_conn(self):""""""Thebotos3connectionobjectusedforcommunicationwithS3.""""""ifnothasattr(self,""_s3_conn""):self._s3_conn=boto.connect_s3()returnself._s3_conn","@propertydefs3_conn(self):""""""Thebotos3connectionobjectusedforcommunicationwithS3.""""""ifnothasattr(self,""_s3_conn""):session=boto3.Session(profile_name=self.provider.profile,region_name=self.provider.region)self._s3_conn=session.client('s3')returnself._s3_conn",stacker/actions/base.py
cloudtools/stacker,f7a250072d1d8af6352f49044ec5570ac47378f2,boto3,boto,"@property
    def cfn_bucket(self):
        """"""The cloudformation bucket where templates will be stored.""""""
        if not getattr(self, ""_cfn_bucket"", None):
            try:
                self._cfn_bucket = self.s3_conn.get_bucket(self.bucket_name)
            except boto.exception.S3ResponseError, e:
                if e.error_code == ""NoSuchBucket"":
                    logger.debug(""Creating bucket %s."", self.bucket_name)
                    self._cfn_bucket = self.s3_conn.create_bucket(
                        self.bucket_name)
                elif e.error_code == ""AccessDenied"":
                    logger.exception(""Access denied for bucket %s."",
                                     self.bucket_name)
                    raise
                else:
                    logger.exception(""Error creating bucket %s."",
                                     self.bucket_name)
                    raise
        return self._cfn_bucket","def ensure_cfn_bucket(self):

        """"""The cloudformation bucket where templates will be stored.""""""
        try:
            self.s3_conn.head_bucket(Bucket=self.bucket_name)
        except botocore.exceptions.ClientError as e:
            if e.response['Error']['Message'] == ""Not Found"":
                logger.debug(""Creating bucket %s."", self.bucket_name)
                self.s3_conn.create_bucket(Bucket=self.bucket_name)
            elif e.response['Error']['Message'] == ""Forbidden"":
                logger.exception(""Access denied for bucket %s."",
                                 self.bucket_name)
                raise
            else:
                logger.exception(""Error creating bucket %s. Error %s"",
                                 self.bucket_name, e.response)
                raise",update,"@propertydefcfn_bucket(self):""""""Thecloudformationbucketwheretemplateswillbestored.""""""ifnotgetattr(self,""_cfn_bucket"",None):try:self._cfn_bucket=self.s3_conn.get_bucket(self.bucket_name)exceptboto.exception.S3ResponseError,e:ife.error_code==""NoSuchBucket"":logger.debug(""Creatingbucket%s."",self.bucket_name)self._cfn_bucket=self.s3_conn.create_bucket(self.bucket_name)elife.error_code==""AccessDenied"":logger.exception(""Accessdeniedforbucket%s."",self.bucket_name)raiseelse:logger.exception(""Errorcreatingbucket%s."",self.bucket_name)raisereturnself._cfn_bucket","defensure_cfn_bucket(self):""""""Thecloudformationbucketwheretemplateswillbestored.""""""try:self.s3_conn.head_bucket(Bucket=self.bucket_name)exceptbotocore.exceptions.ClientErrorase:ife.response['Error']['Message']==""NotFound"":logger.debug(""Creatingbucket%s."",self.bucket_name)self.s3_conn.create_bucket(Bucket=self.bucket_name)elife.response['Error']['Message']==""Forbidden"":logger.exception(""Accessdeniedforbucket%s."",self.bucket_name)raiseelse:logger.exception(""Errorcreatingbucket%s.Error%s"",self.bucket_name,e.response)raise",stacker/actions/base.py
cloudtools/stacker,f7a250072d1d8af6352f49044ec5570ac47378f2,boto3,boto,"if self.cfn_bucket.get_key(key_name) and not force:

            logger.debug(""Cloudformation template %s already exists."",
                         template_url)
            return template_url
        key = self.cfn_bucket.new_key(key_name)
        key.set_contents_from_string(blueprint.rendered, encrypt_key=True)
        logger.debug(""Blueprint %s pushed to %s."", blueprint.name,
                     template_url)
        return template_url","
        try:
            template_exists = self.s3_conn.head_object(Bucket=self.bucket_name, Key=key_name) is not None
        except botocore.exceptions.ClientError as e:
            if e.response['Error']['Code'] == '404':
                template_exists = False
            else:
                raise

        if template_exists and not force:
            logger.debug(""Cloudformation template %s already exists."",
                         template_url)
            return template_url
        self.s3_conn.put_object(Bucket=self.bucket_name, Key=key_name,
                                      Body=blueprint.rendered, ServerSideEncryption='AES256')
        logger.debug(""Blueprint %s pushed to %s."", blueprint.name,
                     template_url)
        return template_url",update,"ifself.cfn_bucket.get_key(key_name)andnotforce:logger.debug(""Cloudformationtemplate%salreadyexists."",template_url)returntemplate_urlkey=self.cfn_bucket.new_key(key_name)key.set_contents_from_string(blueprint.rendered,encrypt_key=True)logger.debug(""Blueprint%spushedto%s."",blueprint.name,template_url)returntemplate_url","try:template_exists=self.s3_conn.head_object(Bucket=self.bucket_name,Key=key_name)isnotNoneexceptbotocore.exceptions.ClientErrorase:ife.response['Error']['Code']=='404':template_exists=Falseelse:raiseiftemplate_existsandnotforce:logger.debug(""Cloudformationtemplate%salreadyexists."",template_url)returntemplate_urlself.s3_conn.put_object(Bucket=self.bucket_name,Key=key_name,Body=blueprint.rendered,ServerSideEncryption='AES256')logger.debug(""Blueprint%spushedto%s."",blueprint.name,template_url)returntemplate_url",stacker/actions/base.py
cloudtools/stacker,f7a250072d1d8af6352f49044ec5570ac47378f2,boto3,boto,"return retry_with_backoff(fn, args=args, kwargs=kwargs, attempts=attempts,
                              exc_list=(boto.exception.BotoServerError, ),
                              retry_checker=_throttling_checker)","return retry_with_backoff(fn, args=args, kwargs=kwargs, attempts=attempts,
                              exc_list=(botocore.exceptions.ClientError, ),
                              retry_checker=_throttling_checker)",update,"returnretry_with_backoff(fn,args=args,kwargs=kwargs,attempts=attempts,exc_list=(boto.exception.BotoServerError,),retry_checker=_throttling_checker)","returnretry_with_backoff(fn,args=args,kwargs=kwargs,attempts=attempts,exc_list=(botocore.exceptions.ClientError,),retry_checker=_throttling_checker)",stacker/providers/aws.py
cloudtools/stacker,f7a250072d1d8af6352f49044ec5570ac47378f2,boto3,boto,"@property
    def cloudformation(self):
        if not hasattr(self, ""_cloudformation""):
            self._cloudformation = cloudformation.connect_to_region(
                self.region)
        return self._cloudformation","@property
    def cloudformation(self):
        if not hasattr(self, ""_cloudformation""):
            session = boto3.Session(profile_name=self.profile, region_name=self.region)
            self._cloudformation = session.client('cloudformation')
        return self._cloudformation",update,"@propertydefcloudformation(self):ifnothasattr(self,""_cloudformation""):self._cloudformation=cloudformation.connect_to_region(self.region)returnself._cloudformation","@propertydefcloudformation(self):ifnothasattr(self,""_cloudformation""):session=boto3.Session(profile_name=self.profile,region_name=self.region)self._cloudformation=session.client('cloudformation')returnself._cloudformation",stacker/providers/aws.py
cloudtools/stacker,f7a250072d1d8af6352f49044ec5570ac47378f2,boto3,boto,"def get_stack(self, stack_name, **kwargs):
        try:
            return retry_on_throttling(self.cloudformation.describe_stacks,
                                       args=[stack_name])[0]
        except boto.exception.BotoServerError as e:
            if ""does not exist"" not in e.message:
                raise
            raise exceptions.StackDoesNotExist(stack_name)","ef get_stack(self, stack_name, **kwargs):
        try:
            return retry_on_throttling(self.cloudformation.describe_stacks,
                                       kwargs=dict(StackName=stack_name))['Stacks'][0]
        except botocore.exceptions.ClientError as e:
            if ""does not exist"" not in e.message:
                raise
            raise exceptions.StackDoesNotExist(stack_name)",update,"defget_stack(self,stack_name,**kwargs):try:returnretry_on_throttling(self.cloudformation.describe_stacks,args=[stack_name])[0]exceptboto.exception.BotoServerErrorase:if""doesnotexist""notine.message:raiseraiseexceptions.StackDoesNotExist(stack_name)","efget_stack(self,stack_name,**kwargs):try:returnretry_on_throttling(self.cloudformation.describe_stacks,kwargs=dict(StackName=stack_name))['Stacks'][0]exceptbotocore.exceptions.ClientErrorase:if""doesnotexist""notine.message:raiseraiseexceptions.StackDoesNotExist(stack_name)",stacker/providers/aws.py
conda/conda,cab16216b49e7778548f4aaa4aebb35c9caccb1c,boto3,boto,"        try:
            import boto
        except ImportError:
            stderrlog.info('\nError: boto is required for S3 channels. '
                           'Please install it with `conda install boto`\n'
                           'Make sure to run `source deactivate` if you '
                           'are in a conda environment.\n')
            resp.status_code = 404
            return resp

        conn = boto.connect_s3()

        bucket_name, key_string = url_to_s3_info(request.url)

        # Get the bucket without validation that it exists and that we have
        # permissions to list its contents.
        bucket = conn.get_bucket(bucket_name, validate=False)

        try:
            key = bucket.get_key(key_string)
        except boto.exception.S3ResponseError as exc:
            # This exception will occur if the bucket does not exist or if the
            # user does not have permission to list its contents.
            resp.status_code = 404
            resp.raw = exc
            return resp

        if key and key.exists:
            modified = key.last_modified
            content_type = key.content_type or ""text/plain""
            resp.headers = CaseInsensitiveDict({
                ""Content-Type"": content_type,
                ""Content-Length"": key.size,
                ""Last-Modified"": modified,
            })

            _, self._temp_file = mkstemp()
            key.get_contents_to_filename(self._temp_file)
            f = open(self._temp_file, 'rb')
            resp.raw = f
            resp.close = resp.raw.close
        else:
            resp.status_code = 404

        return resp","try:
            import boto3
            from botocore.exceptions import BotoCoreError, ClientError
            bucket_name, key_string = url_to_s3_info(request.url)

            # Get the key without validation that it exists and that we have
            # permissions to list its contents.
            key = boto3.resource('s3').Object(bucket_name, key_string[1:])

            try:
                response = key.get()
            except (BotoCoreError, ClientError) as exc:
                # This exception will occur if the bucket does not exist or if the
                # user does not have permission to list its contents.
                resp.status_code = 404
                message = {
                    ""error"": ""error downloading file from s3"",
                    ""path"": request.url,
                    ""exception"": repr(exc),
                }
                fh = SpooledTemporaryFile()
                fh.write(ensure_binary(json.dumps(message)))
                fh.seek(0)
                resp.raw = fh
                resp.close = resp.raw.close
                return resp

            key_headers = response['ResponseMetadata']['HTTPHeaders']


            resp.headers = CaseInsensitiveDict({
                ""Content-Type"": key_headers.get('content-type', ""text/plain""),
                ""Content-Length"": key_headers['content-length'],
                ""Last-Modified"": key_headers['last-modified'],
            })

            f = SpooledTemporaryFile()
            key.download_fileobj(f)
            f.seek(0)
            resp.raw = f
            resp.close = resp.raw.close



            return resp
        except ImportError:
            stderrlog.info('\nError: boto3 is required for S3 channels. '
                           'Please install with `conda install boto3`\n'
                           'Make sure to run `source deactivate` if you '
                           'are in a conda environment.\n')
            resp.status_code = 404
            return resp",update,"try:importbotoexceptImportError:stderrlog.info('\nError:botoisrequiredforS3channels.''Pleaseinstallitwith`condainstallboto`\n''Makesuretorun`sourcedeactivate`ifyou''areinacondaenvironment.\n')resp.status_code=404returnrespconn=boto.connect_s3()bucket_name,key_string=url_to_s3_info(request.url)#Getthebucketwithoutvalidationthatitexistsandthatwehave#permissionstolistitscontents.bucket=conn.get_bucket(bucket_name,validate=False)try:key=bucket.get_key(key_string)exceptboto.exception.S3ResponseErrorasexc:#Thisexceptionwilloccurifthebucketdoesnotexistorifthe#userdoesnothavepermissiontolistitscontents.resp.status_code=404resp.raw=excreturnrespifkeyandkey.exists:modified=key.last_modifiedcontent_type=key.content_typeor""text/plain""resp.headers=CaseInsensitiveDict({""Content-Type"":content_type,""Content-Length"":key.size,""Last-Modified"":modified,})_,self._temp_file=mkstemp()key.get_contents_to_filename(self._temp_file)f=open(self._temp_file,'rb')resp.raw=fresp.close=resp.raw.closeelse:resp.status_code=404returnresp","try:importboto3frombotocore.exceptionsimportBotoCoreError,ClientErrorbucket_name,key_string=url_to_s3_info(request.url)#Getthekeywithoutvalidationthatitexistsandthatwehave#permissionstolistitscontents.key=boto3.resource('s3').Object(bucket_name,key_string[1:])try:response=key.get()except(BotoCoreError,ClientError)asexc:#Thisexceptionwilloccurifthebucketdoesnotexistorifthe#userdoesnothavepermissiontolistitscontents.resp.status_code=404message={""error"":""errordownloadingfilefroms3"",""path"":request.url,""exception"":repr(exc),}fh=SpooledTemporaryFile()fh.write(ensure_binary(json.dumps(message)))fh.seek(0)resp.raw=fhresp.close=resp.raw.closereturnrespkey_headers=response['ResponseMetadata']['HTTPHeaders']resp.headers=CaseInsensitiveDict({""Content-Type"":key_headers.get('content-type',""text/plain""),""Content-Length"":key_headers['content-length'],""Last-Modified"":key_headers['last-modified'],})f=SpooledTemporaryFile()key.download_fileobj(f)f.seek(0)resp.raw=fresp.close=resp.raw.closereturnrespexceptImportError:stderrlog.info('\nError:boto3isrequiredforS3channels.''Pleaseinstallwith`condainstallboto3`\n''Makesuretorun`sourcedeactivate`ifyou''areinacondaenvironment.\n')resp.status_code=404returnresp",conda/gateways/adapters/s3.py
e-dard/flask-s3,34cc79dcaee001a8232126067d474a240c997e36,boto3,boto,"k = Key(bucket=bucket, name=key_name)
            # Set custom headers
            for header, value in app.config['S3_HEADERS'].iteritems():
                k.set_metadata(header, value)
            k.set_contents_from_filename(file_path)
            k.make_public()","with open(file_path) as fp:
                s3.put_object(Bucket=bucket,
                              Key=key_name,
                              Body=fp.read(),
                              ACL=""public-read"",
                              Metadata=app.config['S3_HEADERS'])",update,"k=Key(bucket=bucket,name=key_name)#Setcustomheadersforheader,valueinapp.config['S3_HEADERS'].iteritems():k.set_metadata(header,value)k.set_contents_from_filename(file_path)k.make_public()","withopen(file_path)asfp:s3.put_object(Bucket=bucket,Key=key_name,Body=fp.read(),ACL=""public-read"",Metadata=app.config['S3_HEADERS'])",flask_s3.py
e-dard/flask-s3,34cc79dcaee001a8232126067d474a240c997e36,boto3,boto,"# connect to s3
    if not location:
        conn = S3Connection(user, password)  # (default region)
    else:
        conn = connect_to_region(location,
                                 aws_access_key_id=user,
                                 aws_secret_access_key=password)

    # get_or_create bucket
    try:
        try:
            bucket = conn.create_bucket(bucket_name)
        except S3CreateError as e:
            if e.error_code == u'BucketAlreadyOwnedByYou':
                bucket = conn.get_bucket(bucket_name)
            else:
                raise e

        bucket.make_public(recursive=False)
    except S3CreateError as e:
        raise e

    if app.config['S3_ONLY_MODIFIED']:
        try:
            hashes = json.loads(
                Key(bucket=bucket,
                    name="".file-hashes"").get_contents_as_string())
        except S3ResponseError as e:
            logger.warn(""No file hashes found: %s"" % e)
            hashes = None

        new_hashes = _upload_files(app, all_files, bucket, hashes=hashes)

        try:
            k = Key(bucket=bucket, name="".file-hashes"")
            k.set_contents_from_string(json.dumps(dict(new_hashes)))
        except S3ResponseError as e:


            logger.warn(""Unable to upload file hashes: %s"" % e)
    else:
        _upload_files(app, all_files, bucket)","# connect to s3
    s3 = boto3.client(""s3"",
                      region_name=location or None,
                      aws_access_key_id=user,
                      aws_secret_access_key=password)



    # get_or_create bucket
    try:
        s3.head_bucket(Bucket=bucket_name)
    except ClientError as e:
        if int(e.response['Error']['Code']) == 404:
            # Create the bucket
            bucket = s3.create_bucket(Bucket=bucket_name)
        else:
            raise

    s3.put_bucket_acl(Bucket=bucket_name, ACL='public-read')



    if app.config['S3_ONLY_MODIFIED']:
        try:
            hashes_object = s3.get_object(Bucket=bucket_name, Key='.file-hashes')
            hashes = json.loads(str(hashes_object['Body'].read()))
        except ClientError as e:

            logger.warn(""No file hashes found: %s"" % e)
            hashes = None

        new_hashes = _upload_files(s3, app, all_files, bucket_name, hashes=hashes)

        try:
            s3.put_object(Bucket=bucket_name,
                          Key='.file-hashes',
                          Body=json.dumps(dict(new_hashes)),
                          ACL='private')
        except boto3.exceptions.S3UploadFailedError as e:
            logger.warn(""Unable to upload file hashes: %s"" % e)
    else:
        _upload_files(s3, app, all_files, bucket_name)",update,"#connecttos3ifnotlocation:conn=S3Connection(user,password)#(defaultregion)else:conn=connect_to_region(location,aws_access_key_id=user,aws_secret_access_key=password)#get_or_createbuckettry:try:bucket=conn.create_bucket(bucket_name)exceptS3CreateErrorase:ife.error_code==u'BucketAlreadyOwnedByYou':bucket=conn.get_bucket(bucket_name)else:raiseebucket.make_public(recursive=False)exceptS3CreateErrorase:raiseeifapp.config['S3_ONLY_MODIFIED']:try:hashes=json.loads(Key(bucket=bucket,name="".file-hashes"").get_contents_as_string())exceptS3ResponseErrorase:logger.warn(""Nofilehashesfound:%s""%e)hashes=Nonenew_hashes=_upload_files(app,all_files,bucket,hashes=hashes)try:k=Key(bucket=bucket,name="".file-hashes"")k.set_contents_from_string(json.dumps(dict(new_hashes)))exceptS3ResponseErrorase:logger.warn(""Unabletouploadfilehashes:%s""%e)else:_upload_files(app,all_files,bucket)","#connecttos3s3=boto3.client(""s3"",region_name=locationorNone,aws_access_key_id=user,aws_secret_access_key=password)#get_or_createbuckettry:s3.head_bucket(Bucket=bucket_name)exceptClientErrorase:ifint(e.response['Error']['Code'])==404:#Createthebucketbucket=s3.create_bucket(Bucket=bucket_name)else:raises3.put_bucket_acl(Bucket=bucket_name,ACL='public-read')ifapp.config['S3_ONLY_MODIFIED']:try:hashes_object=s3.get_object(Bucket=bucket_name,Key='.file-hashes')hashes=json.loads(str(hashes_object['Body'].read()))exceptClientErrorase:logger.warn(""Nofilehashesfound:%s""%e)hashes=Nonenew_hashes=_upload_files(s3,app,all_files,bucket_name,hashes=hashes)try:s3.put_object(Bucket=bucket_name,Key='.file-hashes',Body=json.dumps(dict(new_hashes)),ACL='private')exceptboto3.exceptions.S3UploadFailedErrorase:logger.warn(""Unabletouploadfilehashes:%s""%e)else:_upload_files(s3,app,all_files,bucket_name)",flask_s3.py
openedx/edx-ora2,9726e2aaef7314f45a2fd3cc70b18208c1ae21e6,boto3,boto,"""""""Connect to s3

    Creates a connection to s3 for file URLs.

    """"""
    # Try to get the AWS credentials from settings if they are available
    # If not, these will default to `None`, and boto will try to use
    # environment vars or configuration files instead.
    aws_access_key_id = getattr(settings, 'AWS_ACCESS_KEY_ID', None)
    aws_secret_access_key = getattr(settings, 'AWS_SECRET_ACCESS_KEY', None)


    return boto.connect_s3(

        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key

    )","""""""Connect to s3

    Creates a connection to s3 for file URLs.

    """"""
    # Try to get the AWS credentials from settings if they are available
    # If not, these will default to `None`, and boto3 will try to use
    # environment vars or configuration files instead.
    aws_access_key_id = getattr(settings, ""AWS_ACCESS_KEY_ID"", None)
    aws_secret_access_key = getattr(settings, ""AWS_SECRET_ACCESS_KEY"", None)
    endpoint_url = getattr(settings, ""AWS_S3_ENDPOINT_URL"", None)

    return boto3.client(
        ""s3"",
        aws_access_key_id=aws_access_key_id,
        aws_secret_access_key=aws_secret_access_key,
        endpoint_url=endpoint_url,
    )",update,"""""""Connecttos3Createsaconnectiontos3forfileURLs.""""""#TrytogettheAWScredentialsfromsettingsiftheyareavailable#Ifnot,thesewilldefaultto`None`,andbotowilltrytouse#environmentvarsorconfigurationfilesinstead.aws_access_key_id=getattr(settings,'AWS_ACCESS_KEY_ID',None)aws_secret_access_key=getattr(settings,'AWS_SECRET_ACCESS_KEY',None)returnboto.connect_s3(aws_access_key_id=aws_access_key_id,aws_secret_access_key=aws_secret_access_key)","""""""Connecttos3Createsaconnectiontos3forfileURLs.""""""#TrytogettheAWScredentialsfromsettingsiftheyareavailable#Ifnot,thesewilldefaultto`None`,andboto3willtrytouse#environmentvarsorconfigurationfilesinstead.aws_access_key_id=getattr(settings,""AWS_ACCESS_KEY_ID"",None)aws_secret_access_key=getattr(settings,""AWS_SECRET_ACCESS_KEY"",None)endpoint_url=getattr(settings,""AWS_S3_ENDPOINT_URL"",None)returnboto3.client(""s3"",aws_access_key_id=aws_access_key_id,aws_secret_access_key=aws_secret_access_key,endpoint_url=endpoint_url,)",openassessment/fileupload/backends/s3.py
openedx/edx-ora2,9726e2aaef7314f45a2fd3cc70b18208c1ae21e6,boto3,boto,,"def object_exists(conn, bucket_name, key_name):
    """"""
    Check if a key exists in the given S3 bucket.
    """"""
    try:
        conn.head_object(Bucket=bucket_name, Key=key_name)
    except botocore.exceptions.ClientError as e:
        if e.response[""Error""][""Code""] == ""404"":
            return False
        raise e
    return True",insert,,"defobject_exists(conn,bucket_name,key_name):""""""CheckifakeyexistsinthegivenS3bucket.""""""try:conn.head_object(Bucket=bucket_name,Key=key_name)exceptbotocore.exceptions.ClientErrorase:ife.response[""Error""][""Code""]==""404"":returnFalseraiseereturnTrue",openassessment/fileupload/backends/s3.py
openedx/edx-ora2,9726e2aaef7314f45a2fd3cc70b18208c1ae21e6,boto3,boto,"    def remove_file(self, key):
        bucket_name, key_name = self._retrieve_parameters(key)
        conn = _connect_to_s3()
        bucket = conn.get_bucket(bucket_name)
        s3_key = bucket.get_key(key_name)
        if s3_key:
            bucket.delete_key(s3_key)
            return True
        return False","    def remove_file(self, key):
        bucket_name, key_name = self._retrieve_parameters(key)
        conn = _connect_to_s3()
        if object_exists(conn, bucket_name, key_name):
            conn.delete_object(Bucket=bucket_name, Key=key_name)


            return True
        return False",update,"defremove_file(self,key):bucket_name,key_name=self._retrieve_parameters(key)conn=_connect_to_s3()bucket=conn.get_bucket(bucket_name)s3_key=bucket.get_key(key_name)ifs3_key:bucket.delete_key(s3_key)returnTruereturnFalse","defremove_file(self,key):bucket_name,key_name=self._retrieve_parameters(key)conn=_connect_to_s3()ifobject_exists(conn,bucket_name,key_name):conn.delete_object(Bucket=bucket_name,Key=key_name)returnTruereturnFalse",openassessment/fileupload/backends/s3.py
openedx/edx-ora2,9726e2aaef7314f45a2fd3cc70b18208c1ae21e6,boto3,boto,"def get_download_url(self, key):
        bucket_name, key_name = self._retrieve_parameters(key)
        try:
            conn = _connect_to_s3()
            bucket = conn.get_bucket(bucket_name)
            s3_key = bucket.get_key(key_name)
            return s3_key.generate_url(expires_in=self.DOWNLOAD_URL_TIMEOUT) if s3_key else """"","    def get_download_url(self, key):
        bucket_name, key_name = self._retrieve_parameters(key)
        try:
            conn = _connect_to_s3()
            if not object_exists(conn, bucket_name, key_name):
                return """"
            return conn.generate_presigned_url(
                ""get_object"",
                Params={""Bucket"": bucket_name, ""Key"": key_name},
                ExpiresIn=self.DOWNLOAD_URL_TIMEOUT,
            )",update,"defget_download_url(self,key):bucket_name,key_name=self._retrieve_parameters(key)try:conn=_connect_to_s3()bucket=conn.get_bucket(bucket_name)s3_key=bucket.get_key(key_name)returns3_key.generate_url(expires_in=self.DOWNLOAD_URL_TIMEOUT)ifs3_keyelse""""","defget_download_url(self,key):bucket_name,key_name=self._retrieve_parameters(key)try:conn=_connect_to_s3()ifnotobject_exists(conn,bucket_name,key_name):return""""returnconn.generate_presigned_url(""get_object"",Params={""Bucket"":bucket_name,""Key"":key_name},ExpiresIn=self.DOWNLOAD_URL_TIMEOUT,)",openassessment/fileupload/backends/s3.py
openedx/edx-ora2,9726e2aaef7314f45a2fd3cc70b18208c1ae21e6,boto3,boto,"def get_upload_url(self, key, content_type):
        bucket_name, key_name = self._retrieve_parameters(key)
        try:
            conn = _connect_to_s3()
            upload_url = conn.generate_url(
                expires_in=self.UPLOAD_URL_TIMEOUT,
                method='PUT',
                bucket=bucket_name,
                key=key_name,
                headers={'Content-Length': '5242880', 'Content-Type': content_type}


            )
            return upload_url
        except Exception as ex:
            logger.exception(
                u""An internal exception occurred while generating an upload URL.""
            )
            raise FileUploadInternalError(ex)","class Backend(BaseBackend):
    """""" S3 Bucked File Upload Backend. """"""

    def get_upload_url(self, key, content_type):
        bucket_name, key_name = self._retrieve_parameters(key)
        try:
            conn = _connect_to_s3()
            return conn.generate_presigned_url(
                ""put_object"",
                Params={
                    ""Bucket"": bucket_name,
                    ""Key"": key_name,
                    ""ContentType"": content_type,
                },
                ExpiresIn=self.UPLOAD_URL_TIMEOUT,
            )

        except Exception as ex:
            log.exception(
                u""An internal exception occurred while generating an upload URL.""
            )
            raise FileUploadInternalError(ex)",update,"defget_upload_url(self,key,content_type):bucket_name,key_name=self._retrieve_parameters(key)try:conn=_connect_to_s3()upload_url=conn.generate_url(expires_in=self.UPLOAD_URL_TIMEOUT,method='PUT',bucket=bucket_name,key=key_name,headers={'Content-Length':'5242880','Content-Type':content_type})returnupload_urlexceptExceptionasex:logger.exception(u""AninternalexceptionoccurredwhilegeneratinganuploadURL."")raiseFileUploadInternalError(ex)","classBackend(BaseBackend):""""""S3BuckedFileUploadBackend.""""""defget_upload_url(self,key,content_type):bucket_name,key_name=self._retrieve_parameters(key)try:conn=_connect_to_s3()returnconn.generate_presigned_url(""put_object"",Params={""Bucket"":bucket_name,""Key"":key_name,""ContentType"":content_type,},ExpiresIn=self.UPLOAD_URL_TIMEOUT,)exceptExceptionasex:log.exception(u""AninternalexceptionoccurredwhilegeneratinganuploadURL."")raiseFileUploadInternalError(ex)",openassessment/fileupload/backends/s3.py
webrecorder/pywb,54b265aaa811e008a62aae37be6ce36cec9b0a42,boto3,boto,"class S3Loader(BaseLoader):
    def __init__(self, **kwargs):
        self.s3conn = None
        self.aws_access_key_id = kwargs.get('aws_access_key_id')
        self.aws_secret_access_key = kwargs.get('aws_secret_access_key')

    def load(self, url, offset, length):
        if not s3_avail:  #pragma: no cover
           raise IOError('To load from s3 paths, ' +
                          'you must install boto: pip install boto')

        aws_access_key_id = self.aws_access_key_id
        aws_secret_access_key = self.aws_secret_access_key

        parts = urlsplit(url)

        if parts.username and parts.password:
            aws_access_key_id = unquote_plus(parts.username)
            aws_secret_access_key = unquote_plus(parts.password)
            bucket_name = parts.netloc.split('@', 1)[-1]
        else:
            bucket_name = parts.netloc

        if not self.s3conn:
            try:
                self.s3conn = connect_s3(aws_access_key_id, aws_secret_access_key)
            except Exception:  #pragma: no cover
                self.s3conn = connect_s3(anon=True)

        bucket = self.s3conn.get_bucket(bucket_name)

        key = bucket.get_key(parts.path)

        if offset == 0 and length == -1:
            headers = {}
        else:
            headers = {'Range': BlockLoader._make_range_header(offset, length)}

        # Read range
        key.open_read(headers=headers)
        return key","class S3Loader(BaseLoader):
    def __init__(self, **kwargs):
        self.client = None
        self.aws_access_key_id = kwargs.get('aws_access_key_id')
        self.aws_secret_access_key = kwargs.get('aws_secret_access_key')

    def load(self, url, offset, length):
        if not s3_avail:  #pragma: no cover
           raise IOError('To load from s3 paths, ' +
                          'you must install boto3: pip install boto3')

        aws_access_key_id = self.aws_access_key_id
        aws_secret_access_key = self.aws_secret_access_key

        parts = urlsplit(url)

        if parts.username and parts.password:
            aws_access_key_id = unquote_plus(parts.username)
            aws_secret_access_key = unquote_plus(parts.password)
            bucket_name = parts.netloc.split('@', 1)[-1]
        else:
            bucket_name = parts.netloc

        key = parts.path[1:]









        if offset == 0 and length == -1:
            range_ = ''
        else:
            range_ = BlockLoader._make_range_header(offset, length)

        def s3_load(anon=False):
            if not self.client:
                if anon:
                    config = Config(signature_version=UNSIGNED)
                else:
                    config = None

                client = boto3.client('s3', aws_access_key_id=aws_access_key_id,
                                            aws_secret_access_key=aws_secret_access_key,
                                            config=config)
            else:
                client = self.client

            res = client.get_object(Bucket=bucket_name,
                                    Key=key,
                                    Range=range_)

            if not self.client:
                self.client = client

            return res

        try:
            obj = s3_load(anon=False)

        except Exception:
            if not self.client:
                obj = s3_load(anon=True)
            else:
                raise

        return obj['Body']",update,"classS3Loader(BaseLoader):def__init__(self,**kwargs):self.s3conn=Noneself.aws_access_key_id=kwargs.get('aws_access_key_id')self.aws_secret_access_key=kwargs.get('aws_secret_access_key')defload(self,url,offset,length):ifnots3_avail:#pragma:nocoverraiseIOError('Toloadfroms3paths,'+'youmustinstallboto:pipinstallboto')aws_access_key_id=self.aws_access_key_idaws_secret_access_key=self.aws_secret_access_keyparts=urlsplit(url)ifparts.usernameandparts.password:aws_access_key_id=unquote_plus(parts.username)aws_secret_access_key=unquote_plus(parts.password)bucket_name=parts.netloc.split('@',1)[-1]else:bucket_name=parts.netlocifnotself.s3conn:try:self.s3conn=connect_s3(aws_access_key_id,aws_secret_access_key)exceptException:#pragma:nocoverself.s3conn=connect_s3(anon=True)bucket=self.s3conn.get_bucket(bucket_name)key=bucket.get_key(parts.path)ifoffset==0andlength==-1:headers={}else:headers={'Range':BlockLoader._make_range_header(offset,length)}#Readrangekey.open_read(headers=headers)returnkey","classS3Loader(BaseLoader):def__init__(self,**kwargs):self.client=Noneself.aws_access_key_id=kwargs.get('aws_access_key_id')self.aws_secret_access_key=kwargs.get('aws_secret_access_key')defload(self,url,offset,length):ifnots3_avail:#pragma:nocoverraiseIOError('Toloadfroms3paths,'+'youmustinstallboto3:pipinstallboto3')aws_access_key_id=self.aws_access_key_idaws_secret_access_key=self.aws_secret_access_keyparts=urlsplit(url)ifparts.usernameandparts.password:aws_access_key_id=unquote_plus(parts.username)aws_secret_access_key=unquote_plus(parts.password)bucket_name=parts.netloc.split('@',1)[-1]else:bucket_name=parts.netlockey=parts.path[1:]ifoffset==0andlength==-1:range_=''else:range_=BlockLoader._make_range_header(offset,length)defs3_load(anon=False):ifnotself.client:ifanon:config=Config(signature_version=UNSIGNED)else:config=Noneclient=boto3.client('s3',aws_access_key_id=aws_access_key_id,aws_secret_access_key=aws_secret_access_key,config=config)else:client=self.clientres=client.get_object(Bucket=bucket_name,Key=key,Range=range_)ifnotself.client:self.client=clientreturnrestry:obj=s3_load(anon=False)exceptException:ifnotself.client:obj=s3_load(anon=True)else:raisereturnobj['Body']",pywb/utils/loaders.py
marrow/mailer,c0a5eb6235f12e06d56c0fa285748549c20eaee0,boto3,boto,"def startup(self):
        self.connection = boto.ses.connect_to_region(self.region, **self.config)","    def startup(self):
        self.connection = boto3.client('ses', region_name=self.region, **self.config)",update,"defstartup(self):self.connection=boto.ses.connect_to_region(self.region,**self.config)","defstartup(self):self.connection=boto3.client('ses',region_name=self.region,**self.config)",marrow/mailer/transport/ses.py
marrow/mailer,c0a5eb6235f12e06d56c0fa285748549c20eaee0,boto3,boto,"   def deliver(self, message):
        try:
            destinations = [r.encode(encoding='utf-8') for r in message.recipients]
            response = self.connection.send_raw_email(str(message), message.author.encode(), destinations)
            




            return (
                    response['SendRawEmailResponse']['SendRawEmailResult']['MessageId'],
                    response['SendRawEmailResponse']['ResponseMetadata']['RequestId']
                )
        
        except SESConnection.ResponseError:
            raise","def deliver(self, message):
        try:
            destinations = [str(r) for r in message.recipients]
            response = self.connection.send_raw_email(
                RawMessage = {'Data': str(message)},
                Source = str(message.author),
                Destinations = destinations,
            )

            return (
                response.get('MessageId', 'messageId NOT FOUND'),
                response.get('RequestId', {}).get('ResponseMetadata')
            )

        except ClientError as e:
            raise",update,"defdeliver(self,message):try:destinations=[r.encode(encoding='utf-8')forrinmessage.recipients]response=self.connection.send_raw_email(str(message),message.author.encode(),destinations)return(response['SendRawEmailResponse']['SendRawEmailResult']['MessageId'],response['SendRawEmailResponse']['ResponseMetadata']['RequestId'])exceptSESConnection.ResponseError:raise","defdeliver(self,message):try:destinations=[str(r)forrinmessage.recipients]response=self.connection.send_raw_email(RawMessage={'Data':str(message)},Source=str(message.author),Destinations=destinations,)return(response.get('MessageId','messageIdNOTFOUND'),response.get('RequestId',{}).get('ResponseMetadata'))exceptClientErrorase:raise",marrow/mailer/transport/ses.py
gorilla-co/s3pypi,018015527eb8084aa26165a3345d96e5eb3d054e,boto3,boto,"def __init__(self, bucket, secret=None, region=None):
        s3 = boto.s3.connect_to_region(region) if region else boto.connect_s3()
        self.bucket = s3.get_bucket(bucket)
        self.secret = secret

        self.url = 'http://' + self.bucket.get_website_endpoint()
        if secret:
            self.url += '/' + secret","def __init__(self, bucket, secret=None, region=None):
        self.s3 = boto3.resource('s3', region_name=region)
        self.bucket = bucket
        self.secret = secret",update,"def__init__(self,bucket,secret=None,region=None):s3=boto.s3.connect_to_region(region)ifregionelseboto.connect_s3()self.bucket=s3.get_bucket(bucket)self.secret=secretself.url='http://'+self.bucket.get_website_endpoint()ifsecret:self.url+='/'+secret","def__init__(self,bucket,secret=None,region=None):self.s3=boto3.resource('s3',region_name=region)self.bucket=bucketself.secret=secret",s3pypi/storage.py
gorilla-co/s3pypi,018015527eb8084aa26165a3345d96e5eb3d054e,boto3,boto,"    def _key(self, package, filename):
        path = '%s/%s' % (package.name, filename)
        return Key(self.bucket, '%s/%s' % (self.secret, path) if self.secret else path)"," def _object(self, package, filename):




        path = '%s/%s' % (package.name, filename)
        return self.s3.Object(self.bucket, '%s/%s' % (self.secret, path) if self.secret else path)",update,"def_key(self,package,filename):path='%s/%s'%(package.name,filename)returnKey(self.bucket,'%s/%s'%(self.secret,path)ifself.secretelsepath)","def_object(self,package,filename):path='%s/%s'%(package.name,filename)returnself.s3.Object(self.bucket,'%s/%s'%(self.secret,path)ifself.secretelsepath)",s3pypi/storage.py
gorilla-co/s3pypi,018015527eb8084aa26165a3345d96e5eb3d054e,boto3,boto,"def get_index(self, package):
        try:
            html = self._key(package, 'index.html').get_contents_as_string()
            return Index.parse(self.url, html)
        except S3ResponseError:
            return Index(self.url, [])

    def put_index(self, package, index):
        k = self._key(package, 'index.html')
        k.set_metadata('Content-Type', 'text/html')
        k.set_metadata('Cache-Control', 'public, must-revalidate, proxy-revalidate, max-age=0')
        k.set_contents_from_string(index.to_html())
        k.set_acl('public-read')


    def put_package(self, package):
        for filename in package.files:
            k = self._key(package, filename)
            k.set_metadata('Content-Type', 'application/x-gzip')
            k.set_contents_from_filename(os.path.join('dist', filename))
            k.set_acl('public-read')","def get_index(self, package):
        try:
            html = self._object(package, 'index.html').get()['Body'].read()
            return Index.parse(html)
        except ClientError:
            return Index([])

    def put_index(self, package, index):
        self._object(package, 'index.html').put(
            Body=index.to_html(),
            ContentType='text/html',
            CacheControl='public, must-revalidate, proxy-revalidate, max-age=0',
            ACL='public-read'
        )

    def put_package(self, package):
        for filename in package.files:
            with open(os.path.join('dist', filename)) as f:
                self._object(package, filename).put(
                    Body=f,
                    ContentType='application/x-gzip',
                    ACL='public-read'
                )",update,"defget_index(self,package):try:html=self._key(package,'index.html').get_contents_as_string()returnIndex.parse(self.url,html)exceptS3ResponseError:returnIndex(self.url,[])defput_index(self,package,index):k=self._key(package,'index.html')k.set_metadata('Content-Type','text/html')k.set_metadata('Cache-Control','public,must-revalidate,proxy-revalidate,max-age=0')k.set_contents_from_string(index.to_html())k.set_acl('public-read')defput_package(self,package):forfilenameinpackage.files:k=self._key(package,filename)k.set_metadata('Content-Type','application/x-gzip')k.set_contents_from_filename(os.path.join('dist',filename))k.set_acl('public-read')","defget_index(self,package):try:html=self._object(package,'index.html').get()['Body'].read()returnIndex.parse(html)exceptClientError:returnIndex([])defput_index(self,package,index):self._object(package,'index.html').put(Body=index.to_html(),ContentType='text/html',CacheControl='public,must-revalidate,proxy-revalidate,max-age=0',ACL='public-read')defput_package(self,package):forfilenameinpackage.files:withopen(os.path.join('dist',filename))asf:self._object(package,filename).put(Body=f,ContentType='application/x-gzip',ACL='public-read')",s3pypi/storage.py
rhelmer/caching-s3-proxy,a0427d623677c7082fbbba0682b258fff0eb5b3d,boto3,boto,,self.s3 = boto3.resource('s3'),insert,,self.s3=boto3.resource('s3'),proxy/__init__.py
rhelmer/caching-s3-proxy,a0427d623677c7082fbbba0682b258fff0eb5b3d,boto3,boto,"        s3_result = self.fetch_s3_object(bucket, key)
        if s3_result:
            status = '200 OK'
            response_headers = [('Content-type', 'text/plain')]
            start_response(status, response_headers)
            return [s3_result]
        else:
            status = '404 NOT FOUND'
            response_headers = [('Content-type', 'text/plain')]
            start_response(status, response_headers)
            return []","try:
            s3_result = self.fetch_s3_object(bucket, key)
            status = '200 OK'
            response_headers = [('Content-type', 'text/plain')]
        except botocore.exceptions.ClientError as ce:
            s3_result = ce.response['Error']['Message']

            status = '404 NOT FOUND'
            response_headers = [('Content-type', 'text/plain')]

        start_response(status, response_headers)
        return [s3_result]",update,"s3_result=self.fetch_s3_object(bucket,key)ifs3_result:status='200OK'response_headers=[('Content-type','text/plain')]start_response(status,response_headers)return[s3_result]else:status='404NOTFOUND'response_headers=[('Content-type','text/plain')]start_response(status,response_headers)return[]","try:s3_result=self.fetch_s3_object(bucket,key)status='200OK'response_headers=[('Content-type','text/plain')]exceptbotocore.exceptions.ClientErrorasce:s3_result=ce.response['Error']['Message']status='404NOTFOUND'response_headers=[('Content-type','text/plain')]start_response(status,response_headers)return[s3_result]",proxy/__init__.py
seatgeek/haldane,473badf04be3e5f917a9f774c6cae7d9c31f4485,boto3,boto,"@blueprint_http.errorhandler(boto.exception.EC2ResponseError)
def handle_ec2_response_error(error):
    logger.info('{0}: {1}'.format(error.error_code, error.error_message))
    response = jsonify({
        'type': error_link,
        'title': '{0} Error in EC2 Respone'.format(error.error_code),
        'detail': error.error_message,
        'status': 500,
    })
    response.status_code = 500","@blueprint_http.errorhandler(boto3.exceptions.Boto3Error)
def handle_boto3_response_error(error):
    logger.info('{0}'.format(error))
    response = jsonify({
        'type': error_link,
        'title': '{0} Error in EC2 Response'.format(error),
        'status': 500,
    })
    response.status_code = 500
    return response


@blueprint_http.errorhandler(botocore.exceptions.BotoCoreError)
def handle_botocore_response_error(error):
    logger.info('{0}'.format(vars(error)))
    response = jsonify({
        'type': error_link,
        'title': '{0} Error in EC2 Response'.format(error),
        'status': 500,
    })
    response.status_code = 500",update,"@blueprint_http.errorhandler(boto.exception.EC2ResponseError)defhandle_ec2_response_error(error):logger.info('{0}:{1}'.format(error.error_code,error.error_message))response=jsonify({'type':error_link,'title':'{0}ErrorinEC2Respone'.format(error.error_code),'detail':error.error_message,'status':500,})response.status_code=500","@blueprint_http.errorhandler(boto3.exceptions.Boto3Error)defhandle_boto3_response_error(error):logger.info('{0}'.format(error))response=jsonify({'type':error_link,'title':'{0}ErrorinEC2Response'.format(error),'status':500,})response.status_code=500returnresponse@blueprint_http.errorhandler(botocore.exceptions.BotoCoreError)defhandle_botocore_response_error(error):logger.info('{0}'.format(vars(error)))response=jsonify({'type':error_link,'title':'{0}ErrorinEC2Response'.format(error),'status':500,})response.status_code=500",app/views.py
,,boto3,boto,"def get_amis(regions, query=None):
    amis = []
    for region in regions:
        amis.extend(get_amis_in_region(region))","def get_amis(regions, query=None):
    amis = []
    for region in regions:
        ec2_resource = boto3.resource('ec2', region_name=region)
        amis.extend(get_amis_in_region(ec2_resource, region))",update,"defget_amis(regions,query=None):amis=[]forregioninregions:amis.extend(get_amis_in_region(region))","defget_amis(regions,query=None):amis=[]forregioninregions:ec2_resource=boto3.resource('ec2',region_name=region)amis.extend(get_amis_in_region(ec2_resource,region))",
,,boto3,boto,"def get_amis_in_region(region):
    conn = boto.ec2.connect_to_region(
        region,
        aws_access_key_id=Config.AWS_ACCESS_KEY_ID,
        aws_secret_access_key=Config.AWS_SECRET_ACCESS_KEY,
    )

    if conn is None:
        return {}

    images = conn.get_all_images(owners=['self'])
    amis = []
    for image in images:
        ami = {
            'architecture': image.architecture,
            'billing_products': image.billing_products,
            'creationDate': image.creationDate,
            'description': image.description,
            'hypervisor': image.hypervisor,
            'id': image.id,
            'instance_lifecycle': image.instance_lifecycle,
            'is_public': image.is_public,
            'item': image.item,
            'kernel_id': image.kernel_id,
            'location': image.location,
            'name': image.name,
            'ownerId': image.ownerId,
            'owner_alias': image.owner_alias,
            'owner_id': image.owner_id,
            'platform': image.platform,
            'product_codes': image.product_codes,
            'ramdisk_id': image.ramdisk_id,
            'region': image.region.name,
            'root_device_name': image.root_device_name,
            'root_device_type': image.root_device_type,
            'sriov_net_support': image.sriov_net_support,
            'state': image.state,
            'tags': image.tags,
            'type': image.type,
            'virtualization_type': image.virtualization_type,
        }
















        block_device_mapping = {}
        for mount, block_device in image.block_device_mapping.items():
            block_device_mapping[mount] = vars(block_device)
            del block_device_mapping[mount]['connection']











        ami['block_device_mapping'] = block_device_mapping

        amis.append(ami)
    return amis","def get_amis_in_region(resource, region):










    amis = []
    for image in resource.images.filter(Owners=['self']).all():
        ami = {
            'architecture': image.architecture,
            'creation_date': image.creation_date,

            'description': image.description,
            'hypervisor': image.hypervisor,
            'id': image.image_id,
            'is_public': image.public,


            'kernel_id': image.kernel_id,
            'location': image.image_location,
            'name': image.name,
            'owner_alias': image.image_owner_alias,

            'owner_id': image.owner_id,
            'platform': image.platform,
            'product_codes': image.product_codes,
            'ramdisk_id': image.ramdisk_id,
            'region': region,
            'root_device_name': image.root_device_name,
            'root_device_type': image.root_device_type,
            'sriov_net_support': image.sriov_net_support,
            'state': image.state,
            'type': image.image_type,

            'virtualization_type': image.virtualization_type,
        }

        block_device_fields = [
            'attach_time',
            'delete_on_termination',
            'ebs',
            'encrypted',
            'ephemeral_name',
            'iops',
            'no_device',
            'snapshot_id',
            'status',
            'volume_id',
            'volume_size',
            'volume_type',
        ]

        block_device_mapping = {}
        for block_device in image.block_device_mappings:
            device_name = block_device['DeviceName']
            data = block_device.get('Ebs', {})
            data['ephemeral_name'] = block_device.get('VirtualName')
            data['no_device'] = block_device.get('NoDevice')
            data = dict((underscore(k), v) for k, v in data.iteritems())
            for field in block_device_fields:
                data[field] = data.get(field)
            block_device_mapping[device_name] = data

        tags = image.tags
        if not tags:
            tags = []

        ami['block_device_mapping'] = block_device_mapping
        ami['tags'] = dict((tag['Key'], tag['Value']) for tag in tags)
        amis.append(ami)
    return amis",update,"defget_amis_in_region(region):conn=boto.ec2.connect_to_region(region,aws_access_key_id=Config.AWS_ACCESS_KEY_ID,aws_secret_access_key=Config.AWS_SECRET_ACCESS_KEY,)ifconnisNone:return{}images=conn.get_all_images(owners=['self'])amis=[]forimageinimages:ami={'architecture':image.architecture,'billing_products':image.billing_products,'creationDate':image.creationDate,'description':image.description,'hypervisor':image.hypervisor,'id':image.id,'instance_lifecycle':image.instance_lifecycle,'is_public':image.is_public,'item':image.item,'kernel_id':image.kernel_id,'location':image.location,'name':image.name,'ownerId':image.ownerId,'owner_alias':image.owner_alias,'owner_id':image.owner_id,'platform':image.platform,'product_codes':image.product_codes,'ramdisk_id':image.ramdisk_id,'region':image.region.name,'root_device_name':image.root_device_name,'root_device_type':image.root_device_type,'sriov_net_support':image.sriov_net_support,'state':image.state,'tags':image.tags,'type':image.type,'virtualization_type':image.virtualization_type,}block_device_mapping={}formount,block_deviceinimage.block_device_mapping.items():block_device_mapping[mount]=vars(block_device)delblock_device_mapping[mount]['connection']ami['block_device_mapping']=block_device_mappingamis.append(ami)returnamis","defget_amis_in_region(resource,region):amis=[]forimageinresource.images.filter(Owners=['self']).all():ami={'architecture':image.architecture,'creation_date':image.creation_date,'description':image.description,'hypervisor':image.hypervisor,'id':image.image_id,'is_public':image.public,'kernel_id':image.kernel_id,'location':image.image_location,'name':image.name,'owner_alias':image.image_owner_alias,'owner_id':image.owner_id,'platform':image.platform,'product_codes':image.product_codes,'ramdisk_id':image.ramdisk_id,'region':region,'root_device_name':image.root_device_name,'root_device_type':image.root_device_type,'sriov_net_support':image.sriov_net_support,'state':image.state,'type':image.image_type,'virtualization_type':image.virtualization_type,}block_device_fields=['attach_time','delete_on_termination','ebs','encrypted','ephemeral_name','iops','no_device','snapshot_id','status','volume_id','volume_size','volume_type',]block_device_mapping={}forblock_deviceinimage.block_device_mappings:device_name=block_device['DeviceName']data=block_device.get('Ebs',{})data['ephemeral_name']=block_device.get('VirtualName')data['no_device']=block_device.get('NoDevice')data=dict((underscore(k),v)fork,vindata.iteritems())forfieldinblock_device_fields:data[field]=data.get(field)block_device_mapping[device_name]=datatags=image.tagsifnottags:tags=[]ami['block_device_mapping']=block_device_mappingami['tags']=dict((tag['Key'],tag['Value'])fortagintags)amis.append(ami)returnamis",
,,boto3,boto,"def get_elastic_ips(conn, region):
    return [address.public_ip for address in conn.get_all_addresses()]","def get_elastic_ips(ec2_resource):
    classic_addresses = ec2_resource.classic_addresses.all()
    return [address.public_ip for address in classic_addresses]",update,"defget_elastic_ips(conn,region):return[address.public_ipforaddressinconn.get_all_addresses()]",defget_elastic_ips(ec2_resource):classic_addresses=ec2_resource.classic_addresses.all()return[address.public_ipforaddressinclassic_addresses],
seatgeek/haldane,473badf04be3e5f917a9f774c6cae7d9c31f4485,boto3,boto,"def get_nodes_in_region(region):
    conn = boto.ec2.connect_to_region(
        region,
        aws_access_key_id=Config.AWS_ACCESS_KEY_ID,
        aws_secret_access_key=Config.AWS_SECRET_ACCESS_KEY,
    )

    if conn is None:
        return {}

    elastic_ips = get_elastic_ips(conn, region)
    images = get_amis_in_region(region)

    instances = []
    reservations = conn.get_all_reservations()
    instances_for_region = [i for r in reservations for i in r.instances]
    for instance in instances_for_region:
        instance_data = instance.__dict__
        name = set_retrieve(instance_data, 'tags.Name')
        ip_address = set_retrieve(instance_data, 'ip_address')
        pip_address = set_retrieve(instance_data, 'private_ip_address')
        instance_id = set_retrieve(instance_data, 'id')","def get_nodes_in_region(region):
    ec2_resource = boto3.resource('ec2', region_name=region)
    elastic_ips = get_elastic_ips(ec2_resource)
    images = get_amis_in_region(ec2_resource, region)









    instances = []
    instances_for_region = ec2_resource.instances.all()

    for instance in instances_for_region:
        ip_address = instance.public_ip_address
        pip_address = instance.private_ip_address
        instance_id = instance.instance_id



        if ip_address is None:
            ip_address = ''
        if pip_address is None:
            pip_address = ''
        if instance_id is None:
            instance_id = ''

        tags = instance.tags
        tags = dict((tag['Key'], tag['Value']) for tag in tags)",update,"defget_nodes_in_region(region):conn=boto.ec2.connect_to_region(region,aws_access_key_id=Config.AWS_ACCESS_KEY_ID,aws_secret_access_key=Config.AWS_SECRET_ACCESS_KEY,)ifconnisNone:return{}elastic_ips=get_elastic_ips(conn,region)images=get_amis_in_region(region)instances=[]reservations=conn.get_all_reservations()instances_for_region=[iforrinreservationsforiinr.instances]forinstanceininstances_for_region:instance_data=instance.__dict__name=set_retrieve(instance_data,'tags.Name')ip_address=set_retrieve(instance_data,'ip_address')pip_address=set_retrieve(instance_data,'private_ip_address')instance_id=set_retrieve(instance_data,'id')","defget_nodes_in_region(region):ec2_resource=boto3.resource('ec2',region_name=region)elastic_ips=get_elastic_ips(ec2_resource)images=get_amis_in_region(ec2_resource,region)instances=[]instances_for_region=ec2_resource.instances.all()forinstanceininstances_for_region:ip_address=instance.public_ip_addresspip_address=instance.private_ip_addressinstance_id=instance.instance_idifip_addressisNone:ip_address=''ifpip_addressisNone:pip_address=''ifinstance_idisNone:instance_id=''tags=instance.tagstags=dict((tag['Key'],tag['Value'])fortagintags)",app/views.py
spulec/PyQS,ac07140b339b8c8c943a9f2e0b11ff862b7e3f5e,boto3,boto,"def get_or_create_queue(queue_name):
    global conn
    if conn is None:
        conn = boto.connect_sqs()
    queue = conn.get_queue(queue_name)
    if queue:
        return queue
    else:
        return conn.create_queue(queue_name)","def get_or_create_queue(queue_name):
    sqs = boto3.resource('sqs')
    try:
        return sqs.get_queue_by_name(QueueName=queue_name)
    except ClientError as exc:
        if exc.response['Error']['Code'] == 'AWS.SimpleQueueService.NonExistentQueue':
            return sqs.create_queue(QueueName=queue_name)
        else:
            raise",update,defget_or_create_queue(queue_name):globalconnifconnisNone:conn=boto.connect_sqs()queue=conn.get_queue(queue_name)ifqueue:returnqueueelse:returnconn.create_queue(queue_name),defget_or_create_queue(queue_name):sqs=boto3.resource('sqs')try:returnsqs.get_queue_by_name(QueueName=queue_name)exceptClientErrorasexc:ifexc.response['Error']['Code']=='AWS.SimpleQueueService.NonExistentQueue':returnsqs.create_queue(QueueName=queue_name)else:raise,pyqs/decorator.py
spulec/PyQS,ac07140b339b8c8c943a9f2e0b11ff862b7e3f5e,boto3,boto,"def get_conn(region=None, access_key_id=None, secret_access_key=None):
    return boto.connect_sqs(aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key, region=_get_region(region))","def get_conn(region='us-east-1', access_key_id=None, secret_access_key=None):
    return boto3.client(""sqs"", aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key, region_name=region)",update,"defget_conn(region=None,access_key_id=None,secret_access_key=None):returnboto.connect_sqs(aws_access_key_id=access_key_id,aws_secret_access_key=secret_access_key,region=_get_region(region))","defget_conn(region='us-east-1',access_key_id=None,secret_access_key=None):returnboto3.client(""sqs"",aws_access_key_id=access_key_id,aws_secret_access_key=secret_access_key,region_name=region)",pyqs/worker.py
spulec/PyQS,ac07140b339b8c8c943a9f2e0b11ff862b7e3f5e,boto3,boto,"def _get_region(region_name):
    if region_name is not None:
        for region in boto.sqs.regions():
            if region.name == region_name:
                return region",,delete,def_get_region(region_name):ifregion_nameisnotNone:forregioninboto.sqs.regions():ifregion.name==region_name:returnregion,,tests/test_manager_worker.py
spulec/PyQS,ac07140b339b8c8c943a9f2e0b11ff862b7e3f5e,boto3,boto,"    conn = boto.connect_sqs()
    conn.create_queue(""email"")","conn = boto3.client('sqs', region_name='us-east-1')
    conn.create_queue(QueueName=""email"")",update,"conn=boto.connect_sqs()conn.create_queue(""email"")","conn=boto3.client('sqs',region_name='us-east-1')conn.create_queue(QueueName=""email"")",tests/test_manager_worker.py
spulec/PyQS,ac07140b339b8c8c943a9f2e0b11ff862b7e3f5e,boto3,boto,"def test_manager_worker_with_queue_prefix():
    """"""
    Test managing process can find queues by prefix
    """"""
    conn = boto.connect_sqs()
    conn.create_queue(""email.foobar"")
    conn.create_queue(""email.baz"")

    manager = ManagerWorker(queue_prefixes=['email.*'], worker_concurrency=1, interval=1, batchsize=10)

    len(manager.reader_children).should.equal(2)
    children = manager.reader_children
    # Pull all the read children and sort by name to make testing easier
    sorted_children = sorted(children, key=lambda child: child.sqs_queue.name)

    sorted_children[0].sqs_queue.name.should.equal(""email.baz"")
    sorted_children[1].sqs_queue.name.should.equal(""email.foobar"")","def test_manager_worker_with_queue_prefix():
    """"""
    Test managing process can find queues by prefix
    """"""
    conn = boto3.client('sqs', region_name='us-east-1')
    conn.create_queue(QueueName=""email.foobar"")
    conn.create_queue(QueueName=""email.baz"")

    manager = ManagerWorker(queue_prefixes=['email.*'], worker_concurrency=1, interval=1, batchsize=10)

    len(manager.reader_children).should.equal(2)
    children = manager.reader_children
    # Pull all the read children and sort by name to make testing easier
    sorted_children = sorted(children, key=lambda child: child.queue_url)

    sorted_children[0].queue_url.should.equal(""https://queue.amazonaws.com/123456789012/email.baz"")
    sorted_children[1].queue_url.should.equal(""https://queue.amazonaws.com/123456789012/email.foobar"")",update,"deftest_manager_worker_with_queue_prefix():""""""Testmanagingprocesscanfindqueuesbyprefix""""""conn=boto.connect_sqs()conn.create_queue(""email.foobar"")conn.create_queue(""email.baz"")manager=ManagerWorker(queue_prefixes=['email.*'],worker_concurrency=1,interval=1,batchsize=10)len(manager.reader_children).should.equal(2)children=manager.reader_children#Pullallthereadchildrenandsortbynametomaketestingeasiersorted_children=sorted(children,key=lambdachild:child.sqs_queue.name)sorted_children[0].sqs_queue.name.should.equal(""email.baz"")sorted_children[1].sqs_queue.name.should.equal(""email.foobar"")","deftest_manager_worker_with_queue_prefix():""""""Testmanagingprocesscanfindqueuesbyprefix""""""conn=boto3.client('sqs',region_name='us-east-1')conn.create_queue(QueueName=""email.foobar"")conn.create_queue(QueueName=""email.baz"")manager=ManagerWorker(queue_prefixes=['email.*'],worker_concurrency=1,interval=1,batchsize=10)len(manager.reader_children).should.equal(2)children=manager.reader_children#Pullallthereadchildrenandsortbynametomaketestingeasiersorted_children=sorted(children,key=lambdachild:child.queue_url)sorted_children[0].queue_url.should.equal(""https://queue.amazonaws.com/123456789012/email.baz"")sorted_children[1].queue_url.should.equal(""https://queue.amazonaws.com/123456789012/email.foobar"")",tests/test_manager_worker.py
spulec/PyQS,ac07140b339b8c8c943a9f2e0b11ff862b7e3f5e,boto3,boto,"def test_manager_start_and_stop():
    """"""
    Test managing process can start and stop child processes
    """"""
    conn = boto.connect_sqs()
    conn.create_queue(""email"")

    manager = ManagerWorker(queue_prefixes=['email'], worker_concurrency=2, interval=1, batchsize=10)","def test_manager_start_and_stop():
    """"""
    Test managing process can start and stop child processes
    """"""
    conn = boto3.client('sqs', region_name='us-east-1')
    conn.create_queue(QueueName=""email"")

    manager = ManagerWorker(queue_prefixes=['email'], worker_concurrency=2, interval=1, batchsize=10)",update,"deftest_manager_start_and_stop():""""""Testmanagingprocesscanstartandstopchildprocesses""""""conn=boto.connect_sqs()conn.create_queue(""email"")manager=ManagerWorker(queue_prefixes=['email'],worker_concurrency=2,interval=1,batchsize=10)","deftest_manager_start_and_stop():""""""Testmanagingprocesscanstartandstopchildprocesses""""""conn=boto3.client('sqs',region_name='us-east-1')conn.create_queue(QueueName=""email"")manager=ManagerWorker(queue_prefixes=['email'],worker_concurrency=2,interval=1,batchsize=10)",tests/test_manager_worker.py
spulec/PyQS,ac07140b339b8c8c943a9f2e0b11ff862b7e3f5e,boto3,boto,"def test_master_spawns_worker_processes():
    """"""
    Test managing process creates child workers
    """"""

    # Setup SQS Queue
    conn = boto.connect_sqs()
    conn.create_queue(""tester"")

    # Setup Manager
    manager = ManagerWorker([""tester""], 1, 1, 10)","@mock_sqs_deprecated
def test_master_spawns_worker_processes():
    """"""
    Test managing process creates child workers
    """"""

    # Setup SQS Queue
    conn = boto3.client('sqs', region_name='us-east-1')
    conn.create_queue(QueueName=""tester"")

    # Setup Manager
    manager = ManagerWorker([""tester""], 1, 1, 10)",update,"deftest_master_spawns_worker_processes():""""""Testmanagingprocesscreateschildworkers""""""#SetupSQSQueueconn=boto.connect_sqs()conn.create_queue(""tester"")#SetupManagermanager=ManagerWorker([""tester""],1,1,10)","@mock_sqs_deprecateddeftest_master_spawns_worker_processes():""""""Testmanagingprocesscreateschildworkers""""""#SetupSQSQueueconn=boto3.client('sqs',region_name='us-east-1')conn.create_queue(QueueName=""tester"")#SetupManagermanager=ManagerWorker([""tester""],1,1,10)",tests/test_manager_worker.py
spulec/PyQS,ac07140b339b8c8c943a9f2e0b11ff862b7e3f5e,boto3,boto,"def test_master_replaces_reader_processes():
    """"""
    Test managing process replaces reader children
    """"""

    # Setup SQS Queue
    conn = boto.connect_sqs()
    conn.create_queue(""tester"")

    # Setup Manager
    manager = ManagerWorker(queue_prefixes=[""tester""], worker_concurrency=1, interval=1, batchsize=10)","@mock_sqs_deprecated
def test_master_replaces_reader_processes():
    """"""
    Test managing process replaces reader children
    """"""

    # Setup SQS Queue
    conn = boto3.client('sqs', region_name='us-east-1')
    conn.create_queue(QueueName=""tester"")

    # Setup Manager
    manager = ManagerWorker(queue_prefixes=[""tester""], worker_concurrency=1, interval=1, batchsize=10)",update,"deftest_master_replaces_reader_processes():""""""Testmanagingprocessreplacesreaderchildren""""""#SetupSQSQueueconn=boto.connect_sqs()conn.create_queue(""tester"")#SetupManagermanager=ManagerWorker(queue_prefixes=[""tester""],worker_concurrency=1,interval=1,batchsize=10)","@mock_sqs_deprecateddeftest_master_replaces_reader_processes():""""""Testmanagingprocessreplacesreaderchildren""""""#SetupSQSQueueconn=boto3.client('sqs',region_name='us-east-1')conn.create_queue(QueueName=""tester"")#SetupManagermanager=ManagerWorker(queue_prefixes=[""tester""],worker_concurrency=1,interval=1,batchsize=10)",tests/test_manager_worker.py
spulec/PyQS,ac07140b339b8c8c943a9f2e0b11ff862b7e3f5e,boto3,boto,"# Setup SQS Queue
    conn = boto.connect_sqs()
    queue = conn.create_queue(""tester"")","    # Setup SQS Queue
    conn = boto3.client('sqs', region_name='us-east-1')
    queue_url = conn.create_queue(QueueName=""tester"")['QueueUrl']",update,"#SetupSQSQueueconn=boto.connect_sqs()queue=conn.create_queue(""tester"")","#SetupSQSQueueconn=boto3.client('sqs',region_name='us-east-1')queue_url=conn.create_queue(QueueName=""tester"")['QueueUrl']",tests/test_manager_worker.py
spulec/PyQS,ac07140b339b8c8c943a9f2e0b11ff862b7e3f5e,boto3,boto,"    for _ in range(20):
        queue.write(message)","    for _ in range(20):
        conn.send_message(QueueUrl=queue_url, MessageBody=message)",update,for_inrange(20):queue.write(message),"for_inrange(20):conn.send_message(QueueUrl=queue_url,MessageBody=message)",tests/test_manager_worker.py
spulec/PyQS,ac07140b339b8c8c943a9f2e0b11ff862b7e3f5e,boto3,boto,"def test_basic_delay():
    """"""
    Test delaying task to default queue
    """"""
    conn = boto.connect_sqs()
    conn.create_queue(""tests.tasks.index_incrementer"")

    index_incrementer.delay(""foobar"", **{'extra': 'more'})

    all_queues = conn.get_all_queues()
    len(all_queues).should.equal(1)

    queue = all_queues[0]
    queue.name.should.equal(""tests.tasks.index_incrementer"")
    queue.count().should.equal(1)


    message = queue.get_messages()[0].get_body()
    message_dict = json.loads(message)
    message_dict.should.equal({
        'task': 'tests.tasks.index_incrementer',
        'args': [""foobar""],","def test_basic_delay():
    """"""
    Test delaying task to default queue
    """"""
    conn = boto3.client('sqs', region_name='us-east-1')
    conn.create_queue(QueueName=""tests.tasks.index_incrementer"")

    index_incrementer.delay(""foobar"", **{'extra': 'more'})

    all_queues = conn.list_queues()['QueueUrls']
    len(all_queues).should.equal(1)

    queue_url = all_queues[0]
    queue_url.should.equal(""https://queue.amazonaws.com/123456789012/tests.tasks.index_incrementer"")
    queue = conn.get_queue_attributes(QueueUrl=queue_url)['Attributes']
    queue['ApproximateNumberOfMessages'].should.equal('1')

    message = conn.receive_message(QueueUrl=queue_url)['Messages'][0]
    message_dict = json.loads(message['Body'])
    message_dict.should.equal({
        'task': 'tests.tasks.index_incrementer',
        'args': [""foobar""],",update,"deftest_basic_delay():""""""Testdelayingtasktodefaultqueue""""""conn=boto.connect_sqs()conn.create_queue(""tests.tasks.index_incrementer"")index_incrementer.delay(""foobar"",**{'extra':'more'})all_queues=conn.get_all_queues()len(all_queues).should.equal(1)queue=all_queues[0]queue.name.should.equal(""tests.tasks.index_incrementer"")queue.count().should.equal(1)message=queue.get_messages()[0].get_body()message_dict=json.loads(message)message_dict.should.equal({'task':'tests.tasks.index_incrementer','args':[""foobar""],","deftest_basic_delay():""""""Testdelayingtasktodefaultqueue""""""conn=boto3.client('sqs',region_name='us-east-1')conn.create_queue(QueueName=""tests.tasks.index_incrementer"")index_incrementer.delay(""foobar"",**{'extra':'more'})all_queues=conn.list_queues()['QueueUrls']len(all_queues).should.equal(1)queue_url=all_queues[0]queue_url.should.equal(""https://queue.amazonaws.com/123456789012/tests.tasks.index_incrementer"")queue=conn.get_queue_attributes(QueueUrl=queue_url)['Attributes']queue['ApproximateNumberOfMessages'].should.equal('1')message=conn.receive_message(QueueUrl=queue_url)['Messages'][0]message_dict=json.loads(message['Body'])message_dict.should.equal({'task':'tests.tasks.index_incrementer','args':[""foobar""],",tests/test_tasks.py
spulec/PyQS,ac07140b339b8c8c943a9f2e0b11ff862b7e3f5e,boto3,boto,"def test_specified_queue():
    """"""
    Test delaying task to specific queue
    """"""
    conn = boto.connect_sqs()

    send_email.delay(""email subject"")

    all_queues = conn.get_all_queues()
    len(all_queues).should.equal(1)

    queue = all_queues[0]
    queue.name.should.equal(""email"")
    queue.count().should.equal(1)","@mock_sqs_deprecated()
def test_specified_queue():
    """"""
    Test delaying task to specific queue
    """"""
    conn = boto3.client('sqs', region_name='us-east-1')

    send_email.delay(""email subject"")

    queue_urls = conn.list_queues()['QueueUrls']
    len(queue_urls).should.equal(1)

    queue_url = queue_urls[0]
    queue_url.should.equal(""https://queue.amazonaws.com/123456789012/email"")
    queue = conn.get_queue_attributes(QueueUrl=queue_url)['Attributes']
    queue['ApproximateNumberOfMessages'].should.equal('1')",update,"deftest_specified_queue():""""""Testdelayingtasktospecificqueue""""""conn=boto.connect_sqs()send_email.delay(""emailsubject"")all_queues=conn.get_all_queues()len(all_queues).should.equal(1)queue=all_queues[0]queue.name.should.equal(""email"")queue.count().should.equal(1)","@mock_sqs_deprecated()deftest_specified_queue():""""""Testdelayingtasktospecificqueue""""""conn=boto3.client('sqs',region_name='us-east-1')send_email.delay(""emailsubject"")queue_urls=conn.list_queues()['QueueUrls']len(queue_urls).should.equal(1)queue_url=queue_urls[0]queue_url.should.equal(""https://queue.amazonaws.com/123456789012/email"")queue=conn.get_queue_attributes(QueueUrl=queue_url)['Attributes']queue['ApproximateNumberOfMessages'].should.equal('1')",tests/test_tasks.py
spulec/PyQS,ac07140b339b8c8c943a9f2e0b11ff862b7e3f5e,boto3,boto,"def test_message_delay():
    """"""
    Test delaying task with delay_seconds
    """"""
    conn = boto.connect_sqs()

    delayed_task.delay()

    all_queues = conn.get_all_queues()
    len(all_queues).should.equal(1)

    queue = all_queues[0]
    queue.name.should.equal(""delayed"")
    queue.count().should.equal(0)","@mock_sqs_deprecated()
def test_message_delay():
    """"""
    Test delaying task with delay_seconds
    """"""
    conn = boto3.client('sqs', region_name='us-east-1')

    delayed_task.delay()

    queue_urls = conn.list_queues()['QueueUrls']
    len(queue_urls).should.equal(1)

    queue_url = queue_urls[0]
    queue_url.should.equal(""https://queue.amazonaws.com/123456789012/delayed"")
    queue = conn.get_queue_attributes(QueueUrl=queue_url)['Attributes']
    queue['ApproximateNumberOfMessages'].should.equal('0')",update,"deftest_message_delay():""""""Testdelayingtaskwithdelay_seconds""""""conn=boto.connect_sqs()delayed_task.delay()all_queues=conn.get_all_queues()len(all_queues).should.equal(1)queue=all_queues[0]queue.name.should.equal(""delayed"")queue.count().should.equal(0)","@mock_sqs_deprecated()deftest_message_delay():""""""Testdelayingtaskwithdelay_seconds""""""conn=boto3.client('sqs',region_name='us-east-1')delayed_task.delay()queue_urls=conn.list_queues()['QueueUrls']len(queue_urls).should.equal(1)queue_url=queue_urls[0]queue_url.should.equal(""https://queue.amazonaws.com/123456789012/delayed"")queue=conn.get_queue_attributes(QueueUrl=queue_url)['Attributes']queue['ApproximateNumberOfMessages'].should.equal('0')",tests/test_tasks.py
spulec/PyQS,ac07140b339b8c8c943a9f2e0b11ff862b7e3f5e,boto3,boto,"def test_message_add_delay():
    """"""
    Test configuring the delay time of a task
    """"""
    conn = boto.connect_sqs()

    send_email.delay(""email subject"", _delay_seconds=5)

    all_queues = conn.get_all_queues()
    len(all_queues).should.equal(1)

    queue = all_queues[0]
    queue.name.should.equal(""email"")
    queue.count().should.equal(0)","@mock_sqs_deprecated()
def test_message_add_delay():
    """"""
    Test configuring the delay time of a task
    """"""
    conn = boto3.client('sqs', region_name='us-east-1')

    send_email.delay(""email subject"", _delay_seconds=5)

    queue_urls = conn.list_queues()['QueueUrls']
    len(queue_urls).should.equal(1)

    queue_url = queue_urls[0]
    queue_url.should.equal(""https://queue.amazonaws.com/123456789012/email"")
    queue = conn.get_queue_attributes(QueueUrl=queue_url)['Attributes']
    queue['ApproximateNumberOfMessages'].should.equal('0')",update,"deftest_message_add_delay():""""""Testconfiguringthedelaytimeofatask""""""conn=boto.connect_sqs()send_email.delay(""emailsubject"",_delay_seconds=5)all_queues=conn.get_all_queues()len(all_queues).should.equal(1)queue=all_queues[0]queue.name.should.equal(""email"")queue.count().should.equal(0)","@mock_sqs_deprecated()deftest_message_add_delay():""""""Testconfiguringthedelaytimeofatask""""""conn=boto3.client('sqs',region_name='us-east-1')send_email.delay(""emailsubject"",_delay_seconds=5)queue_urls=conn.list_queues()['QueueUrls']len(queue_urls).should.equal(1)queue_url=queue_urls[0]queue_url.should.equal(""https://queue.amazonaws.com/123456789012/email"")queue=conn.get_queue_attributes(QueueUrl=queue_url)['Attributes']queue['ApproximateNumberOfMessages'].should.equal('0')",tests/test_tasks.py
spulec/PyQS,ac07140b339b8c8c943a9f2e0b11ff862b7e3f5e,boto3,boto,"def test_message_no_delay():
    """"""
    Test removing the delay time of a task
    """"""
    conn = boto.connect_sqs()

    delayed_task.delay(_delay_seconds=0)

    all_queues = conn.get_all_queues()
    len(all_queues).should.equal(1)

    queue = all_queues[0]
    queue.name.should.equal(""delayed"")
    queue.count().should.equal(1)","def test_message_no_delay():
    """"""
    Test removing the delay time of a task
    """"""
    conn = boto3.client('sqs', region_name='us-east-1')

    delayed_task.delay(_delay_seconds=0)

    queue_urls = conn.list_queues()['QueueUrls']
    len(queue_urls).should.equal(1)

    queue_url = queue_urls[0]
    queue_url.should.equal(""https://queue.amazonaws.com/123456789012/delayed"")
    queue = conn.get_queue_attributes(QueueUrl=queue_url)['Attributes']
    queue['ApproximateNumberOfMessages'].should.equal('1')",update,"deftest_message_no_delay():""""""Testremovingthedelaytimeofatask""""""conn=boto.connect_sqs()delayed_task.delay(_delay_seconds=0)all_queues=conn.get_all_queues()len(all_queues).should.equal(1)queue=all_queues[0]queue.name.should.equal(""delayed"")queue.count().should.equal(1)","deftest_message_no_delay():""""""Testremovingthedelaytimeofatask""""""conn=boto3.client('sqs',region_name='us-east-1')delayed_task.delay(_delay_seconds=0)queue_urls=conn.list_queues()['QueueUrls']len(queue_urls).should.equal(1)queue_url=queue_urls[0]queue_url.should.equal(""https://queue.amazonaws.com/123456789012/delayed"")queue=conn.get_queue_attributes(QueueUrl=queue_url)['Attributes']queue['ApproximateNumberOfMessages'].should.equal('1')",tests/test_tasks.py
spulec/PyQS,ac07140b339b8c8c943a9f2e0b11ff862b7e3f5e,boto3,boto,"def test_custom_function_path():
    """"""
    Test delaying task with custom function path
    """"""
    conn = boto.connect_sqs()

    custom_path_task.delay()

    all_queues = conn.get_all_queues()
    queue = all_queues[0]
    queue.name.should.equal(""foobar"")
    queue.count().should.equal(1)



    message = queue.get_messages()[0].get_body()
    message_dict = json.loads(message)
    message_dict.should.equal({
        'task': 'custom_function.path',
        'args': [],","@mock_sqs_deprecated()
def test_custom_function_path():
    """"""
    Test delaying task with custom function path
    """"""
    conn = boto3.client('sqs', region_name='us-east-1')

    custom_path_task.delay()

    queue_urls = conn.list_queues()['QueueUrls']
    len(queue_urls).should.equal(1)
    queue_url = queue_urls[0]
    queue_url.should.equal(""https://queue.amazonaws.com/123456789012/foobar"")
    queue = conn.get_queue_attributes(QueueUrl=queue_url)['Attributes']
    queue['ApproximateNumberOfMessages'].should.equal('1')

    message = conn.receive_message(QueueUrl=queue_url)['Messages'][0]
    message_dict = json.loads(message['Body'])
    message_dict.should.equal({
        'task': 'custom_function.path',
        'args': [],",update,"deftest_custom_function_path():""""""Testdelayingtaskwithcustomfunctionpath""""""conn=boto.connect_sqs()custom_path_task.delay()all_queues=conn.get_all_queues()queue=all_queues[0]queue.name.should.equal(""foobar"")queue.count().should.equal(1)message=queue.get_messages()[0].get_body()message_dict=json.loads(message)message_dict.should.equal({'task':'custom_function.path','args':[],","@mock_sqs_deprecated()deftest_custom_function_path():""""""Testdelayingtaskwithcustomfunctionpath""""""conn=boto3.client('sqs',region_name='us-east-1')custom_path_task.delay()queue_urls=conn.list_queues()['QueueUrls']len(queue_urls).should.equal(1)queue_url=queue_urls[0]queue_url.should.equal(""https://queue.amazonaws.com/123456789012/foobar"")queue=conn.get_queue_attributes(QueueUrl=queue_url)['Attributes']queue['ApproximateNumberOfMessages'].should.equal('1')message=conn.receive_message(QueueUrl=queue_url)['Messages'][0]message_dict=json.loads(message['Body'])message_dict.should.equal({'task':'custom_function.path','args':[],",tests/test_tasks.py
wamdam/backy2,4736930be19a91c75550434f356a8e9c31a11251,boto3,boto,"import boto.exception
import boto.s3.connection","import boto3
from botocore.client import Config as BotoCoreClientConfig
from botocore.exceptions import ClientError
from botocore.handlers import set_list_objects_encoding_type_url",update,importboto.exceptionimportboto.s3.connection,importboto3frombotocore.clientimportConfigasBotoCoreClientConfigfrombotocore.exceptionsimportClientErrorfrombotocore.handlersimportset_list_objects_encoding_type_url,src/backy2/data_backends/s3.py
wamdam/backy2,4736930be19a91c75550434f356a8e9c31a11251,boto3,boto,"def __init__(self, config):
        aws_access_key_id = config.get('aws_access_key_id')





        aws_secret_access_key = config.get('aws_secret_access_key')
        host = config.get('host')
        port = config.getint('port')
        is_secure = config.getboolean('is_secure')
        bucket_name = config.get('bucket_name', 'backy2')









        simultaneous_writes = config.getint('simultaneous_writes', 1)
        simultaneous_reads = config.getint('simultaneous_reads', 1)
        calling_format=boto.s3.connection.OrdinaryCallingFormat()  # TODO: Old?
        bandwidth_read = config.getint('bandwidth_read', 0)
        bandwidth_write = config.getint('bandwidth_write', 0)

        self.read_throttling = TokenBucket()
        self.read_throttling.set_rate(bandwidth_read)  # 0 disables throttling
        self.write_throttling = TokenBucket()
        self.write_throttling.set_rate(bandwidth_write)  # 0 disables throttling

        self.conn = boto.connect_s3(
                aws_access_key_id=aws_access_key_id,
                aws_secret_access_key=aws_secret_access_key,
                host=host,
                port=port,
                is_secure=is_secure,
                calling_format=calling_format
            )
        # create our bucket
        try:
            self.bucket = self.conn.create_bucket(bucket_name)
        except boto.exception.S3CreateError:
            # exists...
            self.bucket = self.conn.get_bucket(bucket_name)
            pass
        except OSError as e:
            # no route to host
            self.fatal_error = e
            logger.error('Fatal error, dying: {}'.format(e))
            print('Fatal error: {}'.format(e))
            exit(10)","def __init__(self, config):
        aws_access_key_id = config.get('aws_access_key_id')
        if aws_access_key_id is None:
            aws_access_key_id_file = config.get('aws_access_key_id_file')
            with open(aws_access_key_id_file, 'r', encoding=""ascii"") as f:
                aws_access_key_id = f.read().rstrip()

        aws_secret_access_key = config.get('aws_secret_access_key')
        if aws_secret_access_key is None:
            aws_secret_access_key_file = config.get('aws_secret_access_key_file')
            with open(aws_secret_access_key_file, 'r', encoding=""ascii"") as f:
                aws_secret_access_key = f.read().rstrip()

        region_name = config.get('region_name', '')
        endpoint_url = config.get('endpoint_url', '')
        use_ssl = config.get('use_ssl', '')
        self._bucket_name = config.get('bucket_name', '')
        addressing_style = config.get('addressing_style', '')
        signature_version = config.get('signature_version', '')
        self._disable_encoding_type = config.get('disable_encoding_type', '')

        simultaneous_writes = config.getint('simultaneous_writes', 1)
        simultaneous_reads = config.getint('simultaneous_reads', 1)

        bandwidth_read = config.getint('bandwidth_read', 0)
        bandwidth_write = config.getint('bandwidth_write', 0)

        self.read_throttling = TokenBucket()
        self.read_throttling.set_rate(bandwidth_read)  # 0 disables throttling
        self.write_throttling = TokenBucket()
        self.write_throttling.set_rate(bandwidth_write)  # 0 disables throttling


        self._resource_config = {
            'aws_access_key_id': aws_access_key_id,
            'aws_secret_access_key': aws_secret_access_key,
        }

        if region_name:
            self._resource_config['region_name'] = region_name

        if endpoint_url:
            self._resource_config['endpoint_url'] = endpoint_url

        if use_ssl:
            self._resource_config['use_ssl'] = use_ssl

        resource_config = {}
        if addressing_style:
            resource_config['s3'] = {'addressing_style': addressing_style}

        if signature_version:
            resource_config['signature_version'] = signature_version

        self._resource_config['config'] = BotoCoreClientConfig(**resource_config)",update,"def__init__(self,config):aws_access_key_id=config.get('aws_access_key_id')aws_secret_access_key=config.get('aws_secret_access_key')host=config.get('host')port=config.getint('port')is_secure=config.getboolean('is_secure')bucket_name=config.get('bucket_name','backy2')simultaneous_writes=config.getint('simultaneous_writes',1)simultaneous_reads=config.getint('simultaneous_reads',1)calling_format=boto.s3.connection.OrdinaryCallingFormat()#TODO:Old?bandwidth_read=config.getint('bandwidth_read',0)bandwidth_write=config.getint('bandwidth_write',0)self.read_throttling=TokenBucket()self.read_throttling.set_rate(bandwidth_read)#0disablesthrottlingself.write_throttling=TokenBucket()self.write_throttling.set_rate(bandwidth_write)#0disablesthrottlingself.conn=boto.connect_s3(aws_access_key_id=aws_access_key_id,aws_secret_access_key=aws_secret_access_key,host=host,port=port,is_secure=is_secure,calling_format=calling_format)#createourbuckettry:self.bucket=self.conn.create_bucket(bucket_name)exceptboto.exception.S3CreateError:#exists...self.bucket=self.conn.get_bucket(bucket_name)passexceptOSErrorase:#noroutetohostself.fatal_error=elogger.error('Fatalerror,dying:{}'.format(e))print('Fatalerror:{}'.format(e))exit(10)","def__init__(self,config):aws_access_key_id=config.get('aws_access_key_id')ifaws_access_key_idisNone:aws_access_key_id_file=config.get('aws_access_key_id_file')withopen(aws_access_key_id_file,'r',encoding=""ascii"")asf:aws_access_key_id=f.read().rstrip()aws_secret_access_key=config.get('aws_secret_access_key')ifaws_secret_access_keyisNone:aws_secret_access_key_file=config.get('aws_secret_access_key_file')withopen(aws_secret_access_key_file,'r',encoding=""ascii"")asf:aws_secret_access_key=f.read().rstrip()region_name=config.get('region_name','')endpoint_url=config.get('endpoint_url','')use_ssl=config.get('use_ssl','')self._bucket_name=config.get('bucket_name','')addressing_style=config.get('addressing_style','')signature_version=config.get('signature_version','')self._disable_encoding_type=config.get('disable_encoding_type','')simultaneous_writes=config.getint('simultaneous_writes',1)simultaneous_reads=config.getint('simultaneous_reads',1)bandwidth_read=config.getint('bandwidth_read',0)bandwidth_write=config.getint('bandwidth_write',0)self.read_throttling=TokenBucket()self.read_throttling.set_rate(bandwidth_read)#0disablesthrottlingself.write_throttling=TokenBucket()self.write_throttling.set_rate(bandwidth_write)#0disablesthrottlingself._resource_config={'aws_access_key_id':aws_access_key_id,'aws_secret_access_key':aws_secret_access_key,}ifregion_name:self._resource_config['region_name']=region_nameifendpoint_url:self._resource_config['endpoint_url']=endpoint_urlifuse_ssl:self._resource_config['use_ssl']=use_sslresource_config={}ifaddressing_style:resource_config['s3']={'addressing_style':addressing_style}ifsignature_version:resource_config['signature_version']=signature_versionself._resource_config['config']=BotoCoreClientConfig(**resource_config)",src/backy2/data_backends/s3.py
wamdam/backy2,4736930be19a91c75550434f356a8e9c31a11251,boto3,boto,,"def _get_bucket(self):
        session = boto3.session.Session()
        if self._disable_encoding_type:
            session.events.unregister('before-parameter-build.s3.ListObjects', set_list_objects_encoding_type_url)
        resource = session.resource('s3', **self._resource_config)
        bucket = resource.Bucket(self._bucket_name)
        return bucket",insert,,"def_get_bucket(self):session=boto3.session.Session()ifself._disable_encoding_type:session.events.unregister('before-parameter-build.s3.ListObjects',set_list_objects_encoding_type_url)resource=session.resource('s3',**self._resource_config)bucket=resource.Bucket(self._bucket_name)returnbucket",src/backy2/data_backends/s3.py
wamdam/backy2,4736930be19a91c75550434f356a8e9c31a11251,boto3,boto,"def _writer(self, id_):
        """""" A threaded background writer """"""

        while True:
            entry = self._write_queue.get()
            if entry is None or self.fatal_error:
                logger.debug(""Writer {} finishing."".format(id_))
                break
            uid, data = entry
            self.writer_thread_status[id_] = STATUS_THROTTLING
            time.sleep(self.write_throttling.consume(len(data)))
            self.writer_thread_status[id_] = STATUS_NOTHING
            t1 = time.time()
            self.writer_thread_status[id_] = STATUS_NEWKEY
            key = self.bucket.new_key(uid)



            self.writer_thread_status[id_] = STATUS_NOTHING
            try:
                self.writer_thread_status[id_] = STATUS_WRITING
                r = key.set_contents_from_string(data)
                self.writer_thread_status[id_] = STATUS_NOTHING
            except (
                    OSError,
                    boto.exception.BotoServerError,
                    boto.exception.S3ResponseError,
                    ) as e:
                # OSError happens when the S3 host is gone (i.e. network died,
                # host down, ...). boto tries hard to recover, however after
                # several attempts it will give up and raise.
                # BotoServerError happens, when there is no server.
                # S3ResponseError sometimes happens, when the cluster is about
                # to shutdown. Hard to reproduce because the writer must write
                # in exactly this moment.
                # We let the backup job die here fataly.
                self.fatal_error = e
                logger.error('Fatal error, dying: {}'.format(e))
                #exit('Fatal error: {}'.format(e))  # this only raises SystemExit
                os._exit(11)
            t2 = time.time()
            assert r == len(data)
            self._write_queue.task_done()
            logger.debug('Writer {} wrote data async. uid {} in {:.2f}s (Queue size is {})'.format(id_, uid, t2-t1, self._write_queue.qsize()))","def _writer(self, id_):
        """""" A threaded background writer """"""
        bucket = self._get_bucket()
        while True:
            entry = self._write_queue.get()
            if entry is None or self.fatal_error:
                logger.debug(""Writer {} finishing."".format(id_))
                break
            uid, data = entry
            self.writer_thread_status[id_] = STATUS_THROTTLING
            time.sleep(self.write_throttling.consume(len(data)))
            self.writer_thread_status[id_] = STATUS_NOTHING
            t1 = time.time()
            self.writer_thread_status[id_] = STATUS_NEWKEY
            obj = bucket.Object(uid)
            self.writer_thread_status[id_] = STATUS_NOTHING
            self.writer_thread_status[id_] = STATUS_WRITING
            obj.put(Body=data)
            self.writer_thread_status[id_] = STATUS_NOTHING

            t2 = time.time()

            self._write_queue.task_done()
            logger.debug('Writer {} wrote data async. uid {} in {:.2f}s (Queue size is {})'.format(id_, uid, t2-t1, self._write_queue.qsize()))",update,"def_writer(self,id_):""""""Athreadedbackgroundwriter""""""whileTrue:entry=self._write_queue.get()ifentryisNoneorself.fatal_error:logger.debug(""Writer{}finishing."".format(id_))breakuid,data=entryself.writer_thread_status[id_]=STATUS_THROTTLINGtime.sleep(self.write_throttling.consume(len(data)))self.writer_thread_status[id_]=STATUS_NOTHINGt1=time.time()self.writer_thread_status[id_]=STATUS_NEWKEYkey=self.bucket.new_key(uid)self.writer_thread_status[id_]=STATUS_NOTHINGtry:self.writer_thread_status[id_]=STATUS_WRITINGr=key.set_contents_from_string(data)self.writer_thread_status[id_]=STATUS_NOTHINGexcept(OSError,boto.exception.BotoServerError,boto.exception.S3ResponseError,)ase:#OSErrorhappenswhentheS3hostisgone(i.e.networkdied,#hostdown,...).bototrieshardtorecover,howeverafter#severalattemptsitwillgiveupandraise.#BotoServerErrorhappens,whenthereisnoserver.#S3ResponseErrorsometimeshappens,whentheclusterisabout#toshutdown.Hardtoreproducebecausethewritermustwrite#inexactlythismoment.#Weletthebackupjobdieherefataly.self.fatal_error=elogger.error('Fatalerror,dying:{}'.format(e))#exit('Fatalerror:{}'.format(e))#thisonlyraisesSystemExitos._exit(11)t2=time.time()assertr==len(data)self._write_queue.task_done()logger.debug('Writer{}wrotedataasync.uid{}in{:.2f}s(Queuesizeis{})'.format(id_,uid,t2-t1,self._write_queue.qsize()))","def_writer(self,id_):""""""Athreadedbackgroundwriter""""""bucket=self._get_bucket()whileTrue:entry=self._write_queue.get()ifentryisNoneorself.fatal_error:logger.debug(""Writer{}finishing."".format(id_))breakuid,data=entryself.writer_thread_status[id_]=STATUS_THROTTLINGtime.sleep(self.write_throttling.consume(len(data)))self.writer_thread_status[id_]=STATUS_NOTHINGt1=time.time()self.writer_thread_status[id_]=STATUS_NEWKEYobj=bucket.Object(uid)self.writer_thread_status[id_]=STATUS_NOTHINGself.writer_thread_status[id_]=STATUS_WRITINGobj.put(Body=data)self.writer_thread_status[id_]=STATUS_NOTHINGt2=time.time()self._write_queue.task_done()logger.debug('Writer{}wrotedataasync.uid{}in{:.2f}s(Queuesizeis{})'.format(id_,uid,t2-t1,self._write_queue.qsize()))",src/backy2/data_backends/s3.py
zulip/zulip,cea7d713cdb9297cd9856ee4fbbcb79747872ae8,boto3,boto,"from boto.s3.connection import S3Connection
from boto.s3.key import Key  # for mypy","import boto3
from boto3.resources.base import ServiceResource",update,fromboto.s3.connectionimportS3Connectionfromboto.s3.keyimportKey#formypy,importboto3fromboto3.resources.baseimportServiceResource,zerver/lib/export.py
zulip/zulip,cea7d713cdb9297cd9856ee4fbbcb79747872ae8,boto3,boto,"def _check_key_metadata(email_gateway_bot: Optional[UserProfile],
                        key: Key, processing_avatars: bool,
                        realm: Realm, user_ids: Set[int]) -> None:","def _check_key_metadata(email_gateway_bot: Optional[UserProfile],
                        key: ServiceResource, processing_avatars: bool,
                        realm: Realm, user_ids: Set[int]) -> None:",update,"def_check_key_metadata(email_gateway_bot:Optional[UserProfile],key:Key,processing_avatars:bool,realm:Realm,user_ids:Set[int])->None:","def_check_key_metadata(email_gateway_bot:Optional[UserProfile],key:ServiceResource,processing_avatars:bool,realm:Realm,user_ids:Set[int])->None:",zerver/lib/export.py
zulip/zulip,cea7d713cdb9297cd9856ee4fbbcb79747872ae8,boto3,boto,"def _get_exported_s3_record(
        bucket_name: str,
        key: Key,
        processing_emoji: bool) -> Dict[str, Union[str, int]]:
    # Helper function for export_files_from_s3
    record = dict(s3_path=key.name, bucket=bucket_name,
                  size=key.size, last_modified=key.last_modified,
                  content_type=key.content_type, md5=key.md5)
    record.update(key.metadata)

    if processing_emoji:
        record['file_name'] = os.path.basename(key.name)

    if ""user_profile_id"" in record:
        user_profile = get_user_profile_by_id(record['user_profile_id'])","def _get_exported_s3_record(
        bucket_name: str,
        key: ServiceResource,
        processing_emoji: bool) -> Dict[str, Union[str, int]]:
    # Helper function for export_files_from_s3
    record = dict(s3_path=key.key, bucket=bucket_name,
                  size=key.content_length, last_modified=key.last_modified,
                  content_type=key.content_type, md5=key.e_tag)
    record.update(key.metadata)

    if processing_emoji:
        record['file_name'] = os.path.basename(key.key)

    if ""user_profile_id"" in record:
        user_profile = get_user_profile_by_id(record['user_profile_id'])",update,"def_get_exported_s3_record(bucket_name:str,key:Key,processing_emoji:bool)->Dict[str,Union[str,int]]:#Helperfunctionforexport_files_from_s3record=dict(s3_path=key.name,bucket=bucket_name,size=key.size,last_modified=key.last_modified,content_type=key.content_type,md5=key.md5)record.update(key.metadata)ifprocessing_emoji:record['file_name']=os.path.basename(key.name)if""user_profile_id""inrecord:user_profile=get_user_profile_by_id(record['user_profile_id'])","def_get_exported_s3_record(bucket_name:str,key:ServiceResource,processing_emoji:bool)->Dict[str,Union[str,int]]:#Helperfunctionforexport_files_from_s3record=dict(s3_path=key.key,bucket=bucket_name,size=key.content_length,last_modified=key.last_modified,content_type=key.content_type,md5=key.e_tag)record.update(key.metadata)ifprocessing_emoji:record['file_name']=os.path.basename(key.key)if""user_profile_id""inrecord:user_profile=get_user_profile_by_id(record['user_profile_id'])",zerver/lib/export.py
zulip/zulip,cea7d713cdb9297cd9856ee4fbbcb79747872ae8,boto3,boto,"def _save_s3_object_to_file(key: Key, output_dir: str, processing_avatars: bool,
                            processing_emoji: bool, processing_realm_icon_and_logo: bool) -> None:
    # Helper function for export_files_from_s3
    if processing_avatars or processing_emoji or processing_realm_icon_and_logo:
        filename = os.path.join(output_dir, key.name)
    else:
        fields = key.name.split('/')
        if len(fields) != 3:
            raise AssertionError(""Suspicious key with invalid format %s"" % (key.name,))
        filename = os.path.join(output_dir, key.name)

    if ""../"" in filename:
        raise AssertionError(""Suspicious file with invalid format %s"" % (filename,))

    dirname = os.path.dirname(filename)
    if not os.path.exists(dirname):
        os.makedirs(dirname)
    key.get_contents_to_filename(filename)","def _save_s3_object_to_file(key: ServiceResource, output_dir: str, processing_avatars: bool,
                            processing_emoji: bool, processing_realm_icon_and_logo: bool) -> None:
    # Helper function for export_files_from_s3
    if processing_avatars or processing_emoji or processing_realm_icon_and_logo:
        filename = os.path.join(output_dir, key.key)
    else:
        fields = key.key.split('/')
        if len(fields) != 3:
            raise AssertionError(""Suspicious key with invalid format %s"" % (key.key,))
        filename = os.path.join(output_dir, key.key)

    if ""../"" in filename:
        raise AssertionError(""Suspicious file with invalid format %s"" % (filename,))

    dirname = os.path.dirname(filename)
    if not os.path.exists(dirname):
        os.makedirs(dirname)
    key.download_file(filename)",update,"def_save_s3_object_to_file(key:Key,output_dir:str,processing_avatars:bool,processing_emoji:bool,processing_realm_icon_and_logo:bool)->None:#Helperfunctionforexport_files_from_s3ifprocessing_avatarsorprocessing_emojiorprocessing_realm_icon_and_logo:filename=os.path.join(output_dir,key.name)else:fields=key.name.split('/')iflen(fields)!=3:raiseAssertionError(""Suspiciouskeywithinvalidformat%s""%(key.name,))filename=os.path.join(output_dir,key.name)if""../""infilename:raiseAssertionError(""Suspiciousfilewithinvalidformat%s""%(filename,))dirname=os.path.dirname(filename)ifnotos.path.exists(dirname):os.makedirs(dirname)key.get_contents_to_filename(filename)","def_save_s3_object_to_file(key:ServiceResource,output_dir:str,processing_avatars:bool,processing_emoji:bool,processing_realm_icon_and_logo:bool)->None:#Helperfunctionforexport_files_from_s3ifprocessing_avatarsorprocessing_emojiorprocessing_realm_icon_and_logo:filename=os.path.join(output_dir,key.key)else:fields=key.key.split('/')iflen(fields)!=3:raiseAssertionError(""Suspiciouskeywithinvalidformat%s""%(key.key,))filename=os.path.join(output_dir,key.key)if""../""infilename:raiseAssertionError(""Suspiciousfilewithinvalidformat%s""%(filename,))dirname=os.path.dirname(filename)ifnotos.path.exists(dirname):os.makedirs(dirname)key.download_file(filename)",zerver/lib/export.py
zulip/zulip,cea7d713cdb9297cd9856ee4fbbcb79747872ae8,boto3,boto,"def export_files_from_s3(realm: Realm, bucket_name: str, output_dir: Path,
                         processing_avatars: bool=False, processing_emoji: bool=False,
                         processing_realm_icon_and_logo: bool=False) -> None:
    conn = S3Connection(settings.S3_KEY, settings.S3_SECRET_KEY)
    bucket = conn.get_bucket(bucket_name, validate=True)","def export_files_from_s3(realm: Realm, bucket_name: str, output_dir: Path,
                         processing_avatars: bool=False, processing_emoji: bool=False,
                         processing_realm_icon_and_logo: bool=False) -> None:
    session = boto3.Session(settings.S3_KEY, settings.S3_SECRET_KEY)
    s3 = session.resource('s3')
    bucket = s3.Bucket(bucket_name)",update,"defexport_files_from_s3(realm:Realm,bucket_name:str,output_dir:Path,processing_avatars:bool=False,processing_emoji:bool=False,processing_realm_icon_and_logo:bool=False)->None:conn=S3Connection(settings.S3_KEY,settings.S3_SECRET_KEY)bucket=conn.get_bucket(bucket_name,validate=True)","defexport_files_from_s3(realm:Realm,bucket_name:str,output_dir:Path,processing_avatars:bool=False,processing_emoji:bool=False,processing_realm_icon_and_logo:bool=False)->None:session=boto3.Session(settings.S3_KEY,settings.S3_SECRET_KEY)s3=session.resource('s3')bucket=s3.Bucket(bucket_name)",zerver/lib/export.py
zulip/zulip,cea7d713cdb9297cd9856ee4fbbcb79747872ae8,boto3,boto,"bucket_name = settings.S3_AUTH_UPLOADS_BUCKET
        conn = S3Connection(settings.S3_KEY, settings.S3_SECRET_KEY)
        bucket = conn.get_bucket(bucket_name, validate=True)","bucket_name = settings.S3_AUTH_UPLOADS_BUCKET
        session = boto3.Session(settings.S3_KEY, settings.S3_SECRET_KEY)
        bucket = session.resource('s3').Bucket(bucket_name)",update,"bucket_name=settings.S3_AUTH_UPLOADS_BUCKETconn=S3Connection(settings.S3_KEY,settings.S3_SECRET_KEY)bucket=conn.get_bucket(bucket_name,validate=True)","bucket_name=settings.S3_AUTH_UPLOADS_BUCKETsession=boto3.Session(settings.S3_KEY,settings.S3_SECRET_KEY)bucket=session.resource('s3').Bucket(bucket_name)",zerver/lib/import_realm.py
zulip/zulip,cea7d713cdb9297cd9856ee4fbbcb79747872ae8,boto3,boto,"if s3_uploads:
            key = Key(bucket)
            key.key = relative_path"," if s3_uploads:
            key = bucket.Object(relative_path)
            metadata = {}",update,ifs3_uploads:key=Key(bucket)key.key=relative_path,ifs3_uploads:key=bucket.Object(relative_path)metadata={},zerver/lib/import_realm.py
zulip/zulip,cea7d713cdb9297cd9856ee4fbbcb79747872ae8,boto3,boto,"key.set_metadata(""user_profile_id"", str(user_profile.id))

            if 'last_modified' in record:
                key.set_metadata(""orig_last_modified"", str(record['last_modified']))
            key.set_metadata(""realm_id"", str(record['realm_id']))","metadata[""user_profile_id""] = str(user_profile.id)

            if 'last_modified' in record:
                metadata[""orig_last_modified""] = str(record['last_modified'])
            metadata[""realm_id""] = str(record['realm_id'])",update,"key.set_metadata(""user_profile_id"",str(user_profile.id))if'last_modified'inrecord:key.set_metadata(""orig_last_modified"",str(record['last_modified']))key.set_metadata(""realm_id"",str(record['realm_id']))","metadata[""user_profile_id""]=str(user_profile.id)if'last_modified'inrecord:metadata[""orig_last_modified""]=str(record['last_modified'])metadata[""realm_id""]=str(record['realm_id'])",zerver/lib/import_realm.py
zulip/zulip,cea7d713cdb9297cd9856ee4fbbcb79747872ae8,boto3,boto,"key.set_contents_from_filename(os.path.join(import_dir, record['path']), headers=headers)","key.upload_file(os.path.join(import_dir, record['path']),
                            ExtraArgs={
                                'ContentType': content_type,
                                'Metadata': metadata})",update,"key.set_contents_from_filename(os.path.join(import_dir,record['path']),headers=headers)","key.upload_file(os.path.join(import_dir,record['path']),ExtraArgs={'ContentType':content_type,'Metadata':metadata})",zerver/lib/import_realm.py
zulip/zulip,cea7d713cdb9297cd9856ee4fbbcb79747872ae8,boto3,boto,"def create_s3_buckets(*bucket_names: Tuple[str]) -> List[Bucket]:
    conn = S3Connection(settings.S3_KEY, settings.S3_SECRET_KEY)
    buckets = [conn.create_bucket(name) for name in bucket_names]","def create_s3_buckets(*bucket_names: Tuple[str]) -> List[ServiceResource]:
    session = boto3.Session(settings.S3_KEY, settings.S3_SECRET_KEY)
    s3 = session.resource('s3')
    buckets = [s3.create_bucket(Bucket=name) for name in bucket_names]",update,"defcreate_s3_buckets(*bucket_names:Tuple[str])->List[Bucket]:conn=S3Connection(settings.S3_KEY,settings.S3_SECRET_KEY)buckets=[conn.create_bucket(name)fornameinbucket_names]","defcreate_s3_buckets(*bucket_names:Tuple[str])->List[ServiceResource]:session=boto3.Session(settings.S3_KEY,settings.S3_SECRET_KEY)s3=session.resource('s3')buckets=[s3.create_bucket(Bucket=name)fornameinbucket_names]",zerver/lib/test_helpers.py
zulip/zulip,cea7d713cdb9297cd9856ee4fbbcb79747872ae8,boto3,boto,"from boto.s3.bucket import Bucket
from boto.s3.key import Key
from boto.s3.connection import S3Connection","import boto3
import botocore
from botocore.client import Config
from boto3.resources.base import ServiceResource
from boto3.session import Session",update,fromboto.s3.bucketimportBucketfromboto.s3.keyimportKeyfromboto.s3.connectionimportS3Connection,importboto3importbotocorefrombotocore.clientimportConfigfromboto3.resources.baseimportServiceResourcefromboto3.sessionimportSession,zerver/lib/upload.py
zulip/zulip,cea7d713cdb9297cd9856ee4fbbcb79747872ae8,boto3,boto,"def get_bucket(conn: S3Connection, bucket_name: str) -> Bucket:
    # Calling get_bucket() with validate=True can apparently lead
    # to expensive S3 bills:
    #    https://www.appneta.com/blog/s3-list-get-bucket-default/
    # The benefits of validation aren't completely clear to us, and
    # we want to save on our bills, so we set the validate flag to False.
    # (We think setting validate to True would cause us to fail faster
    #  in situations where buckets don't exist, but that shouldn't be
    #  an issue for us.)
    bucket = conn.get_bucket(bucket_name, validate=False)
    return bucket","def get_bucket(session: Session, bucket_name: str) -> ServiceResource:
    # See https://github.com/python/typeshed/issues/2706
    # for why this return type is a `ServiceResource`.
    bucket = session.resource('s3').Bucket(bucket_name)






    return bucket",update,"defget_bucket(conn:S3Connection,bucket_name:str)->Bucket:#Callingget_bucket()withvalidate=Truecanapparentlylead#toexpensiveS3bills:#https://www.appneta.com/blog/s3-list-get-bucket-default/#Thebenefitsofvalidationaren'tcompletelycleartous,and#wewanttosaveonourbills,sowesetthevalidateflagtoFalse.#(WethinksettingvalidatetoTruewouldcauseustofailfaster#insituationswherebucketsdon'texist,butthatshouldn'tbe#anissueforus.)bucket=conn.get_bucket(bucket_name,validate=False)returnbucket","defget_bucket(session:Session,bucket_name:str)->ServiceResource:#Seehttps://github.com/python/typeshed/issues/2706#forwhythisreturntypeisa`ServiceResource`.bucket=session.resource('s3').Bucket(bucket_name)returnbucket",zerver/lib/upload.py
zulip/zulip,cea7d713cdb9297cd9856ee4fbbcb79747872ae8,boto3,boto,"def upload_image_to_s3(
        bucket_name: str,
        file_name: str,
        content_type: Optional[str],
        user_profile: UserProfile,
        contents: bytes) -> None:

    conn = S3Connection(settings.S3_KEY, settings.S3_SECRET_KEY)
    bucket = get_bucket(conn, bucket_name)
    key = Key(bucket)
    key.key = file_name
    key.set_metadata(""user_profile_id"", str(user_profile.id))
    key.set_metadata(""realm_id"", str(user_profile.realm_id))


    headers = {}
    if content_type is not None:
        headers[""Content-Type""] = content_type
    if content_type not in INLINE_MIME_TYPES:
        headers[""Content-Disposition""] = ""attachment""

    key.set_contents_from_string(contents, headers=headers)","def upload_image_to_s3(
        bucket_name: str,
        file_name: str,
        content_type: Optional[str],
        user_profile: UserProfile,
        contents: bytes) -> None:

    session = boto3.Session(settings.S3_KEY, settings.S3_SECRET_KEY)
    bucket = get_bucket(session, bucket_name)
    key = bucket.Object(file_name)
    metadata = {
        ""user_profile_id"": str(user_profile.id),
        ""realm_id"": str(user_profile.realm_id)
    }

    content_disposition = ''
    if content_type is None:
        content_type = ''
    if content_type not in INLINE_MIME_TYPES:
        content_disposition = ""attachment""

    key.put(Body=contents, Metadata=metadata, ContentType=content_type,
            ContentDisposition=content_disposition)",update,"defupload_image_to_s3(bucket_name:str,file_name:str,content_type:Optional[str],user_profile:UserProfile,contents:bytes)->None:conn=S3Connection(settings.S3_KEY,settings.S3_SECRET_KEY)bucket=get_bucket(conn,bucket_name)key=Key(bucket)key.key=file_namekey.set_metadata(""user_profile_id"",str(user_profile.id))key.set_metadata(""realm_id"",str(user_profile.realm_id))headers={}ifcontent_typeisnotNone:headers[""Content-Type""]=content_typeifcontent_typenotinINLINE_MIME_TYPES:headers[""Content-Disposition""]=""attachment""key.set_contents_from_string(contents,headers=headers)","defupload_image_to_s3(bucket_name:str,file_name:str,content_type:Optional[str],user_profile:UserProfile,contents:bytes)->None:session=boto3.Session(settings.S3_KEY,settings.S3_SECRET_KEY)bucket=get_bucket(session,bucket_name)key=bucket.Object(file_name)metadata={""user_profile_id"":str(user_profile.id),""realm_id"":str(user_profile.realm_id)}content_disposition=''ifcontent_typeisNone:content_type=''ifcontent_typenotinINLINE_MIME_TYPES:content_disposition=""attachment""key.put(Body=contents,Metadata=metadata,ContentType=content_type,ContentDisposition=content_disposition)",zerver/lib/upload.py
zulip/zulip,cea7d713cdb9297cd9856ee4fbbcb79747872ae8,boto3,boto,"def get_signed_upload_url(path: str) -> str:
    conn = S3Connection(settings.S3_KEY, settings.S3_SECRET_KEY)
    return conn.generate_url(SIGNED_UPLOAD_URL_DURATION, 'GET',
                             bucket=settings.S3_AUTH_UPLOADS_BUCKET, key=path)






def get_realm_for_filename(path: str) -> Optional[int]:
    conn = S3Connection(settings.S3_KEY, settings.S3_SECRET_KEY)
    key: Optional[Key] = get_bucket(conn, settings.S3_AUTH_UPLOADS_BUCKET).get_key(path)
    if key is None:
        # This happens if the key does not exist.



        return None
    return get_user_profile_by_id(key.metadata[""user_profile_id""]).realm_id","def get_signed_upload_url(path: str) -> str:
    client = boto3.client('s3', aws_access_key_id=settings.S3_KEY,
                          aws_secret_access_key=settings.S3_SECRET_KEY)
    return client.generate_presigned_url(ClientMethod='get_object',
                                         Params={
                                             'Bucket': settings.S3_AUTH_UPLOADS_BUCKET,
                                             'Key': path},
                                         ExpiresIn=SIGNED_UPLOAD_URL_DURATION,
                                         HttpMethod='GET')

def get_realm_for_filename(path: str) -> Optional[int]:
    session = boto3.Session(settings.S3_KEY, settings.S3_SECRET_KEY)
    bucket = get_bucket(session, settings.S3_AUTH_UPLOADS_BUCKET)
    key = bucket.Object(path)

    try:
        user_profile_id = key.metadata['user_profile_id']
    except botocore.exceptions.ClientError:
        return None
    return get_user_profile_by_id(user_profile_id).realm_id",update,"defget_signed_upload_url(path:str)->str:conn=S3Connection(settings.S3_KEY,settings.S3_SECRET_KEY)returnconn.generate_url(SIGNED_UPLOAD_URL_DURATION,'GET',bucket=settings.S3_AUTH_UPLOADS_BUCKET,key=path)defget_realm_for_filename(path:str)->Optional[int]:conn=S3Connection(settings.S3_KEY,settings.S3_SECRET_KEY)key:Optional[Key]=get_bucket(conn,settings.S3_AUTH_UPLOADS_BUCKET).get_key(path)ifkeyisNone:#Thishappensifthekeydoesnotexist.returnNonereturnget_user_profile_by_id(key.metadata[""user_profile_id""]).realm_id","defget_signed_upload_url(path:str)->str:client=boto3.client('s3',aws_access_key_id=settings.S3_KEY,aws_secret_access_key=settings.S3_SECRET_KEY)returnclient.generate_presigned_url(ClientMethod='get_object',Params={'Bucket':settings.S3_AUTH_UPLOADS_BUCKET,'Key':path},ExpiresIn=SIGNED_UPLOAD_URL_DURATION,HttpMethod='GET')defget_realm_for_filename(path:str)->Optional[int]:session=boto3.Session(settings.S3_KEY,settings.S3_SECRET_KEY)bucket=get_bucket(session,settings.S3_AUTH_UPLOADS_BUCKET)key=bucket.Object(path)try:user_profile_id=key.metadata['user_profile_id']exceptbotocore.exceptions.ClientError:returnNonereturnget_user_profile_by_id(user_profile_id).realm_id",zerver/lib/upload.py
zulip/zulip,cea7d713cdb9297cd9856ee4fbbcb79747872ae8,boto3,boto,"class S3UploadBackend(ZulipUploadBackend):
    def __init__(self) -> None:
        self.connection = S3Connection(settings.S3_KEY, settings.S3_SECRET_KEY)

    def delete_file_from_s3(self, path_id: str, bucket_name: str) -> bool:
        bucket = get_bucket(self.connection, bucket_name)

        # check if file exists
        key: Optional[Key] = bucket.get_key(path_id)
        if key is not None:
            bucket.delete_key(key)
            return True

        file_name = path_id.split(""/"")[-1]
        logging.warning(""%s does not exist. Its entry in the database will be removed."", file_name)
        return False","class S3UploadBackend(ZulipUploadBackend):
    def __init__(self) -> None:
        self.session = boto3.Session(settings.S3_KEY, settings.S3_SECRET_KEY)

    def delete_file_from_s3(self, path_id: str, bucket_name: str) -> bool:
        bucket = get_bucket(self.session, bucket_name)
        key = bucket.Object(path_id)

        try:
            key.load()
        except botocore.exceptions.ClientError:
            file_name = path_id.split(""/"")[-1]
            logging.warning(""%s does not exist. Its entry in the database will be removed."", file_name)
            return False
        key.delete()
        return True",update,"classS3UploadBackend(ZulipUploadBackend):def__init__(self)->None:self.connection=S3Connection(settings.S3_KEY,settings.S3_SECRET_KEY)defdelete_file_from_s3(self,path_id:str,bucket_name:str)->bool:bucket=get_bucket(self.connection,bucket_name)#checkiffileexistskey:Optional[Key]=bucket.get_key(path_id)ifkeyisnotNone:bucket.delete_key(key)returnTruefile_name=path_id.split(""/"")[-1]logging.warning(""%sdoesnotexist.Itsentryinthedatabasewillberemoved."",file_name)returnFalse","classS3UploadBackend(ZulipUploadBackend):def__init__(self)->None:self.session=boto3.Session(settings.S3_KEY,settings.S3_SECRET_KEY)defdelete_file_from_s3(self,path_id:str,bucket_name:str)->bool:bucket=get_bucket(self.session,bucket_name)key=bucket.Object(path_id)try:key.load()exceptbotocore.exceptions.ClientError:file_name=path_id.split(""/"")[-1]logging.warning(""%sdoesnotexist.Itsentryinthedatabasewillberemoved."",file_name)returnFalsekey.delete()returnTrue",zerver/lib/upload.py
zulip/zulip,cea7d713cdb9297cd9856ee4fbbcb79747872ae8,boto3,boto,"def get_avatar_key(self, file_name: str) -> Key:
        bucket = get_bucket(self.connection, settings.S3_AVATAR_BUCKET)

        key = bucket.get_key(file_name)
        return key","def get_avatar_key(self, file_name: str) -> ServiceResource:
        # See https://github.com/python/typeshed/issues/2706
        # for why this return type is a `ServiceResource`.
        bucket = get_bucket(self.session, settings.S3_AVATAR_BUCKET)

        key = bucket.Object(file_name)
        return key",update,"defget_avatar_key(self,file_name:str)->Key:bucket=get_bucket(self.connection,settings.S3_AVATAR_BUCKET)key=bucket.get_key(file_name)returnkey","defget_avatar_key(self,file_name:str)->ServiceResource:#Seehttps://github.com/python/typeshed/issues/2706#forwhythisreturntypeisa`ServiceResource`.bucket=get_bucket(self.session,settings.S3_AVATAR_BUCKET)key=bucket.Object(file_name)returnkey",zerver/lib/upload.py
zulip/zulip,cea7d713cdb9297cd9856ee4fbbcb79747872ae8,boto3,boto,"        def percent_callback(complete: Any, total: Any) -> None:
            sys.stdout.write('.')
            sys.stdout.flush()

        conn = S3Connection(settings.S3_KEY, settings.S3_SECRET_KEY)
        # We use the avatar bucket, because it's world-readable.
        bucket = get_bucket(conn, settings.S3_AVATAR_BUCKET)
        key = Key(bucket)
        key.key = os.path.join(""exports"", generate_random_token(32), os.path.basename(tarball_path))
        key.set_contents_from_filename(tarball_path, cb=percent_callback, num_cb=40)

        public_url = 'https://{bucket}.{host}/{key}'.format(
            host=conn.server_name(),
            bucket=bucket.name,
            key=key.key)








        return public_url","def percent_callback(bytes_transferred: Any) -> None:
            sys.stdout.write('.')
            sys.stdout.flush()

        session = boto3.Session(settings.S3_KEY, settings.S3_SECRET_KEY)
        # We use the avatar bucket, because it's world-readable.
        bucket = get_bucket(session, settings.S3_AVATAR_BUCKET)
        key = bucket.Object(os.path.join(""exports"", generate_random_token(32),
                            os.path.basename(tarball_path)))

        key.upload_file(tarball_path, Callback=percent_callback)

        session = botocore.session.get_session()
        config = Config(signature_version=botocore.UNSIGNED)

        public_url = session.create_client('s3', config=config).generate_presigned_url(
            'get_object',
            Params={
                'Bucket': bucket.name,
                'Key': key.key
            },
            ExpiresIn=0
        )
        return public_url",update,"defpercent_callback(complete:Any,total:Any)->None:sys.stdout.write('.')sys.stdout.flush()conn=S3Connection(settings.S3_KEY,settings.S3_SECRET_KEY)#Weusetheavatarbucket,becauseit'sworld-readable.bucket=get_bucket(conn,settings.S3_AVATAR_BUCKET)key=Key(bucket)key.key=os.path.join(""exports"",generate_random_token(32),os.path.basename(tarball_path))key.set_contents_from_filename(tarball_path,cb=percent_callback,num_cb=40)public_url='https://{bucket}.{host}/{key}'.format(host=conn.server_name(),bucket=bucket.name,key=key.key)returnpublic_url","defpercent_callback(bytes_transferred:Any)->None:sys.stdout.write('.')sys.stdout.flush()session=boto3.Session(settings.S3_KEY,settings.S3_SECRET_KEY)#Weusetheavatarbucket,becauseit'sworld-readable.bucket=get_bucket(session,settings.S3_AVATAR_BUCKET)key=bucket.Object(os.path.join(""exports"",generate_random_token(32),os.path.basename(tarball_path)))key.upload_file(tarball_path,Callback=percent_callback)session=botocore.session.get_session()config=Config(signature_version=botocore.UNSIGNED)public_url=session.create_client('s3',config=config).generate_presigned_url('get_object',Params={'Bucket':bucket.name,'Key':key.key},ExpiresIn=0)returnpublic_url",zerver/lib/upload.py
zulip/zulip,cea7d713cdb9297cd9856ee4fbbcb79747872ae8,boto3,boto,"class S3Uploader(Uploader):
    def __init__(self) -> None:
        super().__init__()
        conn = S3Connection(settings.S3_KEY, settings.S3_SECRET_KEY)
        self.bucket_name = settings.S3_AVATAR_BUCKET
        self.bucket = conn.get_bucket(self.bucket_name, validate=False)

    def copy_files(self, src_key: str, dst_key: str) -> None:
        self.bucket.copy_key(dst_key, self.bucket_name, src_key)","class S3Uploader(Uploader):
    def __init__(self) -> None:
        super().__init__()
        session = boto3.Session(settings.S3_KEY, settings.S3_SECRET_KEY)
        self.bucket_name = settings.S3_AVATAR_BUCKET
        self.bucket = session.resource('s3').Bucket(self.bucket_name)

    def copy_files(self, src_key: str, dst_key: str) -> None:
        source = dict(Bucket=self.bucket_name, Key=src_key)
        self.bucket.copy(source, dst_key)",update,"classS3Uploader(Uploader):def__init__(self)->None:super().__init__()conn=S3Connection(settings.S3_KEY,settings.S3_SECRET_KEY)self.bucket_name=settings.S3_AVATAR_BUCKETself.bucket=conn.get_bucket(self.bucket_name,validate=False)defcopy_files(self,src_key:str,dst_key:str)->None:self.bucket.copy_key(dst_key,self.bucket_name,src_key)","classS3Uploader(Uploader):def__init__(self)->None:super().__init__()session=boto3.Session(settings.S3_KEY,settings.S3_SECRET_KEY)self.bucket_name=settings.S3_AVATAR_BUCKETself.bucket=session.resource('s3').Bucket(self.bucket_name)defcopy_files(self,src_key:str,dst_key:str)->None:source=dict(Bucket=self.bucket_name,Key=src_key)self.bucket.copy(source,dst_key)",zerver/migrations/0149_realm_emoji_drop_unique_constraint.py
lsst-uk/csd3-echo-somerville,3d359e7738404bec83660b42b11a7be1c64cbc25,boto3,boto,"def print_buckets(conn):
	for bucket in conn.get_all_buckets():
        	print(""{name}\t{created}"".format(name = bucket.name,created = bucket.creation_date))","def print_buckets(client):
	response = client.list_buckets()
	for bucket in response['Buckets']:
        	print(f""{bucket['Name']}\t{bucket['CreationDate']}"")",update,"defprint_buckets(conn):forbucketinconn.get_all_buckets():print(""{name}\t{created}"".format(name=bucket.name,created=bucket.creation_date))","defprint_buckets(client):response=client.list_buckets()forbucketinresponse['Buckets']:print(f""{bucket['Name']}\t{bucket['CreationDate']}"")",csd3-side/bucket_manager.py
lsst-uk/csd3-echo-somerville,3d359e7738404bec83660b42b11a7be1c64cbc25,boto3,boto,"def get_conn(access_key, secret_key, host):
	return boto.connect_s3(
		aws_access_key_id = access_key,
		aws_secret_access_key = secret_key,
		host = host,
		calling_format = boto.s3.connection.OrdinaryCallingFormat(),
	)","def get_client(access_key, secret_key, host):
    session = boto3.Session(
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key
    )
    return session.client(
        service_name='s3',
        endpoint_url=f'https://{host}',
        verify=False  # Disable SSL verification for non-AWS S3 endpoints
    )",update,"defget_conn(access_key,secret_key,host):returnboto.connect_s3(aws_access_key_id=access_key,aws_secret_access_key=secret_key,host=host,calling_format=boto.s3.connection.OrdinaryCallingFormat(),)","defget_client(access_key,secret_key,host):session=boto3.Session(aws_access_key_id=access_key,aws_secret_access_key=secret_key)returnsession.client(service_name='s3',endpoint_url=f'https://{host}',verify=False#DisableSSLverificationfornon-AWSS3endpoints)",csd3-side/bucket_manager.py
botify-labs/simpleflow,c3c615ff4216028fbc4b9882f7b9918297b8878b,boto3,boto,"def disable_boto_connection_pooling():
    # boto connection pooling doesn't work very well with multiprocessing, it
    # provokes some errors like this:
    #
    #    [Errno 1] _ssl.c:1429: error:1408F119:SSL routines:SSL3_GET_RECORD:decryption failed or bad record mac
    # when polling on analysis-testjbb-repair-a61ff96e854344748e308fefc9ddff61
    #
    # It's because when forking, file handles are copied and sockets are shared.
    # Even sockets that handle SSL conections to AWS services, but SSL
    # connections are stateful! So with multiple workers, it collides.
    #
    # To disable boto's connection pooling (which in practice makes boto open a
    # *NEW* connection for each call), we make make boto believe we run on
    # Google App Engine, where it disables connection pooling. There's no
    # ""direct"" setting, so that's a hack but that works.
    boto.connection.ON_APP_ENGINE = True",,delete,"defdisable_boto_connection_pooling():#botoconnectionpoolingdoesn'tworkverywellwithmultiprocessing,it#provokessomeerrorslikethis:##[Errno1]_ssl.c:1429:error:1408F119:SSLroutines:SSL3_GET_RECORD:decryptionfailedorbadrecordmac#whenpollingonanalysis-testjbb-repair-a61ff96e854344748e308fefc9ddff61##It'sbecausewhenforking,filehandlesarecopiedandsocketsareshared.#EvensocketsthathandleSSLconectionstoAWSservices,butSSL#connectionsarestateful!Sowithmultipleworkers,itcollides.##Todisableboto'sconnectionpooling(whichinpracticemakesbotoopena#*NEW*connectionforeachcall),wemakemakebotobelievewerunon#GoogleAppEngine,whereitdisablesconnectionpooling.There'sno#""direct""setting,sothat'sahackbutthatworks.boto.connection.ON_APP_ENGINE=True",,simpleflow/command.py
botify-labs/simpleflow,c3c615ff4216028fbc4b9882f7b9918297b8878b,boto3,boto,"    from boto.s3.bucket import Bucket  # NOQA
    from boto.s3.bucketlistresultset import BucketListResultSet  # NOQA","    from mypy_boto3_s3.service_resource import Bucket, ObjectSummary  # NOQA",update,fromboto.s3.bucketimportBucket#NOQAfromboto.s3.bucketlistresultsetimportBucketListResultSet#NOQA,"frommypy_boto3_s3.service_resourceimportBucket,ObjectSummary#NOQA",simpleflow/storage.py
botify-labs/simpleflow,c3c615ff4216028fbc4b9882f7b9918297b8878b,boto3,boto,"def get_connection(host_or_region: str) -> connection.S3Connection:




    # first case: we got a valid DNS (host)
    if ""."" in host_or_region:
        return connection.S3Connection(host=host_or_region)

    # second case: we got a region
    return connect_to_region(host_or_region)","def get_client() -> boto3.session.Session.client:
    return boto3.session.Session().client(""s3"")


def get_resource(host_or_region: str) -> boto3.session.Session.resource:
    # first case: we got a valid DNS (host)
    if ""."" in host_or_region:
        return boto3.resource(""s3"", endpoint_url=f""https://{host_or_region}"")

    # second case: we got a region
    return boto3.resource(""s3"", region_name=host_or_region)",update,"defget_connection(host_or_region:str)->connection.S3Connection:#firstcase:wegotavalidDNS(host)if"".""inhost_or_region:returnconnection.S3Connection(host=host_or_region)#secondcase:wegotaregionreturnconnect_to_region(host_or_region)","defget_client()->boto3.session.Session.client:returnboto3.session.Session().client(""s3"")defget_resource(host_or_region:str)->boto3.session.Session.resource:#firstcase:wegotavalidDNS(host)if"".""inhost_or_region:returnboto3.resource(""s3"",endpoint_url=f""https://{host_or_region}"")#secondcase:wegotaregionreturnboto3.resource(""s3"",region_name=host_or_region)",simpleflow/storage.py
botify-labs/simpleflow,c3c615ff4216028fbc4b9882f7b9918297b8878b,boto3,boto," try:
        conn0 = connection.S3Connection()
        bucket_obj = conn0.get_bucket(bucket, validate=False)

        # get_location() returns a region or an empty string for us-east-1,
        # historically named ""US Standard"" in some places. Maybe other S3
        # calls support an empty string as region, but I prefer to be
        # explicit here.
        location = bucket_obj.get_location() or ""us-east-1""

        # save location for later use
        BUCKET_LOCATIONS_CACHE[bucket] = location
    except S3ResponseError as e:
        if e.error_code == ""AccessDenied"":

            # probably not allowed to perform GetBucketLocation on this bucket

            logger.warning(f""Access denied while trying to get location of bucket {bucket}"")
            location = """"","try:
        # get_bucket_location() returns a region or an empty string for us-east-1,
        # historically named ""US Standard"" in some places. Maybe other S3 calls
        # support an empty string as region, but I prefer to be explicit here.
        location = get_client().get_bucket_location(Bucket=bucket)[""LocationConstraint""] or ""us-east-1""





        # save location for later use
        BUCKET_LOCATIONS_CACHE[bucket] = location
    except ClientError as e:
        error_code = extract_error_code(e)
        if error_code == ""AccessDenied"":
            # probably not allowed to perform GetBucketLocation on this bucket
            # TODO: consider raising instead? who forbids GetBucketLocation anyway?
            logger.warning(f""Access denied while trying to get location of bucket {bucket}"")
            location = """"",update,"try:conn0=connection.S3Connection()bucket_obj=conn0.get_bucket(bucket,validate=False)#get_location()returnsaregionoranemptystringforus-east-1,#historicallynamed""USStandard""insomeplaces.MaybeotherS3#callssupportanemptystringasregion,butIprefertobe#explicithere.location=bucket_obj.get_location()or""us-east-1""#savelocationforlateruseBUCKET_LOCATIONS_CACHE[bucket]=locationexceptS3ResponseErrorase:ife.error_code==""AccessDenied"":#probablynotallowedtoperformGetBucketLocationonthisbucketlogger.warning(f""Accessdeniedwhiletryingtogetlocationofbucket{bucket}"")location=""""","try:#get_bucket_location()returnsaregionoranemptystringforus-east-1,#historicallynamed""USStandard""insomeplaces.MaybeotherS3calls#supportanemptystringasregion,butIprefertobeexplicithere.location=get_client().get_bucket_location(Bucket=bucket)[""LocationConstraint""]or""us-east-1""#savelocationforlateruseBUCKET_LOCATIONS_CACHE[bucket]=locationexceptClientErrorase:error_code=extract_error_code(e)iferror_code==""AccessDenied"":#probablynotallowedtoperformGetBucketLocationonthisbucket#TODO:considerraisinginstead?whoforbidsGetBucketLocationanyway?logger.warning(f""Accessdeniedwhiletryingtogetlocationofbucket{bucket}"")location=""""",simpleflow/storage.py
botify-labs/simpleflow,c3c615ff4216028fbc4b9882f7b9918297b8878b,boto3,boto,"def get_bucket(bucket_name: str) -> Bucket:
    bucket_name, location = sanitize_bucket_and_host(bucket_name)
    conn = get_connection(location)
    if bucket_name not in BUCKET_CACHE:
        bucket = conn.get_bucket(bucket_name, validate=False)
        BUCKET_CACHE[bucket_name] = bucket
    return BUCKET_CACHE[bucket_name]","
def get_bucket(bucket_name: str) -> ""Bucket"":
    bucket_name, location = sanitize_bucket_and_host(bucket_name)
    s3 = get_resource(location)
    if bucket_name not in BUCKET_CACHE:
        bucket = s3.Bucket(bucket_name)
        BUCKET_CACHE[bucket_name] = bucket
    return BUCKET_CACHE[bucket_name]",update,"defget_bucket(bucket_name:str)->Bucket:bucket_name,location=sanitize_bucket_and_host(bucket_name)conn=get_connection(location)ifbucket_namenotinBUCKET_CACHE:bucket=conn.get_bucket(bucket_name,validate=False)BUCKET_CACHE[bucket_name]=bucketreturnBUCKET_CACHE[bucket_name]","defget_bucket(bucket_name:str)->""Bucket"":bucket_name,location=sanitize_bucket_and_host(bucket_name)s3=get_resource(location)ifbucket_namenotinBUCKET_CACHE:bucket=s3.Bucket(bucket_name)BUCKET_CACHE[bucket_name]=bucketreturnBUCKET_CACHE[bucket_name]",simpleflow/storage.py
botify-labs/simpleflow,c3c615ff4216028fbc4b9882f7b9918297b8878b,boto3,boto,"def pull(bucket: str, path: str, dest_file: str) -> None:
    bucket = get_bucket(bucket)
    key = bucket.get_key(path)
    key.get_contents_to_filename(dest_file)


def pull_content(bucket: str, path: str) -> str:
    bucket = get_bucket(bucket)
    key = bucket.get_key(path)
    return key.get_contents_as_string(encoding=""utf-8"")



def push(bucket: str, path: str, src_file: str, content_type: str | None = None) -> None:
    bucket = get_bucket(bucket)
    key = Key(bucket, path)
    headers = {}
    if content_type:
        headers[""content_type""] = content_type
    key.set_contents_from_filename(src_file, headers=headers, encrypt_key=settings.SIMPLEFLOW_S3_SSE)




def push_content(bucket: str, path: str, content: str, content_type: str | None = None) -> None:
    bucket = get_bucket(bucket)
    key = Key(bucket, path)
    headers = {}
    if content_type:
        headers[""content_type""] = content_type
    key.set_contents_from_string(content, headers=headers, encrypt_key=settings.SIMPLEFLOW_S3_SSE)




def list_keys(bucket: str, path: str = None) -> BucketListResultSet:
    bucket = get_bucket(bucket)
    return bucket.list(path)","def pull(bucket: str, path: str, dest_file: str) -> None:
    bucket_resource = get_bucket(bucket)
    bucket_resource.download_file(path, dest_file)



def pull_content(bucket: str, path: str) -> str:
    bucket_resource = get_bucket(bucket)
    bytes_buffer = io.BytesIO()
    bucket_resource.download_fileobj(path, bytes_buffer)
    return bytes_buffer.getvalue().decode()


def push(bucket: str, path: str, src_file: str, content_type: str | None = None) -> None:
    bucket_resource = get_bucket(bucket)
    extra_args = {}

    if content_type:
        extra_args[""ContentType""] = content_type
    if settings.SIMPLEFLOW_S3_SSE:
        extra_args[""ServerSideEncryption""] = ""AES256""
    bucket_resource.upload_file(src_file, path, ExtraArgs=extra_args)


def push_content(bucket: str, path: str, content: str, content_type: str | None = None) -> None:
    bucket_resource = get_bucket(bucket)
    extra_args = {}

    if content_type:
        extra_args[""ContentType""] = content_type
    if settings.SIMPLEFLOW_S3_SSE:
        extra_args[""ServerSideEncryption""] = ""AES256""
    bucket_resource.upload_fileobj(io.BytesIO(content.encode()), path, ExtraArgs=extra_args)


def list_keys(bucket: str, path: str = None) -> list[""ObjectSummary""]:
    bucket_resource = get_bucket(bucket)
    return [obj for obj in bucket_resource.objects.filter(Prefix=path or """").all()]",update,"defpull(bucket:str,path:str,dest_file:str)->None:bucket=get_bucket(bucket)key=bucket.get_key(path)key.get_contents_to_filename(dest_file)defpull_content(bucket:str,path:str)->str:bucket=get_bucket(bucket)key=bucket.get_key(path)returnkey.get_contents_as_string(encoding=""utf-8"")defpush(bucket:str,path:str,src_file:str,content_type:str|None=None)->None:bucket=get_bucket(bucket)key=Key(bucket,path)headers={}ifcontent_type:headers[""content_type""]=content_typekey.set_contents_from_filename(src_file,headers=headers,encrypt_key=settings.SIMPLEFLOW_S3_SSE)defpush_content(bucket:str,path:str,content:str,content_type:str|None=None)->None:bucket=get_bucket(bucket)key=Key(bucket,path)headers={}ifcontent_type:headers[""content_type""]=content_typekey.set_contents_from_string(content,headers=headers,encrypt_key=settings.SIMPLEFLOW_S3_SSE)deflist_keys(bucket:str,path:str=None)->BucketListResultSet:bucket=get_bucket(bucket)returnbucket.list(path)","defpull(bucket:str,path:str,dest_file:str)->None:bucket_resource=get_bucket(bucket)bucket_resource.download_file(path,dest_file)defpull_content(bucket:str,path:str)->str:bucket_resource=get_bucket(bucket)bytes_buffer=io.BytesIO()bucket_resource.download_fileobj(path,bytes_buffer)returnbytes_buffer.getvalue().decode()defpush(bucket:str,path:str,src_file:str,content_type:str|None=None)->None:bucket_resource=get_bucket(bucket)extra_args={}ifcontent_type:extra_args[""ContentType""]=content_typeifsettings.SIMPLEFLOW_S3_SSE:extra_args[""ServerSideEncryption""]=""AES256""bucket_resource.upload_file(src_file,path,ExtraArgs=extra_args)defpush_content(bucket:str,path:str,content:str,content_type:str|None=None)->None:bucket_resource=get_bucket(bucket)extra_args={}ifcontent_type:extra_args[""ContentType""]=content_typeifsettings.SIMPLEFLOW_S3_SSE:extra_args[""ServerSideEncryption""]=""AES256""bucket_resource.upload_fileobj(io.BytesIO(content.encode()),path,ExtraArgs=extra_args)deflist_keys(bucket:str,path:str=None)->list[""ObjectSummary""]:bucket_resource=get_bucket(bucket)return[objforobjinbucket_resource.objects.filter(Prefix=pathor"""").all()]",simpleflow/storage.py
botify-labs/simpleflow,c3c615ff4216028fbc4b9882f7b9918297b8878b,boto3,boto,from boto.swf.exceptions import SWFResponseError  # noqa,from botocore.exceptions import ClientError,update,fromboto.swf.exceptionsimportSWFResponseError#noqa,frombotocore.exceptionsimportClientError,simpleflow/storage.py
botify-labs/simpleflow,c3c615ff4216028fbc4b9882f7b9918297b8878b,boto3,boto," try:
            response = self.connection.describe_workflow_type(self.domain.name, name, version)
        except SWFResponseError as e:
            if e.error_code == ""UnknownResourceFault"":
                raise DoesNotExistError(e.body[""message""])



            raise ResponseError(e.body[""message""])","try:
            response = self.describe_workflow_type(self.domain.name, name, version)
        except ClientError as e:
            error_code = extract_error_code(e)
            message = extract_message(e)
            if error_code == ""UnknownResourceFault"":
                raise DoesNotExistError(message)

            raise ResponseError(message)",update,"try:response=self.connection.describe_workflow_type(self.domain.name,name,version)exceptSWFResponseErrorase:ife.error_code==""UnknownResourceFault"":raiseDoesNotExistError(e.body[""message""])raiseResponseError(e.body[""message""])","try:response=self.describe_workflow_type(self.domain.name,name,version)exceptClientErrorase:error_code=extract_error_code(e)message=extract_message(e)iferror_code==""UnknownResourceFault"":raiseDoesNotExistError(message)raiseResponseError(message)",simpleflow/swf/mapper/querysets/workflow.py
botify-labs/simpleflow,e5cdf6aea67ee7619384b7cdb845836467c6a866,boto3,boto,"from boto.exception import S3ResponseError
from boto.s3 import connect_to_region, connection
from boto.s3.key import Key","import boto3
from botocore.exceptions import ClientError",update,"fromboto.exceptionimportS3ResponseErrorfromboto.s3importconnect_to_region,connectionfromboto.s3.keyimportKey",importboto3frombotocore.exceptionsimportClientError,simpleflow/storage.py
botify-labs/simpleflow,e5cdf6aea67ee7619384b7cdb845836467c6a866,boto3,boto,"from boto.s3.bucket import Bucket  # NOQA
    from boto.s3.bucketlistresultset import BucketListResultSet","    from mypy_boto3_s3.service_resource import Bucket, ObjectSummary  # NOQA",update,fromboto.s3.bucketimportBucket#NOQAfromboto.s3.bucketlistresultsetimportBucketListResultSet,"frommypy_boto3_s3.service_resourceimportBucket,ObjectSummary#NOQA",simpleflow/storage.py
botify-labs/simpleflow,e5cdf6aea67ee7619384b7cdb845836467c6a866,boto3,boto,"def get_connection(host_or_region: str) -> connection.S3Connection:




    # first case: we got a valid DNS (host)
    if ""."" in host_or_region:
        return connection.S3Connection(host=host_or_region)

    # second case: we got a region
    return connect_to_region(host_or_region)","def get_resource(host_or_region: str) -> boto3.session.Session.resource:
    # first case: we got a valid DNS (host)
    if ""."" in host_or_region:
        return boto3.resource(""s3"", endpoint_url=f""https://{host_or_region}"")

    # second case: we got a region
    return boto3.resource(""s3"", region_name=host_or_region)",update,"defget_connection(host_or_region:str)->connection.S3Connection:#firstcase:wegotavalidDNS(host)if"".""inhost_or_region:returnconnection.S3Connection(host=host_or_region)#secondcase:wegotaregionreturnconnect_to_region(host_or_region)","defget_resource(host_or_region:str)->boto3.session.Session.resource:#firstcase:wegotavalidDNS(host)if"".""inhost_or_region:returnboto3.resource(""s3"",endpoint_url=f""https://{host_or_region}"")#secondcase:wegotaregionreturnboto3.resource(""s3"",region_name=host_or_region)",simpleflow/storage.py
botify-labs/simpleflow,e5cdf6aea67ee7619384b7cdb845836467c6a866,boto3,boto,,"def get_client() -> boto3.session.Session.client:
    return boto3.session.Session().client(""s3"")",insert,,"defget_client()->boto3.session.Session.client:returnboto3.session.Session().client(""s3"")",simpleflow/storage.py
botify-labs/simpleflow,e5cdf6aea67ee7619384b7cdb845836467c6a866,boto3,boto,"try:
        conn0 = connection.S3Connection()
        bucket_obj = conn0.get_bucket(bucket, validate=False)

        # get_location() returns a region or an empty string for us-east-1,
        # historically named ""US Standard"" in some places. Maybe other S3
        # calls support an empty string as region, but I prefer to be
        # explicit here.
        location = bucket_obj.get_location() or ""us-east-1""

        # save location for later use
        BUCKET_LOCATIONS_CACHE[bucket] = location
    except S3ResponseError as e:
        if e.error_code == ""AccessDenied"":

            # probably not allowed to perform GetBucketLocation on this bucket

            logger.warning(f""Access denied while trying to get location of bucket {bucket}"")
            location = """"","    try:
        # get_bucket_location() returns a region or an empty string for us-east-1,
        # historically named ""US Standard"" in some places. Maybe other S3 calls
        # support an empty string as region, but I prefer to be explicit here.
        location = get_client().get_bucket_location(Bucket=bucket)[""LocationConstraint""] or ""us-east-1""





        # save location for later use
        BUCKET_LOCATIONS_CACHE[bucket] = location
    except ClientError as e:
        error_code = extract_error_code(e)
        if error_code == ""AccessDenied"":
            # probably not allowed to perform GetBucketLocation on this bucket
            # TODO: consider raising instead? who forbids GetBucketLocation anyway?
            logger.warning(f""Access denied while trying to get location of bucket {bucket}"")
            location = """"",update,"try:conn0=connection.S3Connection()bucket_obj=conn0.get_bucket(bucket,validate=False)#get_location()returnsaregionoranemptystringforus-east-1,#historicallynamed""USStandard""insomeplaces.MaybeotherS3#callssupportanemptystringasregion,butIprefertobe#explicithere.location=bucket_obj.get_location()or""us-east-1""#savelocationforlateruseBUCKET_LOCATIONS_CACHE[bucket]=locationexceptS3ResponseErrorase:ife.error_code==""AccessDenied"":#probablynotallowedtoperformGetBucketLocationonthisbucketlogger.warning(f""Accessdeniedwhiletryingtogetlocationofbucket{bucket}"")location=""""","try:#get_bucket_location()returnsaregionoranemptystringforus-east-1,#historicallynamed""USStandard""insomeplaces.MaybeotherS3calls#supportanemptystringasregion,butIprefertobeexplicithere.location=get_client().get_bucket_location(Bucket=bucket)[""LocationConstraint""]or""us-east-1""#savelocationforlateruseBUCKET_LOCATIONS_CACHE[bucket]=locationexceptClientErrorase:error_code=extract_error_code(e)iferror_code==""AccessDenied"":#probablynotallowedtoperformGetBucketLocationonthisbucket#TODO:considerraisinginstead?whoforbidsGetBucketLocationanyway?logger.warning(f""Accessdeniedwhiletryingtogetlocationofbucket{bucket}"")location=""""",simpleflow/storage.py
botify-labs/simpleflow,e5cdf6aea67ee7619384b7cdb845836467c6a866,boto3,boto,"def get_bucket(bucket_name: str) -> Bucket:
    bucket_name, location = sanitize_bucket_and_host(bucket_name)
    conn = get_connection(location)
    if bucket_name not in BUCKET_CACHE:
        bucket = conn.get_bucket(bucket_name, validate=False)
        BUCKET_CACHE[bucket_name] = bucket
    return BUCKET_CACHE[bucket_name]


def pull(bucket: str, path: str, dest_file: str) -> None:
    bucket = get_bucket(bucket)
    key = bucket.get_key(path)
    key.get_contents_to_filename(dest_file)


def pull_content(bucket: str, path: str) -> str:
    bucket = get_bucket(bucket)
    key = bucket.get_key(path)
    return key.get_contents_as_string(encoding=""utf-8"")



def push(bucket: str, path: str, src_file: str, content_type: str | None = None) -> None:
    bucket = get_bucket(bucket)
    key = Key(bucket, path)
    headers = {}
    if content_type:
        headers[""content_type""] = content_type
    key.set_contents_from_filename(src_file, headers=headers, encrypt_key=settings.SIMPLEFLOW_S3_SSE)




def push_content(bucket: str, path: str, content: str, content_type: str | None = None) -> None:
    bucket = get_bucket(bucket)
    key = Key(bucket, path)
    headers = {}
    if content_type:
        headers[""content_type""] = content_type
    key.set_contents_from_string(content, headers=headers, encrypt_key=settings.SIMPLEFLOW_S3_SSE)




def list_keys(bucket: str, path: str = None) -> BucketListResultSet:
    bucket = get_bucket(bucket)
    return bucket.list(path)","def get_bucket(bucket_name: str) -> ""Bucket"":
    bucket_name, location = sanitize_bucket_and_host(bucket_name)
    s3 = get_resource(location)
    if bucket_name not in BUCKET_CACHE:
        bucket = s3.Bucket(bucket_name)
        BUCKET_CACHE[bucket_name] = bucket
    return BUCKET_CACHE[bucket_name]


def pull(bucket: str, path: str, dest_file: str) -> None:
    bucket_resource = get_bucket(bucket)
    bucket_resource.download_file(path, dest_file)



def pull_content(bucket: str, path: str) -> str:
    bucket_resource = get_bucket(bucket)
    bytes_buffer = io.BytesIO()
    bucket_resource.download_fileobj(path, bytes_buffer)
    return bytes_buffer.getvalue().decode()


def push(bucket: str, path: str, src_file: str, content_type: str | None = None) -> None:
    bucket_resource = get_bucket(bucket)
    extra_args = {}

    if content_type:
        extra_args[""ContentType""] = content_type
    if settings.SIMPLEFLOW_S3_SSE:
        extra_args[""ServerSideEncryption""] = ""AES256""
    bucket_resource.upload_file(src_file, path, ExtraArgs=extra_args)


def push_content(bucket: str, path: str, content: str, content_type: str | None = None) -> None:
    bucket_resource = get_bucket(bucket)
    extra_args = {}

    if content_type:
        extra_args[""ContentType""] = content_type
    if settings.SIMPLEFLOW_S3_SSE:
        extra_args[""ServerSideEncryption""] = ""AES256""
    bucket_resource.upload_fileobj(io.BytesIO(content.encode()), path, ExtraArgs=extra_args)


def list_keys(bucket: str, path: str = None) -> list[""ObjectSummary""]:
    bucket_resource = get_bucket(bucket)
    return [obj for obj in bucket_resource.objects.filter(Prefix=path or """").all()]",update,"defget_bucket(bucket_name:str)->Bucket:bucket_name,location=sanitize_bucket_and_host(bucket_name)conn=get_connection(location)ifbucket_namenotinBUCKET_CACHE:bucket=conn.get_bucket(bucket_name,validate=False)BUCKET_CACHE[bucket_name]=bucketreturnBUCKET_CACHE[bucket_name]defpull(bucket:str,path:str,dest_file:str)->None:bucket=get_bucket(bucket)key=bucket.get_key(path)key.get_contents_to_filename(dest_file)defpull_content(bucket:str,path:str)->str:bucket=get_bucket(bucket)key=bucket.get_key(path)returnkey.get_contents_as_string(encoding=""utf-8"")defpush(bucket:str,path:str,src_file:str,content_type:str|None=None)->None:bucket=get_bucket(bucket)key=Key(bucket,path)headers={}ifcontent_type:headers[""content_type""]=content_typekey.set_contents_from_filename(src_file,headers=headers,encrypt_key=settings.SIMPLEFLOW_S3_SSE)defpush_content(bucket:str,path:str,content:str,content_type:str|None=None)->None:bucket=get_bucket(bucket)key=Key(bucket,path)headers={}ifcontent_type:headers[""content_type""]=content_typekey.set_contents_from_string(content,headers=headers,encrypt_key=settings.SIMPLEFLOW_S3_SSE)deflist_keys(bucket:str,path:str=None)->BucketListResultSet:bucket=get_bucket(bucket)returnbucket.list(path)","defget_bucket(bucket_name:str)->""Bucket"":bucket_name,location=sanitize_bucket_and_host(bucket_name)s3=get_resource(location)ifbucket_namenotinBUCKET_CACHE:bucket=s3.Bucket(bucket_name)BUCKET_CACHE[bucket_name]=bucketreturnBUCKET_CACHE[bucket_name]defpull(bucket:str,path:str,dest_file:str)->None:bucket_resource=get_bucket(bucket)bucket_resource.download_file(path,dest_file)defpull_content(bucket:str,path:str)->str:bucket_resource=get_bucket(bucket)bytes_buffer=io.BytesIO()bucket_resource.download_fileobj(path,bytes_buffer)returnbytes_buffer.getvalue().decode()defpush(bucket:str,path:str,src_file:str,content_type:str|None=None)->None:bucket_resource=get_bucket(bucket)extra_args={}ifcontent_type:extra_args[""ContentType""]=content_typeifsettings.SIMPLEFLOW_S3_SSE:extra_args[""ServerSideEncryption""]=""AES256""bucket_resource.upload_file(src_file,path,ExtraArgs=extra_args)defpush_content(bucket:str,path:str,content:str,content_type:str|None=None)->None:bucket_resource=get_bucket(bucket)extra_args={}ifcontent_type:extra_args[""ContentType""]=content_typeifsettings.SIMPLEFLOW_S3_SSE:extra_args[""ServerSideEncryption""]=""AES256""bucket_resource.upload_fileobj(io.BytesIO(content.encode()),path,ExtraArgs=extra_args)deflist_keys(bucket:str,path:str=None)->list[""ObjectSummary""]:bucket_resource=get_bucket(bucket)return[objforobjinbucket_resource.objects.filter(Prefix=pathor"""").all()]",simpleflow/storage.py
elifesciences/elife-dashboard,d861d36fec941479d37b4d86777d0b4ac1100b7f,boto3,boto,"def get_queue():
    conn = boto.sqs.connect_to_region(settings.sqs_region,
                                      aws_access_key_id=settings.aws_access_key_id,
                                      aws_secret_access_key=settings.aws_secret_access_key)
    queue = conn.get_queue(settings.event_monitor_queue)


    assert queue is not None, ""failed to find %r in region %r"" % (settings.event_monitor_queue, settings.sqs_region)
    return queue","def get_queue():
    conn = boto3.resource('sqs',
                          settings.sqs_region,
                          aws_access_key_id=settings.aws_access_key_id,
                          aws_secret_access_key=settings.aws_secret_access_key)
    queue = conn.get_queue_by_name(settings.event_monitor_queue)
    # lsh@2023-03-03: is this still true in boto3?
    assert queue is not None, ""failed to find %r in region %r"" % (settings.event_monitor_queue, settings.sqs_region)
    return queue",update,"defget_queue():conn=boto.sqs.connect_to_region(settings.sqs_region,aws_access_key_id=settings.aws_access_key_id,aws_secret_access_key=settings.aws_secret_access_key)queue=conn.get_queue(settings.event_monitor_queue)assertqueueisnotNone,""failedtofind%rinregion%r""%(settings.event_monitor_queue,settings.sqs_region)returnqueue","defget_queue():conn=boto3.resource('sqs',settings.sqs_region,aws_access_key_id=settings.aws_access_key_id,aws_secret_access_key=settings.aws_secret_access_key)queue=conn.get_queue_by_name(settings.event_monitor_queue)#lsh@2023-03-03:isthisstilltrueinboto3?assertqueueisnotNone,""failedtofind%rinregion%r""%(settings.event_monitor_queue,settings.sqs_region)returnqueue",process_dashboard_queue.py
elifesciences/elife-dashboard,d861d36fec941479d37b4d86777d0b4ac1100b7f,boto3,boto,"def get_queue(self, queue_name):
        sqs_conn = boto.sqs.connect_to_region(settings.sqs_region)
        queue = sqs_conn.get_queue(queue_name)
        return queue","def get_queue(self, queue_name):
        sqs_conn = boto3.resource('sqs', region_name=region)
        queue = sqs_conn.get_queue_by_name(queue_name)
        return queue",update,"defget_queue(self,queue_name):sqs_conn=boto.sqs.connect_to_region(settings.sqs_region)queue=sqs_conn.get_queue(queue_name)returnqueue","defget_queue(self,queue_name):sqs_conn=boto3.resource('sqs',region_name=region)queue=sqs_conn.get_queue_by_name(queue_name)returnqueue",provider/QueueProvider.py
pewresearch/pewtils,857f0c027446a77fcb2235b443908e37851bd81c,boto3,boto,"from boto.s3.key import Key
from boto.s3.connection import S3Connection, OrdinaryCallingFormat",,delete,"fromboto.s3.keyimportKeyfromboto.s3.connectionimportS3Connection,OrdinaryCallingFormat",,
pewresearch/pewtils,857f0c027446a77fcb2235b443908e37851bd81c,boto3,boto,,import boto3,insert,,importboto3,pewtils/io.py
pewresearch/pewtils,857f0c027446a77fcb2235b443908e37851bd81c,boto3,boto,"if self.use_s3:

            s3_params = {}

            if aws_access is not None:
                s3_params[""aws_access_key_id""] = aws_access
                s3_params[""aws_secret_access_key""] = aws_secret

            if ""."" in bucket:
                s3_params[""calling_format""] = OrdinaryCallingFormat()

            self.s3 = S3Connection(**s3_params).get_bucket(bucket)","if self.use_s3:

            s3_params = {}
            self.s3 = boto3.client('s3')",update,"ifself.use_s3:s3_params={}ifaws_accessisnotNone:s3_params[""aws_access_key_id""]=aws_accesss3_params[""aws_secret_access_key""]=aws_secretif"".""inbucket:s3_params[""calling_format""]=OrdinaryCallingFormat()self.s3=S3Connection(**s3_params).get_bucket(bucket)",ifself.use_s3:s3_params={}self.s3=boto3.client('s3'),pewtils/io.py
pewresearch/pewtils,857f0c027446a77fcb2235b443908e37851bd81c,boto3,boto,"if self.use_s3:
            for key in self.s3.list(prefix=self.path):
                yield key","if self.use_s3:
            for key in self.s3.list_objects(Bucket=self.bucket, Prefix=self.path):
                yield key",update,ifself.use_s3:forkeyinself.s3.list(prefix=self.path):yieldkey,"ifself.use_s3:forkeyinself.s3.list_objects(Bucket=self.bucket,Prefix=self.path):yieldkey",pewtils/io.py
pewresearch/pewtils,857f0c027446a77fcb2235b443908e37851bd81c,boto3,boto," if self.use_s3:
            for key in self.s3.list(prefix=self.path):
                key.delete()","if self.use_s3:
            for key in self.s3.list_objects(Bucket=self.bucket, Prefix=self.path):
                key.delete()",update,ifself.use_s3:forkeyinself.s3.list(prefix=self.path):key.delete(),"ifself.use_s3:forkeyinself.s3.list_objects(Bucket=self.bucket,Prefix=self.path):key.delete()",pewtils/io.py
pewresearch/pewtils,857f0c027446a77fcb2235b443908e37851bd81c,boto3,boto,"key = self.s3.get_key(filepath)
            self.s3.delete_key(key.name)","key = self.s3.delete_object(Bucket=self.bucket, Key=filepath)",update,key=self.s3.get_key(filepath)self.s3.delete_key(key.name),"key=self.s3.delete_object(Bucket=self.bucket,Key=filepath)",pewtils/io.py
pewresearch/pewtils,857f0c027446a77fcb2235b443908e37851bd81c,boto3,boto," if self.use_s3:

            k = Key(self.s3)
            k.key = ""/"".join([self.path, key])
            k.set_contents_from_string(data)","if self.use_s3:
            self.s3.upload_fileobj(io.BytesIO(data), Bucket=self.bucket, Key=""/"".join([self.path, key]))",update,"ifself.use_s3:k=Key(self.s3)k.key=""/"".join([self.path,key])k.set_contents_from_string(data)","ifself.use_s3:self.s3.upload_fileobj(io.BytesIO(data),Bucket=self.bucket,Key=""/"".join([self.path,key]))",pewtils/io.py
pewresearch/pewtils,857f0c027446a77fcb2235b443908e37851bd81c,boto3,boto,"if self.use_s3:




            k = self.s3.get_key(filepath)
            if k:
                try:
                    data = k.get_contents_as_string()
                except ValueError:
                    pass
        else:","if self.use_s3:
            try:
                data = io.StringIO()
                self.s3.download_fileobj(data, Bucket=self.bucket, Key=filepath)

            except:
                data = io.BytesIO()
                self.s3.download_fileobj(data, Bucket=self.bucket, Key=filepath)",update,ifself.use_s3:k=self.s3.get_key(filepath)ifk:try:data=k.get_contents_as_string()exceptValueError:passelse:,"ifself.use_s3:try:data=io.StringIO()self.s3.download_fileobj(data,Bucket=self.bucket,Key=filepath)except:data=io.BytesIO()self.s3.download_fileobj(data,Bucket=self.bucket,Key=filepath)",pewtils/io.py
mlevental/sys-int-uebung-3,299b81e5af3a0b44a2a6a9c49d599e3689d74b35,boto3,boto,"import boto
from boto.s3.key import Key
from boto.exception import S3ResponseError","import boto3
from boto3 import *",update,importbotofromboto.s3.keyimportKeyfromboto.exceptionimportS3ResponseError,importboto3fromboto3import*,index.py
mlevental/sys-int-uebung-3,299b81e5af3a0b44a2a6a9c49d599e3689d74b35,boto3,boto," s3 = boto.connect_s3()
    try:
        bucket = s3.get_bucket(bucketname)
    except S3ResponseError:
        return ""bucket {bucketname} doesn't exist"".format(bucketname=bucketname)

    key = Key(bucket)
    key.key = 'mb'
    key.set_contents_from_filename('mb.png')
    key.get_contents_to_filename('mbFromS3.png')

    return template('index.html', mb='/mbFromS3.png', w=w, h=h, it=it)","s3 = boto3.resource('s3')
    bucket = s3.Bucket(bucketname)




    if not bucket in s3.buckets.all():
        bucket.create()
    bucket.upload_file('mb.png', IMG_NAME)


    imgUrl = s3.meta.client.generate_presigned_url('get_object', Params={'Bucket': bucketname, 'Key': IMG_NAME})
    return template('index.html', w=w, h=h, it=it, imgUrl=imgUrl)",update,"s3=boto.connect_s3()try:bucket=s3.get_bucket(bucketname)exceptS3ResponseError:return""bucket{bucketname}doesn'texist"".format(bucketname=bucketname)key=Key(bucket)key.key='mb'key.set_contents_from_filename('mb.png')key.get_contents_to_filename('mbFromS3.png')returntemplate('index.html',mb='/mbFromS3.png',w=w,h=h,it=it)","s3=boto3.resource('s3')bucket=s3.Bucket(bucketname)ifnotbucketins3.buckets.all():bucket.create()bucket.upload_file('mb.png',IMG_NAME)imgUrl=s3.meta.client.generate_presigned_url('get_object',Params={'Bucket':bucketname,'Key':IMG_NAME})returntemplate('index.html',w=w,h=h,it=it,imgUrl=imgUrl)",index.py
mlevental/sys-int-uebung-3,299b81e5af3a0b44a2a6a9c49d599e3689d74b35,boto3,boto,"def deleteFromS3(bucketname):
    s3 = boto.connect_s3()


    try:
        bucket = s3.get_bucket(bucketname)
    except S3ResponseError:
        return ""bucket {bucketname} doesn't exist"".format(bucketname=bucketname)

    for key in bucket.list():
        key.delete()

    bucket.delete()
    return ""deleted""","def deleteFromS3(bucketname):
    s3 = boto3.resource('s3')
    bucket = s3.Bucket(bucketname)

    if not bucket in s3.buckets.all():
        return 'Bucket ""{bucketname}"" existiert nicht!'.format(bucketname=bucketname)



    for key in bucket.objects.all():
        key.delete()

    bucket.delete()



    return ""Bucket und alle Objekte gel��scht!""",update,"defdeleteFromS3(bucketname):s3=boto.connect_s3()try:bucket=s3.get_bucket(bucketname)exceptS3ResponseError:return""bucket{bucketname}doesn'texist"".format(bucketname=bucketname)forkeyinbucket.list():key.delete()bucket.delete()return""deleted""","defdeleteFromS3(bucketname):s3=boto3.resource('s3')bucket=s3.Bucket(bucketname)ifnotbucketins3.buckets.all():return'Bucket""{bucketname}""existiertnicht!'.format(bucketname=bucketname)forkeyinbucket.objects.all():key.delete()bucket.delete()return""BucketundalleObjektegel��scht!""",index.py
mlevental/sys-int-uebung-3,299b81e5af3a0b44a2a6a9c49d599e3689d74b35,boto3,boto,"@route('/<mb>')
def show_mb(mb):
    return static_file(mb, root=""."")",,delete,"@route('/<mb>')defshow_mb(mb):returnstatic_file(mb,root=""."")",,
elifesciences/elife-dashboard,46228025d41e608851722c89e2bff7aa4fe4af68,boto3,boto,import boto.sqs,import boto3,update,importboto.sqs,importboto3,process_dashboard_queue.py
elifesciences/elife-dashboard,46228025d41e608851722c89e2bff7aa4fe4af68,boto3,boto,"def get_queue():
    conn = boto.sqs.connect_to_region(settings.sqs_region,
                                      aws_access_key_id=settings.aws_access_key_id,
                                      aws_secret_access_key=settings.aws_secret_access_key)
    queue = conn.get_queue(settings.event_monitor_queue)","def get_queue():
    conn = boto3.resource('sqs',
                          settings.sqs_region,
                          aws_access_key_id=settings.aws_access_key_id,
                          aws_secret_access_key=settings.aws_secret_access_key)
    queue = conn.get_queue_by_name(settings.event_monitor_queue)",update,"defget_queue():conn=boto.sqs.connect_to_region(settings.sqs_region,aws_access_key_id=settings.aws_access_key_id,aws_secret_access_key=settings.aws_secret_access_key)queue=conn.get_queue(settings.event_monitor_queue)","defget_queue():conn=boto3.resource('sqs',settings.sqs_region,aws_access_key_id=settings.aws_access_key_id,aws_secret_access_key=settings.aws_secret_access_key)queue=conn.get_queue_by_name(settings.event_monitor_queue)",process_dashboard_queue.py
elifesciences/elife-dashboard,46228025d41e608851722c89e2bff7aa4fe4af68,boto3,boto,"def get_queue(self, queue_name):
        sqs_conn = boto.sqs.connect_to_region(settings.sqs_region)
        queue = sqs_conn.get_queue(queue_name)
        return queue","def get_queue(self, queue_name):
        sqs_conn = boto3.resource('sqs', region_name=settings.sqs_region)
        try:
            queue = sqs_conn.get_queue_by_name(QueueName=queue_name)
            return queue
        except ClientError as error:
            LOG.error(""failed to fetch queue %r in region %r"", queue_name, settings.sqs_region)
            return None",update,"defget_queue(self,queue_name):sqs_conn=boto.sqs.connect_to_region(settings.sqs_region)queue=sqs_conn.get_queue(queue_name)returnqueue","defget_queue(self,queue_name):sqs_conn=boto3.resource('sqs',region_name=settings.sqs_region)try:queue=sqs_conn.get_queue_by_name(QueueName=queue_name)returnqueueexceptClientErroraserror:LOG.error(""failedtofetchqueue%rinregion%r"",queue_name,settings.sqs_region)returnNone",provider/QueueProvider.py
jcforest/python-ec2-backup,18cd6d7b9f7c4870cbdac63802987d6679aabe14,boto3,boto,,aws_config = ec2_backup_config.aws,insert,,aws_config=ec2_backup_config.aws,
jcforest/python-ec2-backup,18cd6d7b9f7c4870cbdac63802987d6679aabe14,boto3,boto,"        ec2 = boto.ec2.connect_to_region(server_region, profile_name = account_profile)
        if ( ec2 is None  ):
            print ""ERROR - "" + server_name + "": unable to connect""
            logger.error( server_name + "": unable to connect to region "" + server_region + "" with profile "" + account_profile )
            continue

        # get instances with tag Name 'EC2BackupFrequency'
        reservations = ec2.get_all_reservations(filters = {'tag:' + tagNameFrequency: '*', 'instance-state-name':'*'})

        if ( len(reservations) == 0 ):
            print ""ERROR - "" + server_name + "": unable to find server with tag name  "" + server_tag
            logger.error( server_name + "": unable to find server with tag name "" + server_tag )
            continue
        # loop through reservations and instances
        for reservation in reservations:
            for instance in reservation.instances:
                #print instance.__dict__.keys()
                if 'Name' in instance.tags:
                    instance_name = instance.tags['Name']
                else:
                    instance_name = instance.id

                instance_bk_frequency = instance.tags[tagNameFrequency]
                if tagNameRetain in instance.tags:
                    instance_bk_retain = instance.tags[tagNameRetain]
                else:
                    instance_bk_retain = str(default_backup_retention)

                print ""\n"" + server_name + "": "" + instance_name + "" - frequency: "" + instance_bk_frequency + "", retain: "" + instance_bk_retain

                current_datetime = datetime.datetime.now()
                date_stamp = current_datetime.strftime(""%Y-%m-%d_%H-%M-%S"")
                ami_name = instance_name + ""_"" + instance_bk_frequency + ""_"" + date_stamp
                ami_remove = (current_datetime + datetime.timedelta(days=int(instance_bk_retain))).strftime(""%Y-%m-%d"")

                try:
                    ec2_id = instance.create_image(ami_name, description='Created by EC2Backup', no_reboot=True, dry_run=False)
                except Exception, e:
                    logger.error(""Backup "" + server_name + "": "" + e.message)
                    continue
                logger.info(""Backup "" + server_name + "": "" + ami_name)
                print ""AMI creation started""
                print ""AMI name: "" + ami_name
                images = ec2.get_all_images(image_ids = ec2_id)
                image = images[0]
                image.add_tag(""Name"", ami_name)
                image.add_tag(tagNameRemove, ami_remove)

        # deregister old images
        print ""Deletion of old AMIs""
        images = ec2.get_all_images(filters={'tag:' + tagNameRemove: '*'})","session = boto3.Session(aws_access_key_id=aws_config['access_key_id'], aws_secret_access_key=aws_config['secret_access_key'], region_name=server_region)
        boto3_ec2 = session.resource('ec2', server_region)






        if ( boto3_ec2 is None  ):
            print 'ERROR - ' + server_name + ': unable to connect(boto3)'
            logger.error( server_name + ': unable to connect to region ' + server_region + ' with profile ' + account_profile )
            continue

        instances = boto3_ec2.instances.filter(
            Filters=[{'Name': 'instance-state-name', 'Values': ['*']}, {'Name': 'tag:' + tagNameFrequency, 'Values': ['*']}])
        for instance in instances:

            instance_name = tagValueByKey(instance.tags,'Name')
            instance_bk_frequency = tagValueByKey(instance.tags, tagNameFrequency)
            instance_bk_retain = tagValueByKey(instance.tags, tagNameRetain)

            if instance_name == None:
                instance_name = instance.id
            if instance_bk_retain == None:
                instance_bk_retain = str(default_backup_retention)

            print '\n' + server_name + ': ' + instance_name + ' - frequency: ' + instance_bk_frequency + ', retain: ' + instance_bk_retain

            current_datetime = datetime.datetime.now()
            date_stamp = current_datetime.strftime('%Y-%m-%d_%H-%M-%S')
            ami_name = instance_name + '_' + instance_bk_frequency + '_' + date_stamp
            ami_remove = (current_datetime + datetime.timedelta(days=int(instance_bk_retain))).strftime('%Y-%m-%d')

            try:
                image = instance.create_image(Name=ami_name, Description='Created by EC2Backup', NoReboot=True, DryRun=False)
            except Exception, e:
                logger.error('Backup ' + server_name + ': ' + e.message)
                print 'Error - ' + server_name + ': ' + e.message
                continue
            logger.info('Backup ' + server_name + ': ' + ami_name)
            print 'AMI creation started'
            print 'AMI name: ' + ami_name
            image.create_tags(DryRun=False, Tags=[{'Key': 'Name','Value': ami_name}])
            image.create_tags(DryRun=False, Tags=[{'Key': str(tagNameRemove), 'Value': str(ami_remove)}])



        # deregister old images
        print 'Deletion of old AMIs'
        images = boto3_ec2.images.filter(Filters=[{'Name': 'tag-key', 'Values': [tagNameRemove]}])",update,"ec2=boto.ec2.connect_to_region(server_region,profile_name=account_profile)if(ec2isNone):print""ERROR-""+server_name+"":unabletoconnect""logger.error(server_name+"":unabletoconnecttoregion""+server_region+""withprofile""+account_profile)continue#getinstanceswithtagName'EC2BackupFrequency'reservations=ec2.get_all_reservations(filters={'tag:'+tagNameFrequency:'*','instance-state-name':'*'})if(len(reservations)==0):print""ERROR-""+server_name+"":unabletofindserverwithtagname""+server_taglogger.error(server_name+"":unabletofindserverwithtagname""+server_tag)continue#loopthroughreservationsandinstancesforreservationinreservations:forinstanceinreservation.instances:#printinstance.__dict__.keys()if'Name'ininstance.tags:instance_name=instance.tags['Name']else:instance_name=instance.idinstance_bk_frequency=instance.tags[tagNameFrequency]iftagNameRetainininstance.tags:instance_bk_retain=instance.tags[tagNameRetain]else:instance_bk_retain=str(default_backup_retention)print""\n""+server_name+"":""+instance_name+""-frequency:""+instance_bk_frequency+"",retain:""+instance_bk_retaincurrent_datetime=datetime.datetime.now()date_stamp=current_datetime.strftime(""%Y-%m-%d_%H-%M-%S"")ami_name=instance_name+""_""+instance_bk_frequency+""_""+date_stampami_remove=(current_datetime+datetime.timedelta(days=int(instance_bk_retain))).strftime(""%Y-%m-%d"")try:ec2_id=instance.create_image(ami_name,description='CreatedbyEC2Backup',no_reboot=True,dry_run=False)exceptException,e:logger.error(""Backup""+server_name+"":""+e.message)continuelogger.info(""Backup""+server_name+"":""+ami_name)print""AMIcreationstarted""print""AMIname:""+ami_nameimages=ec2.get_all_images(image_ids=ec2_id)image=images[0]image.add_tag(""Name"",ami_name)image.add_tag(tagNameRemove,ami_remove)#deregisteroldimagesprint""DeletionofoldAMIs""images=ec2.get_all_images(filters={'tag:'+tagNameRemove:'*'})","session=boto3.Session(aws_access_key_id=aws_config['access_key_id'],aws_secret_access_key=aws_config['secret_access_key'],region_name=server_region)boto3_ec2=session.resource('ec2',server_region)if(boto3_ec2isNone):print'ERROR-'+server_name+':unabletoconnect(boto3)'logger.error(server_name+':unabletoconnecttoregion'+server_region+'withprofile'+account_profile)continueinstances=boto3_ec2.instances.filter(Filters=[{'Name':'instance-state-name','Values':['*']},{'Name':'tag:'+tagNameFrequency,'Values':['*']}])forinstanceininstances:instance_name=tagValueByKey(instance.tags,'Name')instance_bk_frequency=tagValueByKey(instance.tags,tagNameFrequency)instance_bk_retain=tagValueByKey(instance.tags,tagNameRetain)ifinstance_name==None:instance_name=instance.idifinstance_bk_retain==None:instance_bk_retain=str(default_backup_retention)print'\n'+server_name+':'+instance_name+'-frequency:'+instance_bk_frequency+',retain:'+instance_bk_retaincurrent_datetime=datetime.datetime.now()date_stamp=current_datetime.strftime('%Y-%m-%d_%H-%M-%S')ami_name=instance_name+'_'+instance_bk_frequency+'_'+date_stampami_remove=(current_datetime+datetime.timedelta(days=int(instance_bk_retain))).strftime('%Y-%m-%d')try:image=instance.create_image(Name=ami_name,Description='CreatedbyEC2Backup',NoReboot=True,DryRun=False)exceptException,e:logger.error('Backup'+server_name+':'+e.message)print'Error-'+server_name+':'+e.messagecontinuelogger.info('Backup'+server_name+':'+ami_name)print'AMIcreationstarted'print'AMIname:'+ami_nameimage.create_tags(DryRun=False,Tags=[{'Key':'Name','Value':ami_name}])image.create_tags(DryRun=False,Tags=[{'Key':str(tagNameRemove),'Value':str(ami_remove)}])#deregisteroldimagesprint'DeletionofoldAMIs'images=boto3_ec2.images.filter(Filters=[{'Name':'tag-key','Values':[tagNameRemove]}])",ec2_backup.py
ischempp/NewsBlur-copy,f03be464253713bc056c630284fb887594c0e092,boto3,boto,import boto,,delete,importboto,,apps/rss_feeds/icon_importer.py
ischempp/NewsBlur-copy,f03be464253713bc056c630284fb887594c0e092,boto3,boto,"                key = settings.S3_CONN.get_bucket(settings.S3_PAGES_BUCKET_NAME).get_key(feed.s3_pages_key)
                if key:
                    compressed_data = key.get_contents_as_string()","key = settings.S3_CONN.Bucket(settings.S3_PAGES_BUCKET_NAME).Object(key=feed.s3_pages_key)
                if key:
                    compressed_data = key.get()[""Body""]",update,key=settings.S3_CONN.get_bucket(settings.S3_PAGES_BUCKET_NAME).get_key(feed.s3_pages_key)ifkey:compressed_data=key.get_contents_as_string(),"key=settings.S3_CONN.Bucket(settings.S3_PAGES_BUCKET_NAME).Object(key=feed.s3_pages_key)ifkey:compressed_data=key.get()[""Body""]",apps/reader/views.py
ischempp/NewsBlur-copy,f03be464253713bc056c630284fb887594c0e092,boto3,boto,from boto.s3.key import Key,import boto3,update,fromboto.s3.keyimportKey,importboto3,apps/rss_feeds/icon_importer.py
ischempp/NewsBlur-copy,f03be464253713bc056c630284fb887594c0e092,boto3,boto," k = Key(settings.S3_CONN.get_bucket(settings.S3_ICONS_BUCKET_NAME))
        k.key = self.feed.s3_icons_key
        k.set_metadata('Content-Type', 'image/png')
        k.set_metadata('Expires', expires)
        k.set_contents_from_string(base64.b64decode(image_str))
        k.set_acl('public-read')","base64.b64decode(image_str)
        settings.S3_CONN.Object(settings.S3_ICONS_BUCKET_NAME, 
                                self.feed.s3_icons_key).put(Body=base64.b64decode(image_str), 
                                                            ExtraArgs={
                                                                'Content-Type': 'image/png',
                                                                'Expires': expires,
                                                                'ACL': 'public-read',
                                                            })",update,"k=Key(settings.S3_CONN.get_bucket(settings.S3_ICONS_BUCKET_NAME))k.key=self.feed.s3_icons_keyk.set_metadata('Content-Type','image/png')k.set_metadata('Expires',expires)k.set_contents_from_string(base64.b64decode(image_str))k.set_acl('public-read')","base64.b64decode(image_str)settings.S3_CONN.Object(settings.S3_ICONS_BUCKET_NAME,self.feed.s3_icons_key).put(Body=base64.b64decode(image_str),ExtraArgs={'Content-Type':'image/png','Expires':expires,'ACL':'public-read',})",apps/rss_feeds/icon_importer.py
ischempp/NewsBlur-copy,f03be464253713bc056c630284fb887594c0e092,boto3,boto,"key = settings.S3_CONN.get_bucket(settings.S3_PAGES_BUCKET_NAME).get_key(self.feed.s3_pages_key)
            compressed_content = key.get_contents_as_string()","key = settings.S3_CONN.Bucket(settings.S3_PAGES_BUCKET_NAME).Object(key=self.feed.s3_pages_key)
            compressed_content = key.get()[""Body""].read()",update,key=settings.S3_CONN.get_bucket(settings.S3_PAGES_BUCKET_NAME).get_key(self.feed.s3_pages_key)compressed_content=key.get_contents_as_string(),"key=settings.S3_CONN.Bucket(settings.S3_PAGES_BUCKET_NAME).Object(key=self.feed.s3_pages_key)compressed_content=key.get()[""Body""].read()",apps/rss_feeds/icon_importer.py
ischempp/NewsBlur-copy,f03be464253713bc056c630284fb887594c0e092,boto3,boto,from boto.s3.key import Key,,delete,fromboto.s3.keyimportKey,,utils/backups/s3.py
ischempp/NewsBlur-copy,f03be464253713bc056c630284fb887594c0e092,boto3,boto,"def save_page_s3(self, html):
        k = Key(settings.S3_CONN.get_bucket(settings.S3_PAGES_BUCKET_NAME))
        k.key = self.feed.s3_pages_key
        k.set_metadata('Content-Encoding', 'gzip')
        k.set_metadata('Content-Type', 'text/html')
        k.set_metadata('Access-Control-Allow-Origin', '*')
        k.set_contents_from_string(compress_string_with_gzip(html.encode('utf-8')))
        k.set_acl('public-read')","def save_page_s3(self, html):
        s3_object = settings.S3_CONN.Object(settings.S3_PAGES_BUCKET_NAME, 
                                            self.feed.s3_pages_key)
        s3_object.put(Body=compress_string_with_gzip(html.encode('utf-8')), 
                      ExtraArgs={
                          'Content-Type': 'text/html',
                          'Content-Encoding': 'gzip',
                          'Access-Control-Allow-Origin': '*',
                          'Expires': expires,
                          'ACL': 'public-read',
                      })",update,"defsave_page_s3(self,html):k=Key(settings.S3_CONN.get_bucket(settings.S3_PAGES_BUCKET_NAME))k.key=self.feed.s3_pages_keyk.set_metadata('Content-Encoding','gzip')k.set_metadata('Content-Type','text/html')k.set_metadata('Access-Control-Allow-Origin','*')k.set_contents_from_string(compress_string_with_gzip(html.encode('utf-8')))k.set_acl('public-read')","defsave_page_s3(self,html):s3_object=settings.S3_CONN.Object(settings.S3_PAGES_BUCKET_NAME,self.feed.s3_pages_key)s3_object.put(Body=compress_string_with_gzip(html.encode('utf-8')),ExtraArgs={'Content-Type':'text/html','Content-Encoding':'gzip','Access-Control-Allow-Origin':'*','Expires':expires,'ACL':'public-read',})",apps/rss_feeds/page_importer.py
ischempp/NewsBlur-copy,f03be464253713bc056c630284fb887594c0e092,boto3,boto,"        k = Key(settings.S3_CONN.get_bucket(settings.S3_PAGES_BUCKET_NAME))
        k.key = self.feed.s3_pages_key","def delete_page_s3(self):
        k = settings.S3_CONN.Bucket(settings.S3_PAGES_BUCKET_NAME).Object(key=self.feed.s3_pages_key)",update,k=Key(settings.S3_CONN.get_bucket(settings.S3_PAGES_BUCKET_NAME))k.key=self.feed.s3_pages_key,defdelete_page_s3(self):k=settings.S3_CONN.Bucket(settings.S3_PAGES_BUCKET_NAME).Object(key=self.feed.s3_pages_key),apps/rss_feeds/page_importer.py
ischempp/NewsBlur-copy,f03be464253713bc056c630284fb887594c0e092,boto3,boto,"from boto.s3.connection import S3Connection, OrdinaryCallingFormat",import boto3,update,"fromboto.s3.connectionimportS3Connection,OrdinaryCallingFormat",importboto3,apps/rss_feeds/icon_importer.py
ischempp/NewsBlur-copy,f03be464253713bc056c630284fb887594c0e092,boto3,boto,"    S3_CONN = S3Connection(S3_ACCESS_KEY, S3_SECRET, calling_format=OrdinaryCallingFormat())
    # if BACKED_BY_AWS.get('pages_on_s3'):
    #     S3_PAGES_BUCKET = S3_CONN.get_bucket(S3_PAGES_BUCKET_NAME)
    # if BACKED_BY_AWS.get('icons_on_s3'):
    #     S3_ICONS_BUCKET = S3_CONN.get_bucket(S3_ICONS_BUCKET_NAME)","    boto_session = boto3.Session(
        aws_access_key_id=S3_ACCESS_KEY,
        aws_secret_access_key=S3_SECRET,
    )
    S3_CONN = boto_session.resource('s3')",update,"S3_CONN=S3Connection(S3_ACCESS_KEY,S3_SECRET,calling_format=OrdinaryCallingFormat())#ifBACKED_BY_AWS.get('pages_on_s3'):#S3_PAGES_BUCKET=S3_CONN.get_bucket(S3_PAGES_BUCKET_NAME)#ifBACKED_BY_AWS.get('icons_on_s3'):#S3_ICONS_BUCKET=S3_CONN.get_bucket(S3_ICONS_BUCKET_NAME)","boto_session=boto3.Session(aws_access_key_id=S3_ACCESS_KEY,aws_secret_access_key=S3_SECRET,)S3_CONN=boto_session.resource('s3')",newsblur_web/settings.py
ischempp/NewsBlur-copy,f03be464253713bc056c630284fb887594c0e092,boto3,boto,"import ssl

_old_match_hostname = ssl.match_hostname

def _new_match_hostname(cert, hostname):
   if hostname.endswith('.s3.amazonaws.com'):
      pos = hostname.find('.s3.amazonaws.com')
      hostname = hostname[:pos].replace('.', '') + hostname[pos:]
   return _old_match_hostname(cert, hostname)

ssl.match_hostname = _new_match_hostname

def save_file_in_s3(filename):
    conn   = S3Connection(ACCESS_KEY, SECRET)
    bucket = conn.get_bucket(BUCKET_NAME)
    k      = Key(bucket)
    k.key  = filename

    k.set_contents_from_filename(filename)

def get_file_from_s3(filename):
    conn   = S3Connection(ACCESS_KEY, SECRET)
    bucket = conn.get_bucket(BUCKET_NAME)
    k      = Key(bucket)
    k.key  = filename

    k.get_contents_to_filename(filename)

def list_backup_in_s3():
    conn   = S3Connection(ACCESS_KEY, SECRET)
    bucket = conn.get_bucket(BUCKET_NAME)

    for i, key in enumerate(bucket.get_all_keys()):
        print(""[%s] %s"" % (i, key.name))

def delete_all_backups():
    #FIXME: validate filename exists
    conn   = S3Connection(ACCESS_KEY, SECRET)
    bucket = conn.get_bucket(BUCKET_NAME)

    for i, key in enumerate(bucket.get_all_keys()):
        print(""deleting %s"" % (key.name))
        key.delete()

if __name__ == '__main__':
    import sys
    if len(sys.argv) < 3:
        print('Usage: %s <get/set/list/delete> <backup_filename>' % (sys.argv[0]))
    else:
        if sys.argv[1] == 'set':
            save_file_in_s3(sys.argv[2])
        elif sys.argv[1] == 'get':
            get_file_from_s3(sys.argv[2])
        elif sys.argv[1] == 'list':
            list_backup_in_s3()
        elif sys.argv[1] == 'delete':
            delete_all_backups()
        else:
            print('Usage: %s <get/set/list/delete> <backup_filename>' % (sys.argv[0]))",,delete,"importssl_old_match_hostname=ssl.match_hostnamedef_new_match_hostname(cert,hostname):ifhostname.endswith('.s3.amazonaws.com'):pos=hostname.find('.s3.amazonaws.com')hostname=hostname[:pos].replace('.','')+hostname[pos:]return_old_match_hostname(cert,hostname)ssl.match_hostname=_new_match_hostnamedefsave_file_in_s3(filename):conn=S3Connection(ACCESS_KEY,SECRET)bucket=conn.get_bucket(BUCKET_NAME)k=Key(bucket)k.key=filenamek.set_contents_from_filename(filename)defget_file_from_s3(filename):conn=S3Connection(ACCESS_KEY,SECRET)bucket=conn.get_bucket(BUCKET_NAME)k=Key(bucket)k.key=filenamek.get_contents_to_filename(filename)deflist_backup_in_s3():conn=S3Connection(ACCESS_KEY,SECRET)bucket=conn.get_bucket(BUCKET_NAME)fori,keyinenumerate(bucket.get_all_keys()):print(""[%s]%s""%(i,key.name))defdelete_all_backups():#FIXME:validatefilenameexistsconn=S3Connection(ACCESS_KEY,SECRET)bucket=conn.get_bucket(BUCKET_NAME)fori,keyinenumerate(bucket.get_all_keys()):print(""deleting%s""%(key.name))key.delete()if__name__=='__main__':importsysiflen(sys.argv)<3:print('Usage:%s<get/set/list/delete><backup_filename>'%(sys.argv[0]))else:ifsys.argv[1]=='set':save_file_in_s3(sys.argv[2])elifsys.argv[1]=='get':get_file_from_s3(sys.argv[2])elifsys.argv[1]=='list':list_backup_in_s3()elifsys.argv[1]=='delete':delete_all_backups()else:print('Usage:%s<get/set/list/delete><backup_filename>'%(sys.argv[0]))",,apps/reader/views.py
ischempp/NewsBlur-copy,f03be464253713bc056c630284fb887594c0e092,boto3,boto,"def __init__(self, bucket_name=settings.S3_AVATARS_BUCKET_NAME):
        if settings.DEBUG:
            import ssl

            try:
                _create_unverified_https_context = ssl._create_unverified_context
            except AttributeError:
                # Legacy Python that doesn't verify HTTPS certificates by default
                pass
            else:
                # Handle target environment that doesn't support HTTPS verification
                ssl._create_default_https_context = _create_unverified_https_context

        self.s3 = S3Connection(ACCESS_KEY, SECRET)
        self.bucket = self.create_bucket(bucket_name)","def __init__(self, bucket_name=settings.S3_AVATARS_BUCKET_NAME):
        # if settings.DEBUG:
        #     import ssl

        #     try:
        #         _create_unverified_https_context = ssl._create_unverified_context
        #     except AttributeError:
        #         # Legacy Python that doesn't verify HTTPS certificates by default
        #         pass
        #     else:
        #         # Handle target environment that doesn't support HTTPS verification
        #         ssl._create_default_https_context = _create_unverified_https_context
        self.bucket_name = bucket_name
        self.s3 = settings.S3_CONN",update,"def__init__(self,bucket_name=settings.S3_AVATARS_BUCKET_NAME):ifsettings.DEBUG:importssltry:_create_unverified_https_context=ssl._create_unverified_contextexceptAttributeError:#LegacyPythonthatdoesn'tverifyHTTPScertificatesbydefaultpasselse:#Handletargetenvironmentthatdoesn'tsupportHTTPSverificationssl._create_default_https_context=_create_unverified_https_contextself.s3=S3Connection(ACCESS_KEY,SECRET)self.bucket=self.create_bucket(bucket_name)","def__init__(self,bucket_name=settings.S3_AVATARS_BUCKET_NAME):#ifsettings.DEBUG:#importssl#try:#_create_unverified_https_context=ssl._create_unverified_context#exceptAttributeError:##LegacyPythonthatdoesn'tverifyHTTPScertificatesbydefault#pass#else:##Handletargetenvironmentthatdoesn'tsupportHTTPSverification#ssl._create_default_https_context=_create_unverified_https_contextself.bucket_name=bucket_nameself.s3=settings.S3_CONN",utils/s3_utils.py
ischempp/NewsBlur-copy,f03be464253713bc056c630284fb887594c0e092,boto3,boto," def create_bucket(self, bucket_name):
        return self.s3.create_bucket(bucket_name)"," def create_bucket(self, bucket_name):
        return self.s3.create_bucket(Bucket=bucket_name)",update,"defcreate_bucket(self,bucket_name):returnself.s3.create_bucket(bucket_name)","defcreate_bucket(self,bucket_name):returnself.s3.create_bucket(Bucket=bucket_name)",utils/s3_utils.py
ischempp/NewsBlur-copy,f03be464253713bc056c630284fb887594c0e092,boto3,boto,"def _save_object(self, key, file_object, content_type=None):
        k = self._make_key()
        k.key = key
        file_object.seek(0)
        

        if content_type:
            k.set_contents_from_file(file_object, headers={
                'Content-Type': content_type,

            })
        else:
            k.set_contents_from_file(file_object)
        k.set_acl('public-read')","def _save_object(self, key, file_object, content_type=None):


        file_object.seek(0)
        s3_object = self.s3.Object(bucket_name=self.bucket_name, key=key)

        if content_type:
            s3_object.put(Body=file_object, ExtraArgs={
                'Content-Type': content_type,
                'ACL': 'public-read',
            })
        else:
            s3_object.put(Body=file_object)",update,"def_save_object(self,key,file_object,content_type=None):k=self._make_key()k.key=keyfile_object.seek(0)ifcontent_type:k.set_contents_from_file(file_object,headers={'Content-Type':content_type,})else:k.set_contents_from_file(file_object)k.set_acl('public-read')","def_save_object(self,key,file_object,content_type=None):file_object.seek(0)s3_object=self.s3.Object(bucket_name=self.bucket_name,key=key)ifcontent_type:s3_object.put(Body=file_object,ExtraArgs={'Content-Type':content_type,'ACL':'public-read',})else:s3_object.put(Body=file_object)",utils/s3_utils.py
onnela-lab/beiwe-backend,d30c16ddc1446bbb052ac1896510ba96e1326327,boto3,boto,,"from _ssl import SSLError
from httplib import IncompleteRead

from boto import connect_s3
from boto.s3.connection import OrdinaryCallingFormat
from boto.s3.key import Key

from config.constants import DEFAULT_S3_RETRIES, CHUNKS_FOLDER
from config.settings import S3_BUCKET, S3_ACCESS_CREDENTIALS_KEY, S3_ACCESS_CREDENTIALS_USER
from libs import encryption

CONN = connect_s3(aws_access_key_id=S3_ACCESS_CREDENTIALS_USER,
                  aws_secret_access_key=S3_ACCESS_CREDENTIALS_KEY,
                  is_secure=True,
                  calling_format=OrdinaryCallingFormat())


def _get_bucket(name):
    """""" Gets a connection to a bucket, raises an S3ResponseError on failure. """"""
    return CONN.get_bucket(name)


# def s3_upload_raw( key_name, some_string ):
#     """""" Method uploads string to bucket with key_name""""""
#     bucket = _get_bucket(S3_BUCKET)
#     key = bucket.new_key(key_name)
#     key.set_contents_from_string(some_string)


def s3_upload(key_path, data_string, study_object_id, raw_path=False):
    """""" Uploads data to s3, ensures data is encrypted with the key from the provided study.
    Takes an optional argument, raw_path, which defaults to false.  When false the study_id is
    prepended to the S3 file path (key_path), placing the file in the appropriate study folder. """"""

    if not raw_path:
        key_path = study_object_id + ""/"" + key_path

    data = encryption.encrypt_for_server(data_string, study_object_id)
    key = _get_bucket(S3_BUCKET).new_key(key_path)
    key.set_contents_from_string(data)


# def s3_retrieve_raw( key_name ):
#     """""" Method returns file contents with specified S3 key path""""""
#     key = Key(_get_bucket(S3_BUCKET), key_name)
#     return key.read()


def s3_retrieve(key_path, study_object_id, raw_path=False, number_retries=DEFAULT_S3_RETRIES):
    """""" Takes an S3 file path (key_path), and a study ID.  Takes an optional argument, raw_path,
    which defaults to false.  When set to false the path is prepended to place the file in the
    appropriate study_id folder. """"""
    if not raw_path:
        key_path = study_object_id + ""/"" + key_path
    encrypted_data = _do_retrieve(S3_BUCKET, key_path, number_retries=number_retries)
    return encryption.decrypt_server(encrypted_data, study_object_id)


def _do_retrieve(bucket_name, key_path, number_retries=DEFAULT_S3_RETRIES):
    """""" Run-logic to do a data retrieval for a file in an S3 bucket.""""""
    key = Key(_get_bucket(bucket_name), key_path)
    try:
        return key.read()
    except IncompleteRead:
        if number_retries > 0:
            print(""s3_retrieve failed with incomplete read, retrying on %s"" % key_path)
            return _do_retrieve(bucket_name, key_path, number_retries=number_retries - 1)
        raise
    except SSLError as e:
        if 'The read operation timed out' == e.message:
            print ""s3_retreive failed with timeout, retrying on %s"" % key_path
            return _do_retrieve(bucket_name, key_path, number_retries=number_retries - 1)
        raise


# def s3_retrieve_or_none(key_path, study_id, raw_path=False):
#     """""" Like s3_retreive except returns None if the key does not exist instead
#         of erroring.  This API makes an additional network request, increasing
#         cost and latency. """"""
#     if not raw_path: key_path = str(study_id) + ""/"" + key_path
#     key = _get_bucket(S3_BUCKET).get_key(key_path) #this line is the only difference.
#     if not key: return None
#     return encryption.decrypt_server(key.read(), study_id)


def s3_list_files(prefix, as_generator=False):
    """""" Method fetches a list of filenames with prefix.
        note: entering the empty string into this search without later calling
        the object results in a truncated/paginated view.""""""
    return _do_list_files(S3_BUCKET, prefix, as_generator=as_generator)


def _do_list_files(bucket_name, prefix, as_generator=False):
    bucket = _get_bucket(bucket_name)
    results = bucket.list(prefix=prefix)
    if as_generator:
        return (i.name.strip(""/"") for i in results)
    return [i.name.strip(""/"") for i in results]


def s3_delete(key_path):
    raise Exception(""NOOO"")
    if CHUNKS_FOLDER not in key_path:
        raise Exception(""absolutely not deleting %s"" % key_path)
    key = Key(_get_bucket(S3_BUCKET), key_path)
    key.delete()


#unused file based upload function, not up to date with new upload file names.
# def s3_upload_handler_file( key_name, file_obj ):
#     """""" Method uploads file object to bucket with key_name""""""
#     bucket = _get_bucket(S3_BUCKET)
#     key = bucket.new_key(key_name)
#     key.set_metadata('Content-Type', mimetypes.guess_type(key_name))
#     # seek to the beginning of the file and read it into the key
#     file_obj.seek(0)
#     key.set_contents_from_file(file_obj)


################################################################################
######################### Client Key Management ################################
################################################################################

def create_client_key_pair(patient_id, study_id):
    """"""Generate key pairing, push to database, return sanitized key for client.""""""
    public, private = encryption.generate_key_pairing()
    s3_upload(""keys/"" + patient_id + ""_private"", private, study_id )
    s3_upload(""keys/"" + patient_id + ""_public"", public, study_id )


def get_client_public_key_string(patient_id, study_id):
    """"""Grabs a user's public key string from s3.""""""
    key_string = s3_retrieve( ""keys/"" + patient_id +""_public"" , study_id)
    return encryption.prepare_X509_key_for_java( key_string )


def get_client_public_key(patient_id, study_id):
    """"""Grabs a user's public key file from s3.""""""
    key = s3_retrieve( ""keys/"" + patient_id +""_public"", study_id )
    return encryption.import_RSA_key( key )


def get_client_private_key(patient_id, study_id):
    """"""Grabs a user's private key file from s3.""""""
    key = s3_retrieve( ""keys/"" + patient_id +""_private"", study_id)
    return encryption.import_RSA_key( key )",insert,,"from_sslimportSSLErrorfromhttplibimportIncompleteReadfrombotoimportconnect_s3fromboto.s3.connectionimportOrdinaryCallingFormatfromboto.s3.keyimportKeyfromconfig.constantsimportDEFAULT_S3_RETRIES,CHUNKS_FOLDERfromconfig.settingsimportS3_BUCKET,S3_ACCESS_CREDENTIALS_KEY,S3_ACCESS_CREDENTIALS_USERfromlibsimportencryptionCONN=connect_s3(aws_access_key_id=S3_ACCESS_CREDENTIALS_USER,aws_secret_access_key=S3_ACCESS_CREDENTIALS_KEY,is_secure=True,calling_format=OrdinaryCallingFormat())def_get_bucket(name):""""""Getsaconnectiontoabucket,raisesanS3ResponseErroronfailure.""""""returnCONN.get_bucket(name)#defs3_upload_raw(key_name,some_string):#""""""Methoduploadsstringtobucketwithkey_name""""""#bucket=_get_bucket(S3_BUCKET)#key=bucket.new_key(key_name)#key.set_contents_from_string(some_string)defs3_upload(key_path,data_string,study_object_id,raw_path=False):""""""Uploadsdatatos3,ensuresdataisencryptedwiththekeyfromtheprovidedstudy.Takesanoptionalargument,raw_path,whichdefaultstofalse.Whenfalsethestudy_idisprependedtotheS3filepath(key_path),placingthefileintheappropriatestudyfolder.""""""ifnotraw_path:key_path=study_object_id+""/""+key_pathdata=encryption.encrypt_for_server(data_string,study_object_id)key=_get_bucket(S3_BUCKET).new_key(key_path)key.set_contents_from_string(data)#defs3_retrieve_raw(key_name):#""""""MethodreturnsfilecontentswithspecifiedS3keypath""""""#key=Key(_get_bucket(S3_BUCKET),key_name)#returnkey.read()defs3_retrieve(key_path,study_object_id,raw_path=False,number_retries=DEFAULT_S3_RETRIES):""""""TakesanS3filepath(key_path),andastudyID.Takesanoptionalargument,raw_path,whichdefaultstofalse.Whensettofalsethepathisprependedtoplacethefileintheappropriatestudy_idfolder.""""""ifnotraw_path:key_path=study_object_id+""/""+key_pathencrypted_data=_do_retrieve(S3_BUCKET,key_path,number_retries=number_retries)returnencryption.decrypt_server(encrypted_data,study_object_id)def_do_retrieve(bucket_name,key_path,number_retries=DEFAULT_S3_RETRIES):""""""Run-logictodoadataretrievalforafileinanS3bucket.""""""key=Key(_get_bucket(bucket_name),key_path)try:returnkey.read()exceptIncompleteRead:ifnumber_retries>0:print(""s3_retrievefailedwithincompleteread,retryingon%s""%key_path)return_do_retrieve(bucket_name,key_path,number_retries=number_retries-1)raiseexceptSSLErrorase:if'Thereadoperationtimedout'==e.message:print""s3_retreivefailedwithtimeout,retryingon%s""%key_pathreturn_do_retrieve(bucket_name,key_path,number_retries=number_retries-1)raise#defs3_retrieve_or_none(key_path,study_id,raw_path=False):#""""""Likes3_retreiveexceptreturnsNoneifthekeydoesnotexistinstead#oferroring.ThisAPImakesanadditionalnetworkrequest,increasing#costandlatency.""""""#ifnotraw_path:key_path=str(study_id)+""/""+key_path#key=_get_bucket(S3_BUCKET).get_key(key_path)#thislineistheonlydifference.#ifnotkey:returnNone#returnencryption.decrypt_server(key.read(),study_id)defs3_list_files(prefix,as_generator=False):""""""Methodfetchesalistoffilenameswithprefix.note:enteringtheemptystringintothissearchwithoutlatercallingtheobjectresultsinatruncated/paginatedview.""""""return_do_list_files(S3_BUCKET,prefix,as_generator=as_generator)def_do_list_files(bucket_name,prefix,as_generator=False):bucket=_get_bucket(bucket_name)results=bucket.list(prefix=prefix)ifas_generator:return(i.name.strip(""/"")foriinresults)return[i.name.strip(""/"")foriinresults]defs3_delete(key_path):raiseException(""NOOO"")ifCHUNKS_FOLDERnotinkey_path:raiseException(""absolutelynotdeleting%s""%key_path)key=Key(_get_bucket(S3_BUCKET),key_path)key.delete()#unusedfilebaseduploadfunction,notuptodatewithnewuploadfilenames.#defs3_upload_handler_file(key_name,file_obj):#""""""Methoduploadsfileobjecttobucketwithkey_name""""""#bucket=_get_bucket(S3_BUCKET)#key=bucket.new_key(key_name)#key.set_metadata('Content-Type',mimetypes.guess_type(key_name))##seektothebeginningofthefileandreaditintothekey#file_obj.seek(0)#key.set_contents_from_file(file_obj)#########################################################################################################ClientKeyManagement################################################################################################################defcreate_client_key_pair(patient_id,study_id):""""""Generatekeypairing,pushtodatabase,returnsanitizedkeyforclient.""""""public,private=encryption.generate_key_pairing()s3_upload(""keys/""+patient_id+""_private"",private,study_id)s3_upload(""keys/""+patient_id+""_public"",public,study_id)defget_client_public_key_string(patient_id,study_id):""""""Grabsauser'spublickeystringfroms3.""""""key_string=s3_retrieve(""keys/""+patient_id+""_public"",study_id)returnencryption.prepare_X509_key_for_java(key_string)defget_client_public_key(patient_id,study_id):""""""Grabsauser'spublickeyfilefroms3.""""""key=s3_retrieve(""keys/""+patient_id+""_public"",study_id)returnencryption.import_RSA_key(key)defget_client_private_key(patient_id,study_id):""""""Grabsauser'sprivatekeyfilefroms3.""""""key=s3_retrieve(""keys/""+patient_id+""_private"",study_id)returnencryption.import_RSA_key(key)",deprecated/libs/s3_deprecated.py
onnela-lab/beiwe-backend,d30c16ddc1446bbb052ac1896510ba96e1326327,boto3,boto,"from boto import connect_s3
from boto.s3.connection import OrdinaryCallingFormat
from boto.s3.key import Key

from config.constants import DEFAULT_S3_RETRIES, CHUNKS_FOLDER
from config.settings import S3_BUCKET, S3_ACCESS_CREDENTIALS_KEY, S3_ACCESS_CREDENTIALS_USER","import boto3


from config.constants import DEFAULT_S3_RETRIES
from config.settings import S3_BUCKET, S3_ACCESS_CREDENTIALS_KEY, S3_ACCESS_CREDENTIALS_USER, S3_REGION_NAME",update,"frombotoimportconnect_s3fromboto.s3.connectionimportOrdinaryCallingFormatfromboto.s3.keyimportKeyfromconfig.constantsimportDEFAULT_S3_RETRIES,CHUNKS_FOLDERfromconfig.settingsimportS3_BUCKET,S3_ACCESS_CREDENTIALS_KEY,S3_ACCESS_CREDENTIALS_USER","importboto3fromconfig.constantsimportDEFAULT_S3_RETRIESfromconfig.settingsimportS3_BUCKET,S3_ACCESS_CREDENTIALS_KEY,S3_ACCESS_CREDENTIALS_USER,S3_REGION_NAME",libs/s3.py
onnela-lab/beiwe-backend,d30c16ddc1446bbb052ac1896510ba96e1326327,boto3,boto,"CONN = connect_s3(aws_access_key_id=S3_ACCESS_CREDENTIALS_USER,
                  aws_secret_access_key=S3_ACCESS_CREDENTIALS_KEY,
                  is_secure=True,
                  calling_format=OrdinaryCallingFormat())


def _get_bucket(name):
    """""" Gets a connection to a bucket, raises an S3ResponseError on failure. """"""
    return CONN.get_bucket(name)","conn = boto3.client('s3',
                    aws_access_key_id=S3_ACCESS_CREDENTIALS_USER,
                    aws_secret_access_key=S3_ACCESS_CREDENTIALS_KEY,
                    region_name=S3_REGION_NAME)",update,"CONN=connect_s3(aws_access_key_id=S3_ACCESS_CREDENTIALS_USER,aws_secret_access_key=S3_ACCESS_CREDENTIALS_KEY,is_secure=True,calling_format=OrdinaryCallingFormat())def_get_bucket(name):""""""Getsaconnectiontoabucket,raisesanS3ResponseErroronfailure.""""""returnCONN.get_bucket(name)","conn=boto3.client('s3',aws_access_key_id=S3_ACCESS_CREDENTIALS_USER,aws_secret_access_key=S3_ACCESS_CREDENTIALS_KEY,region_name=S3_REGION_NAME)",libs/s3.py
onnela-lab/beiwe-backend,d30c16ddc1446bbb052ac1896510ba96e1326327,boto3,boto,"def _get_bucket(name):
    """""" Gets a connection to a bucket, raises an S3ResponseError on failure. """"""
    return CONN.get_bucket(name)",,delete,"def_get_bucket(name):""""""Getsaconnectiontoabucket,raisesanS3ResponseErroronfailure.""""""returnCONN.get_bucket(name)",,deprecated/libs/s3_deprecated.py
onnela-lab/beiwe-backend,d30c16ddc1446bbb052ac1896510ba96e1326327,boto3,boto,"def s3_upload(key_path, data_string, study_object_id, raw_path=False):
    """""" Uploads data to s3, ensures data is encrypted with the key from the provided study.
    Takes an optional argument, raw_path, which defaults to false.  When false the study_id is
    prepended to the S3 file path (key_path), placing the file in the appropriate study folder. """"""

    if not raw_path:
        key_path = study_object_id + ""/"" + key_path

    data = encryption.encrypt_for_server(data_string, study_object_id)
    key = _get_bucket(S3_BUCKET).new_key(key_path)
    key.set_contents_from_string(data)","def s3_upload(key_path, data_string, study_object_id, raw_path=False):




    if not raw_path:
        key_path = study_object_id + ""/"" + key_path

    data = encryption.encrypt_for_server(data_string, study_object_id)
    conn.put_object(Body=data, Bucket=S3_BUCKET, Key=key_path, ContentType='string')",update,"defs3_upload(key_path,data_string,study_object_id,raw_path=False):""""""Uploadsdatatos3,ensuresdataisencryptedwiththekeyfromtheprovidedstudy.Takesanoptionalargument,raw_path,whichdefaultstofalse.Whenfalsethestudy_idisprependedtotheS3filepath(key_path),placingthefileintheappropriatestudyfolder.""""""ifnotraw_path:key_path=study_object_id+""/""+key_pathdata=encryption.encrypt_for_server(data_string,study_object_id)key=_get_bucket(S3_BUCKET).new_key(key_path)key.set_contents_from_string(data)","defs3_upload(key_path,data_string,study_object_id,raw_path=False):ifnotraw_path:key_path=study_object_id+""/""+key_pathdata=encryption.encrypt_for_server(data_string,study_object_id)conn.put_object(Body=data,Bucket=S3_BUCKET,Key=key_path,ContentType='string')",libs/s3.py
onnela-lab/beiwe-backend,d30c16ddc1446bbb052ac1896510ba96e1326327,boto3,boto,"def _do_retrieve(bucket_name, key_path, number_retries=DEFAULT_S3_RETRIES):
    """""" Run-logic to do a data retrieval for a file in an S3 bucket.""""""
    key = Key(_get_bucket(bucket_name), key_path)
    try:
        return key.read()
    except IncompleteRead:
        if number_retries > 0:
            print(""s3_retrieve failed with incomplete read, retrying on %s"" % key_path)
            return _do_retrieve(bucket_name, key_path, number_retries=number_retries - 1)
        raise
    except SSLError as e:
        if 'The read operation timed out' == e.message:
            print ""s3_retreive failed with timeout, retrying on %s"" % key_path
            return _do_retrieve(bucket_name, key_path, number_retries=number_retries - 1)
        raise","def _do_retrieve(bucket_name, key_path, number_retries=DEFAULT_S3_RETRIES):
    """""" Run-logic to do a data retrieval for a file in an S3 bucket.""""""

    try:
        return conn.get_object(Bucket=bucket_name, Key=key_path, ResponseContentType='string')
    except Exception:
        if number_retries > 0:
            print(""s3_retrieve failed with incomplete read, retrying on %s"" % key_path)
            return _do_retrieve(bucket_name, key_path, number_retries=number_retries - 1)
        raise",update,"def_do_retrieve(bucket_name,key_path,number_retries=DEFAULT_S3_RETRIES):""""""Run-logictodoadataretrievalforafileinanS3bucket.""""""key=Key(_get_bucket(bucket_name),key_path)try:returnkey.read()exceptIncompleteRead:ifnumber_retries>0:print(""s3_retrievefailedwithincompleteread,retryingon%s""%key_path)return_do_retrieve(bucket_name,key_path,number_retries=number_retries-1)raiseexceptSSLErrorase:if'Thereadoperationtimedout'==e.message:print""s3_retreivefailedwithtimeout,retryingon%s""%key_pathreturn_do_retrieve(bucket_name,key_path,number_retries=number_retries-1)raise","def_do_retrieve(bucket_name,key_path,number_retries=DEFAULT_S3_RETRIES):""""""Run-logictodoadataretrievalforafileinanS3bucket.""""""try:returnconn.get_object(Bucket=bucket_name,Key=key_path,ResponseContentType='string')exceptException:ifnumber_retries>0:print(""s3_retrievefailedwithincompleteread,retryingon%s""%key_path)return_do_retrieve(bucket_name,key_path,number_retries=number_retries-1)raise",deprecated/libs/s3_deprecated.py
onnela-lab/beiwe-backend,d30c16ddc1446bbb052ac1896510ba96e1326327,boto3,boto,,"import boto3
from config.settings import S3_ACCESS_CREDENTIALS_KEY, S3_ACCESS_CREDENTIALS_USER

regions = [
    'us-east-2',
    'us-east-1',
    'us-west-1',
    'us-west-2',
    'ap-northeast-1',
    'ap-northeast-2',
    #'ap-northeast-3', # Only available in conjunction with the Asia Pacific (Tokyo) Region
    'ap-south-1',
    'ap-southeast-1',
    'ap-southeast-2',
    'ca-central-1',
    'cn-north-1',
    'cn-northwest-1',
    'eu-central-1',
    'eu-west-1',
    'eu-west-2',
    'eu-west-3',
    'sa-east-1',
]

for region in regions:
    conn = boto3.client('s3',
                        aws_access_key_id=S3_ACCESS_CREDENTIALS_USER,
                        aws_secret_access_key=S3_ACCESS_CREDENTIALS_KEY,
                        region_name=region)

    try:
        conn.list_buckets()
        print ""S3 exists in region:"",region
    except Exception as e:
        print e
        print region,""had an error""",insert,,"importboto3fromconfig.settingsimportS3_ACCESS_CREDENTIALS_KEY,S3_ACCESS_CREDENTIALS_USERregions=['us-east-2','us-east-1','us-west-1','us-west-2','ap-northeast-1','ap-northeast-2',#'ap-northeast-3',#OnlyavailableinconjunctionwiththeAsiaPacific(Tokyo)Region'ap-south-1','ap-southeast-1','ap-southeast-2','ca-central-1','cn-north-1','cn-northwest-1','eu-central-1','eu-west-1','eu-west-2','eu-west-3','sa-east-1',]forregioninregions:conn=boto3.client('s3',aws_access_key_id=S3_ACCESS_CREDENTIALS_USER,aws_secret_access_key=S3_ACCESS_CREDENTIALS_KEY,region_name=region)try:conn.list_buckets()print""S3existsinregion:"",regionexceptExceptionase:printeprintregion,""hadanerror""",scripts/test_s3_region_access.py
heinzfutto/server-backend,d30c16ddc1446bbb052ac1896510ba96e1326327,boto3,boto,,"from _ssl import SSLError
from httplib import IncompleteRead

from boto import connect_s3
from boto.s3.connection import OrdinaryCallingFormat
from boto.s3.key import Key

from config.constants import DEFAULT_S3_RETRIES, CHUNKS_FOLDER
from config.settings import S3_BUCKET, S3_ACCESS_CREDENTIALS_KEY, S3_ACCESS_CREDENTIALS_USER
from libs import encryption

CONN = connect_s3(aws_access_key_id=S3_ACCESS_CREDENTIALS_USER,
                  aws_secret_access_key=S3_ACCESS_CREDENTIALS_KEY,
                  is_secure=True,
                  calling_format=OrdinaryCallingFormat())


def _get_bucket(name):
    """""" Gets a connection to a bucket, raises an S3ResponseError on failure. """"""
    return CONN.get_bucket(name)


# def s3_upload_raw( key_name, some_string ):
#     """""" Method uploads string to bucket with key_name""""""
#     bucket = _get_bucket(S3_BUCKET)
#     key = bucket.new_key(key_name)
#     key.set_contents_from_string(some_string)


def s3_upload(key_path, data_string, study_object_id, raw_path=False):
    """""" Uploads data to s3, ensures data is encrypted with the key from the provided study.
    Takes an optional argument, raw_path, which defaults to false.  When false the study_id is
    prepended to the S3 file path (key_path), placing the file in the appropriate study folder. """"""

    if not raw_path:
        key_path = study_object_id + ""/"" + key_path

    data = encryption.encrypt_for_server(data_string, study_object_id)
    key = _get_bucket(S3_BUCKET).new_key(key_path)
    key.set_contents_from_string(data)


# def s3_retrieve_raw( key_name ):
#     """""" Method returns file contents with specified S3 key path""""""
#     key = Key(_get_bucket(S3_BUCKET), key_name)
#     return key.read()


def s3_retrieve(key_path, study_object_id, raw_path=False, number_retries=DEFAULT_S3_RETRIES):
    """""" Takes an S3 file path (key_path), and a study ID.  Takes an optional argument, raw_path,
    which defaults to false.  When set to false the path is prepended to place the file in the
    appropriate study_id folder. """"""
    if not raw_path:
        key_path = study_object_id + ""/"" + key_path
    encrypted_data = _do_retrieve(S3_BUCKET, key_path, number_retries=number_retries)
    return encryption.decrypt_server(encrypted_data, study_object_id)


def _do_retrieve(bucket_name, key_path, number_retries=DEFAULT_S3_RETRIES):
    """""" Run-logic to do a data retrieval for a file in an S3 bucket.""""""
    key = Key(_get_bucket(bucket_name), key_path)
    try:
        return key.read()
    except IncompleteRead:
        if number_retries > 0:
            print(""s3_retrieve failed with incomplete read, retrying on %s"" % key_path)
            return _do_retrieve(bucket_name, key_path, number_retries=number_retries - 1)
        raise
    except SSLError as e:
        if 'The read operation timed out' == e.message:
            print ""s3_retreive failed with timeout, retrying on %s"" % key_path
            return _do_retrieve(bucket_name, key_path, number_retries=number_retries - 1)
        raise


# def s3_retrieve_or_none(key_path, study_id, raw_path=False):
#     """""" Like s3_retreive except returns None if the key does not exist instead
#         of erroring.  This API makes an additional network request, increasing
#         cost and latency. """"""
#     if not raw_path: key_path = str(study_id) + ""/"" + key_path
#     key = _get_bucket(S3_BUCKET).get_key(key_path) #this line is the only difference.
#     if not key: return None
#     return encryption.decrypt_server(key.read(), study_id)


def s3_list_files(prefix, as_generator=False):
    """""" Method fetches a list of filenames with prefix.
        note: entering the empty string into this search without later calling
        the object results in a truncated/paginated view.""""""
    return _do_list_files(S3_BUCKET, prefix, as_generator=as_generator)


def _do_list_files(bucket_name, prefix, as_generator=False):
    bucket = _get_bucket(bucket_name)
    results = bucket.list(prefix=prefix)
    if as_generator:
        return (i.name.strip(""/"") for i in results)
    return [i.name.strip(""/"") for i in results]


def s3_delete(key_path):
    raise Exception(""NOOO"")
    if CHUNKS_FOLDER not in key_path:
        raise Exception(""absolutely not deleting %s"" % key_path)
    key = Key(_get_bucket(S3_BUCKET), key_path)
    key.delete()


#unused file based upload function, not up to date with new upload file names.
# def s3_upload_handler_file( key_name, file_obj ):
#     """""" Method uploads file object to bucket with key_name""""""
#     bucket = _get_bucket(S3_BUCKET)
#     key = bucket.new_key(key_name)
#     key.set_metadata('Content-Type', mimetypes.guess_type(key_name))
#     # seek to the beginning of the file and read it into the key
#     file_obj.seek(0)
#     key.set_contents_from_file(file_obj)


################################################################################
######################### Client Key Management ################################
################################################################################

def create_client_key_pair(patient_id, study_id):
    """"""Generate key pairing, push to database, return sanitized key for client.""""""
    public, private = encryption.generate_key_pairing()
    s3_upload(""keys/"" + patient_id + ""_private"", private, study_id )
    s3_upload(""keys/"" + patient_id + ""_public"", public, study_id )


def get_client_public_key_string(patient_id, study_id):
    """"""Grabs a user's public key string from s3.""""""
    key_string = s3_retrieve( ""keys/"" + patient_id +""_public"" , study_id)
    return encryption.prepare_X509_key_for_java( key_string )


def get_client_public_key(patient_id, study_id):
    """"""Grabs a user's public key file from s3.""""""
    key = s3_retrieve( ""keys/"" + patient_id +""_public"", study_id )
    return encryption.import_RSA_key( key )


def get_client_private_key(patient_id, study_id):
    """"""Grabs a user's private key file from s3.""""""
    key = s3_retrieve( ""keys/"" + patient_id +""_private"", study_id)
    return encryption.import_RSA_key( key )",insert,,"from_sslimportSSLErrorfromhttplibimportIncompleteReadfrombotoimportconnect_s3fromboto.s3.connectionimportOrdinaryCallingFormatfromboto.s3.keyimportKeyfromconfig.constantsimportDEFAULT_S3_RETRIES,CHUNKS_FOLDERfromconfig.settingsimportS3_BUCKET,S3_ACCESS_CREDENTIALS_KEY,S3_ACCESS_CREDENTIALS_USERfromlibsimportencryptionCONN=connect_s3(aws_access_key_id=S3_ACCESS_CREDENTIALS_USER,aws_secret_access_key=S3_ACCESS_CREDENTIALS_KEY,is_secure=True,calling_format=OrdinaryCallingFormat())def_get_bucket(name):""""""Getsaconnectiontoabucket,raisesanS3ResponseErroronfailure.""""""returnCONN.get_bucket(name)#defs3_upload_raw(key_name,some_string):#""""""Methoduploadsstringtobucketwithkey_name""""""#bucket=_get_bucket(S3_BUCKET)#key=bucket.new_key(key_name)#key.set_contents_from_string(some_string)defs3_upload(key_path,data_string,study_object_id,raw_path=False):""""""Uploadsdatatos3,ensuresdataisencryptedwiththekeyfromtheprovidedstudy.Takesanoptionalargument,raw_path,whichdefaultstofalse.Whenfalsethestudy_idisprependedtotheS3filepath(key_path),placingthefileintheappropriatestudyfolder.""""""ifnotraw_path:key_path=study_object_id+""/""+key_pathdata=encryption.encrypt_for_server(data_string,study_object_id)key=_get_bucket(S3_BUCKET).new_key(key_path)key.set_contents_from_string(data)#defs3_retrieve_raw(key_name):#""""""MethodreturnsfilecontentswithspecifiedS3keypath""""""#key=Key(_get_bucket(S3_BUCKET),key_name)#returnkey.read()defs3_retrieve(key_path,study_object_id,raw_path=False,number_retries=DEFAULT_S3_RETRIES):""""""TakesanS3filepath(key_path),andastudyID.Takesanoptionalargument,raw_path,whichdefaultstofalse.Whensettofalsethepathisprependedtoplacethefileintheappropriatestudy_idfolder.""""""ifnotraw_path:key_path=study_object_id+""/""+key_pathencrypted_data=_do_retrieve(S3_BUCKET,key_path,number_retries=number_retries)returnencryption.decrypt_server(encrypted_data,study_object_id)def_do_retrieve(bucket_name,key_path,number_retries=DEFAULT_S3_RETRIES):""""""Run-logictodoadataretrievalforafileinanS3bucket.""""""key=Key(_get_bucket(bucket_name),key_path)try:returnkey.read()exceptIncompleteRead:ifnumber_retries>0:print(""s3_retrievefailedwithincompleteread,retryingon%s""%key_path)return_do_retrieve(bucket_name,key_path,number_retries=number_retries-1)raiseexceptSSLErrorase:if'Thereadoperationtimedout'==e.message:print""s3_retreivefailedwithtimeout,retryingon%s""%key_pathreturn_do_retrieve(bucket_name,key_path,number_retries=number_retries-1)raise#defs3_retrieve_or_none(key_path,study_id,raw_path=False):#""""""Likes3_retreiveexceptreturnsNoneifthekeydoesnotexistinstead#oferroring.ThisAPImakesanadditionalnetworkrequest,increasing#costandlatency.""""""#ifnotraw_path:key_path=str(study_id)+""/""+key_path#key=_get_bucket(S3_BUCKET).get_key(key_path)#thislineistheonlydifference.#ifnotkey:returnNone#returnencryption.decrypt_server(key.read(),study_id)defs3_list_files(prefix,as_generator=False):""""""Methodfetchesalistoffilenameswithprefix.note:enteringtheemptystringintothissearchwithoutlatercallingtheobjectresultsinatruncated/paginatedview.""""""return_do_list_files(S3_BUCKET,prefix,as_generator=as_generator)def_do_list_files(bucket_name,prefix,as_generator=False):bucket=_get_bucket(bucket_name)results=bucket.list(prefix=prefix)ifas_generator:return(i.name.strip(""/"")foriinresults)return[i.name.strip(""/"")foriinresults]defs3_delete(key_path):raiseException(""NOOO"")ifCHUNKS_FOLDERnotinkey_path:raiseException(""absolutelynotdeleting%s""%key_path)key=Key(_get_bucket(S3_BUCKET),key_path)key.delete()#unusedfilebaseduploadfunction,notuptodatewithnewuploadfilenames.#defs3_upload_handler_file(key_name,file_obj):#""""""Methoduploadsfileobjecttobucketwithkey_name""""""#bucket=_get_bucket(S3_BUCKET)#key=bucket.new_key(key_name)#key.set_metadata('Content-Type',mimetypes.guess_type(key_name))##seektothebeginningofthefileandreaditintothekey#file_obj.seek(0)#key.set_contents_from_file(file_obj)#########################################################################################################ClientKeyManagement################################################################################################################defcreate_client_key_pair(patient_id,study_id):""""""Generatekeypairing,pushtodatabase,returnsanitizedkeyforclient.""""""public,private=encryption.generate_key_pairing()s3_upload(""keys/""+patient_id+""_private"",private,study_id)s3_upload(""keys/""+patient_id+""_public"",public,study_id)defget_client_public_key_string(patient_id,study_id):""""""Grabsauser'spublickeystringfroms3.""""""key_string=s3_retrieve(""keys/""+patient_id+""_public"",study_id)returnencryption.prepare_X509_key_for_java(key_string)defget_client_public_key(patient_id,study_id):""""""Grabsauser'spublickeyfilefroms3.""""""key=s3_retrieve(""keys/""+patient_id+""_public"",study_id)returnencryption.import_RSA_key(key)defget_client_private_key(patient_id,study_id):""""""Grabsauser'sprivatekeyfilefroms3.""""""key=s3_retrieve(""keys/""+patient_id+""_private"",study_id)returnencryption.import_RSA_key(key)",deprecated/libs/s3_deprecated.py
heinzfutto/server-backend,d30c16ddc1446bbb052ac1896510ba96e1326327,boto3,boto,"from _ssl import SSLError
from httplib import IncompleteRead

from boto import connect_s3
from boto.s3.connection import OrdinaryCallingFormat
from boto.s3.key import Key

from config.constants import DEFAULT_S3_RETRIES, CHUNKS_FOLDER
from config.settings import S3_BUCKET, S3_ACCESS_CREDENTIALS_KEY, S3_ACCESS_CREDENTIALS_USER","import boto3


from config.constants import DEFAULT_S3_RETRIES
from config.settings import S3_BUCKET, S3_ACCESS_CREDENTIALS_KEY, S3_ACCESS_CREDENTIALS_USER, S3_REGION_NAME",update,"from_sslimportSSLErrorfromhttplibimportIncompleteReadfrombotoimportconnect_s3fromboto.s3.connectionimportOrdinaryCallingFormatfromboto.s3.keyimportKeyfromconfig.constantsimportDEFAULT_S3_RETRIES,CHUNKS_FOLDERfromconfig.settingsimportS3_BUCKET,S3_ACCESS_CREDENTIALS_KEY,S3_ACCESS_CREDENTIALS_USER","importboto3fromconfig.constantsimportDEFAULT_S3_RETRIESfromconfig.settingsimportS3_BUCKET,S3_ACCESS_CREDENTIALS_KEY,S3_ACCESS_CREDENTIALS_USER,S3_REGION_NAME",libs/s3.py
heinzfutto/server-backend,d30c16ddc1446bbb052ac1896510ba96e1326327,boto3,boto,"CONN = connect_s3(aws_access_key_id=S3_ACCESS_CREDENTIALS_USER,
                  aws_secret_access_key=S3_ACCESS_CREDENTIALS_KEY,
                  is_secure=True,
                  calling_format=OrdinaryCallingFormat())","conn = boto3.client('s3',
                    aws_access_key_id=S3_ACCESS_CREDENTIALS_USER,
                    aws_secret_access_key=S3_ACCESS_CREDENTIALS_KEY,
                    region_name=S3_REGION_NAME)",update,"CONN=connect_s3(aws_access_key_id=S3_ACCESS_CREDENTIALS_USER,aws_secret_access_key=S3_ACCESS_CREDENTIALS_KEY,is_secure=True,calling_format=OrdinaryCallingFormat())","conn=boto3.client('s3',aws_access_key_id=S3_ACCESS_CREDENTIALS_USER,aws_secret_access_key=S3_ACCESS_CREDENTIALS_KEY,region_name=S3_REGION_NAME)",libs/s3.py
heinzfutto/server-backend,d30c16ddc1446bbb052ac1896510ba96e1326327,boto3,boto,"def _get_bucket(name):
    """""" Gets a connection to a bucket, raises an S3ResponseError on failure. """"""
    return CONN.get_bucket(name)",,delete,"def_get_bucket(name):""""""Getsaconnectiontoabucket,raisesanS3ResponseErroronfailure.""""""returnCONN.get_bucket(name)",,deprecated/libs/s3_deprecated.py
heinzfutto/server-backend,d30c16ddc1446bbb052ac1896510ba96e1326327,boto3,boto,"def s3_upload(key_path, data_string, study_object_id, raw_path=False):
    """""" Uploads data to s3, ensures data is encrypted with the key from the provided study.
    Takes an optional argument, raw_path, which defaults to false.  When false the study_id is
    prepended to the S3 file path (key_path), placing the file in the appropriate study folder. """"""

    if not raw_path:
        key_path = study_object_id + ""/"" + key_path

    data = encryption.encrypt_for_server(data_string, study_object_id)
    key = _get_bucket(S3_BUCKET).new_key(key_path)
    key.set_contents_from_string(data)","def s3_upload(key_path, data_string, study_object_id, raw_path=False):




    if not raw_path:
        key_path = study_object_id + ""/"" + key_path

    data = encryption.encrypt_for_server(data_string, study_object_id)
    conn.put_object(Body=data, Bucket=S3_BUCKET, Key=key_path, ContentType='string')",update,"defs3_upload(key_path,data_string,study_object_id,raw_path=False):""""""Uploadsdatatos3,ensuresdataisencryptedwiththekeyfromtheprovidedstudy.Takesanoptionalargument,raw_path,whichdefaultstofalse.Whenfalsethestudy_idisprependedtotheS3filepath(key_path),placingthefileintheappropriatestudyfolder.""""""ifnotraw_path:key_path=study_object_id+""/""+key_pathdata=encryption.encrypt_for_server(data_string,study_object_id)key=_get_bucket(S3_BUCKET).new_key(key_path)key.set_contents_from_string(data)","defs3_upload(key_path,data_string,study_object_id,raw_path=False):ifnotraw_path:key_path=study_object_id+""/""+key_pathdata=encryption.encrypt_for_server(data_string,study_object_id)conn.put_object(Body=data,Bucket=S3_BUCKET,Key=key_path,ContentType='string')",libs/s3.py
heinzfutto/server-backend,d30c16ddc1446bbb052ac1896510ba96e1326327,boto3,boto,"def _do_retrieve(bucket_name, key_path, number_retries=DEFAULT_S3_RETRIES):
    """""" Run-logic to do a data retrieval for a file in an S3 bucket.""""""
    key = Key(_get_bucket(bucket_name), key_path)
    try:
        return key.read()
    except IncompleteRead:","def _do_retrieve(bucket_name, key_path, number_retries=DEFAULT_S3_RETRIES):
    """""" Run-logic to do a data retrieval for a file in an S3 bucket.""""""

    try:
        return conn.get_object(Bucket=bucket_name, Key=key_path, ResponseContentType='string')
    except Exception:",update,"def_do_retrieve(bucket_name,key_path,number_retries=DEFAULT_S3_RETRIES):""""""Run-logictodoadataretrievalforafileinanS3bucket.""""""key=Key(_get_bucket(bucket_name),key_path)try:returnkey.read()exceptIncompleteRead:","def_do_retrieve(bucket_name,key_path,number_retries=DEFAULT_S3_RETRIES):""""""Run-logictodoadataretrievalforafileinanS3bucket.""""""try:returnconn.get_object(Bucket=bucket_name,Key=key_path,ResponseContentType='string')exceptException:",deprecated/libs/s3_deprecated.py
stevencdang/aws,bced56db0fc2b1991fa13306c7cc0a0de4c274f6,boto3,boto,,import boto3,insert,,importboto3,
stevencdang/aws,bced56db0fc2b1991fa13306c7cc0a0de4c274f6,boto3,boto,"logger.debug('Access key is %s###' % cfg.access_key)
	aws = connection.Route53Connection(cfg.access_key,
					   cfg.secret_key,
					   debug=aws_log_level,
					   security_token=None)","logger.debug('Access key is %s' % cfg.access_key)
	
	aws = boto3.client('route53')",update,"logger.debug('Accesskeyis%s###'%cfg.access_key)aws=connection.Route53Connection(cfg.access_key,cfg.secret_key,debug=aws_log_level,security_token=None)",logger.debug('Accesskeyis%s'%cfg.access_key)aws=boto3.client('route53'),route53_dyndns/dyndns.py
stevencdang/aws,bced56db0fc2b1991fa13306c7cc0a0de4c274f6,boto3,boto,,"import socket, os, time, re
import urllib2
from aws.route53_dyndns import settings
from boto.route53 import connection, record
import logging

logger = logging.getLogger(__name__)
logger.setLevel(settings.log_level)

def get_connection():
	global cfg, logger
	#establish a connection to route53
	if settings.log_level is logging.DEBUG:
		aws_log_level = 2
	else:
		aws_log_level = 0	
	logger.debug('Access key is %s' % cfg.access_key)
	aws = connection.Route53Connection(cfg.access_key,
					   cfg.secret_key,
					   debug=aws_log_level,
					   security_token=None)
	return aws


def get_external_ip():
        # Get the ip address using a specific site
        data = urllib2.urlopen(""http://checkip.dyndns.org/"").read()
        # Search the result page for an ip address
	ip_search = re.compile('\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}')
	ip = ip_search.findall(data)
        # Return the result or log the error
	if ip:
		logger.info('found current ip to be:%s' % ip[0])
		ip = ip[0]
	else:
		logger.error('could not find external ip')
	return ip


def find_zone(zones, zone_name):
    for zone in zones:
        logger.debug('checking zone with ID %s and name %s\n \
                matching zone with name %s' % (zone.id, zone.name, zone_name))
        if zone.name == zone_name:
                logger.debug('found zone with ID %s and name %s' % (zone.id, zone.name))
                return zone	
        else:
            logger.debug(""Ignoring zone with name: %s"" % zone)


def define_update_record(zone_name, record_name, new_ip):
	global cfg
        # Connect to AWS
	aws = get_connection()
        # Get list of hosted zones and select the target to update base on parameters
	zones = aws.get_zones()
	logger.debug('Getting zone with name %s' % zone_name)
	zone = find_zone(zones, zone_name)
	logger.debug('Found zone matching name %s with id %s' % (zone.name, zone.id))
	logger.debug('modifying record with name %s' % record_name)
	# Retreiving current IP address of record
	current_rrsets = aws.get_all_rrsets(zone.id)
	current_ip = None
	for rrset in current_rrsets:
            if record_name in rrset.to_xml():
                current_ip = rrset.to_xml().split('<Value>')[1].split('</Value>')[0]
                logger.info(""Current IP address is: %s"" % current_ip)
	updates = record.ResourceRecordSets(aws, zone.id)
	if current_ip is not None:
            # Delete current record with old IP
            logger.info('existing record was found with ip %s, so adding delete before \
                creating updated record' % (current_ip))
            rrecord = updates.add_change('DELETE', 
                                         record_name,
                                         'A',
                                         300
                                         )
            rrecord.add_value(current_ip)
	rrecord = updates.add_change('CREATE', 
                                    record_name,
                                    'A',
                                    300
                                    )
	rrecord.add_value(new_ip)
	updates.commit()
        logger.info(""Completed update to new ip: %s"" % new_ip)


if __name__ == '__main__':
	cfg = settings.Settings()
	# Get info for updating AWS
	ip = get_external_ip()
	zone = cfg.zone_name
	record_name = cfg.record_name
	define_update_record(zone, record_name, ip)",insert,,"importsocket,os,time,reimporturllib2fromaws.route53_dyndnsimportsettingsfromboto.route53importconnection,recordimportlogginglogger=logging.getLogger(__name__)logger.setLevel(settings.log_level)defget_connection():globalcfg,logger#establishaconnectiontoroute53ifsettings.log_levelislogging.DEBUG:aws_log_level=2else:aws_log_level=0logger.debug('Accesskeyis%s'%cfg.access_key)aws=connection.Route53Connection(cfg.access_key,cfg.secret_key,debug=aws_log_level,security_token=None)returnawsdefget_external_ip():#Gettheipaddressusingaspecificsitedata=urllib2.urlopen(""http://checkip.dyndns.org/"").read()#Searchtheresultpageforanipaddressip_search=re.compile('\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}')ip=ip_search.findall(data)#Returntheresultorlogtheerrorifip:logger.info('foundcurrentiptobe:%s'%ip[0])ip=ip[0]else:logger.error('couldnotfindexternalip')returnipdeffind_zone(zones,zone_name):forzoneinzones:logger.debug('checkingzonewithID%sandname%s\n\matchingzonewithname%s'%(zone.id,zone.name,zone_name))ifzone.name==zone_name:logger.debug('foundzonewithID%sandname%s'%(zone.id,zone.name))returnzoneelse:logger.debug(""Ignoringzonewithname:%s""%zone)defdefine_update_record(zone_name,record_name,new_ip):globalcfg#ConnecttoAWSaws=get_connection()#Getlistofhostedzonesandselectthetargettoupdatebaseonparameterszones=aws.get_zones()logger.debug('Gettingzonewithname%s'%zone_name)zone=find_zone(zones,zone_name)logger.debug('Foundzonematchingname%swithid%s'%(zone.name,zone.id))logger.debug('modifyingrecordwithname%s'%record_name)#RetreivingcurrentIPaddressofrecordcurrent_rrsets=aws.get_all_rrsets(zone.id)current_ip=Noneforrrsetincurrent_rrsets:ifrecord_nameinrrset.to_xml():current_ip=rrset.to_xml().split('<Value>')[1].split('</Value>')[0]logger.info(""CurrentIPaddressis:%s""%current_ip)updates=record.ResourceRecordSets(aws,zone.id)ifcurrent_ipisnotNone:#DeletecurrentrecordwitholdIPlogger.info('existingrecordwasfoundwithip%s,soaddingdeletebefore\creatingupdatedrecord'%(current_ip))rrecord=updates.add_change('DELETE',record_name,'A',300)rrecord.add_value(current_ip)rrecord=updates.add_change('CREATE',record_name,'A',300)rrecord.add_value(new_ip)updates.commit()logger.info(""Completedupdatetonewip:%s""%new_ip)if__name__=='__main__':cfg=settings.Settings()#GetinfoforupdatingAWSip=get_external_ip()zone=cfg.zone_namerecord_name=cfg.record_namedefine_update_record(zone,record_name,ip)",route53_dyndns/dyndns_boto2.py
NixOS/nixops-aws,52bd308ddd0e0cd4eff8c3e4bfb5c13a9a1d7ba6,boto3,boto,"import boto
import boto.vpc",import boto3,update,importbotoimportboto.vpc,importboto3,nixops/resources/vpc.py
NixOS/nixops-aws,52bd308ddd0e0cd4eff8c3e4bfb5c13a9a1d7ba6,boto3,boto,"def connect(self):
        if self._conn: return
        assert self.region
        (access_key_id, secret_access_key) = nixops.ec2_utils.fetch_aws_secret_key(self.access_key_id)
        self._conn = boto.vpc.connect_to_region(
            region_name=self.region, aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key)","def connect(self):
        if self._client: return
        assert self.region
        (access_key_id, secret_access_key) = nixops.ec2_utils.fetch_aws_secret_key(self.access_key_id)
        self._client = boto3.client('ec2', region_name=self.region, aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key)",update,"defconnect(self):ifself._conn:returnassertself.region(access_key_id,secret_access_key)=nixops.ec2_utils.fetch_aws_secret_key(self.access_key_id)self._conn=boto.vpc.connect_to_region(region_name=self.region,aws_access_key_id=access_key_id,aws_secret_access_key=secret_access_key)","defconnect(self):ifself._client:returnassertself.region(access_key_id,secret_access_key)=nixops.ec2_utils.fetch_aws_secret_key(self.access_key_id)self._client=boto3.client('ec2',region_name=self.region,aws_access_key_id=access_key_id,aws_secret_access_key=secret_access_key)",nixops/resources/vpc.py
NixOS/nixops-aws,52bd308ddd0e0cd4eff8c3e4bfb5c13a9a1d7ba6,boto3,boto,"def _destroy(self):
        self.state = self.MISSING","def _destroy(self):
        if self.state != self.UP: return
        self.connect()
        self.log(""destroying VPC {0}..."".format(self.vpc_id))
        self._client.delete_vpc(VpcId=self.vpc_id)
        with self.depl._db:
            self.state = self.MISSING
            self.vpc_id = None
            self.region = None
            self.cidr_block = None
            self.instance_tenancy = None",update,def_destroy(self):self.state=self.MISSING,"def_destroy(self):ifself.state!=self.UP:returnself.connect()self.log(""destroyingVPC{0}..."".format(self.vpc_id))self._client.delete_vpc(VpcId=self.vpc_id)withself.depl._db:self.state=self.MISSINGself.vpc_id=Noneself.region=Noneself.cidr_block=Noneself.instance_tenancy=None",nixops/resources/vpc.py
NixOS/nixops-aws,52bd308ddd0e0cd4eff8c3e4bfb5c13a9a1d7ba6,boto3,boto,"def _destroy(self):
        self.state = self.MISSING










    def create(self, defn, check, allow_reboot, allow_recreate):
        self.access_key_id = defn.config['accessKeyId'] or nixops.ec2_utils.get_access_key_id()
        if not self.access_key_id:
            raise Exception(""please set 'accessKeyId', $EC2_ACCESS_KEY or $AWS_ACCESS_KEY_ID"")






        self.region = defn.config['region']
        instance_tenancy = True if defn.config['instanceTenancy'] == 'true' else False
        enable_dns_support = True if defn.config['enableDnsSupport'] == 'true' else False



        if self.state != self.UP:
            self.connect()
            self.log(""creating vpc"")
            vpc = self._conn.create_vpc(cidr_block=defn.config['cidrBlock'], instance_tenancy=instance_tenancy)
            print vpc
            print vpc.id


















        with self.depl._db:
           self.state = self.UP
           self.vpc_id = vpc.id
           self.region = defn.config['region']
           self.cidr_block = defn.config['cidrBlock'] 
           self.instance_tenancy = instance_tenancy
           self.classic_link_enabled = classic_link_enabled
           self.enable_dns_support = enable_dns_support
           self.enable_dns_hostnames = enable_dns_hostnames","def create(self, defn, check, allow_reboot, allow_recreate):
        self.access_key_id = defn.config['accessKeyId'] or nixops.ec2_utils.get_access_key_id()
        if not self.access_key_id:
            raise Exception(""please set 'accessKeyId', $EC2_ACCESS_KEY or $AWS_ACCESS_KEY_ID"")

        if self.state == self.UP and (self.cidr_block != defn.config['cidr_block'] or self.region != defn.config['region']):
            self.warn(""vpc definition changed, recreating..."")
            self._destroy()
            self._client = None

        self.region = defn.config['region']

        self.connect()

        vpc_id = self.vpc_id

        if self.state != self.UP:
            self.log(""creating vpc under region {0}"".format(defn.config['region']))
            vpc = self._client.create_vpc(CidrBlock=defn.config['cidrBlock'], InstanceTenancy=defn.config['instanceTenancy'])

            print vpc
            vpc_id = vpc.VpcId

        if defn.config['enableClassicLink']:
            self.log(""enabling vpc classic link"")
            self._client.enable_vpc_classic_link(VpcId=vpc_id)

        self._client.modify_vpc_attribute(VpcId=vpc_id, EnableDnsSupport=defn.config['enableDnsSupport'],
                    enableDnsHostnames=defn.config['enableDnsHostnames'])

        if defn.config['enableClassicLink']:
            self._client.enable_vpc_classic_link(VpcId=vpc_id)
        else:
            self._client.disable_vpc_classic_link(VpcId=vpc_id)

        def tag_updater(tags):
            self._client.create_tags(VpcId=vpc_id, Tags=[{""Key"": k, ""Value"": tags[k]} for k in tags])

        self.update_tags_using(tag_updater, user_tags=defn.config[""tags""], check=check)

        with self.depl._db:
           self.state = self.UP
           self.vpc_id = vpc_id
           self.region = defn.config['region']
           self.cidr_block = defn.config['cidrBlock']
           self.instance_tenancy = defn.config['instanceTenancy']",update,"def_destroy(self):self.state=self.MISSINGdefcreate(self,defn,check,allow_reboot,allow_recreate):self.access_key_id=defn.config['accessKeyId']ornixops.ec2_utils.get_access_key_id()ifnotself.access_key_id:raiseException(""pleaseset'accessKeyId',$EC2_ACCESS_KEYor$AWS_ACCESS_KEY_ID"")self.region=defn.config['region']instance_tenancy=Trueifdefn.config['instanceTenancy']=='true'elseFalseenable_dns_support=Trueifdefn.config['enableDnsSupport']=='true'elseFalseifself.state!=self.UP:self.connect()self.log(""creatingvpc"")vpc=self._conn.create_vpc(cidr_block=defn.config['cidrBlock'],instance_tenancy=instance_tenancy)printvpcprintvpc.idwithself.depl._db:self.state=self.UPself.vpc_id=vpc.idself.region=defn.config['region']self.cidr_block=defn.config['cidrBlock']self.instance_tenancy=instance_tenancyself.classic_link_enabled=classic_link_enabledself.enable_dns_support=enable_dns_supportself.enable_dns_hostnames=enable_dns_hostnames","defcreate(self,defn,check,allow_reboot,allow_recreate):self.access_key_id=defn.config['accessKeyId']ornixops.ec2_utils.get_access_key_id()ifnotself.access_key_id:raiseException(""pleaseset'accessKeyId',$EC2_ACCESS_KEYor$AWS_ACCESS_KEY_ID"")ifself.state==self.UPand(self.cidr_block!=defn.config['cidr_block']orself.region!=defn.config['region']):self.warn(""vpcdefinitionchanged,recreating..."")self._destroy()self._client=Noneself.region=defn.config['region']self.connect()vpc_id=self.vpc_idifself.state!=self.UP:self.log(""creatingvpcunderregion{0}"".format(defn.config['region']))vpc=self._client.create_vpc(CidrBlock=defn.config['cidrBlock'],InstanceTenancy=defn.config['instanceTenancy'])printvpcvpc_id=vpc.VpcIdifdefn.config['enableClassicLink']:self.log(""enablingvpcclassiclink"")self._client.enable_vpc_classic_link(VpcId=vpc_id)self._client.modify_vpc_attribute(VpcId=vpc_id,EnableDnsSupport=defn.config['enableDnsSupport'],enableDnsHostnames=defn.config['enableDnsHostnames'])ifdefn.config['enableClassicLink']:self._client.enable_vpc_classic_link(VpcId=vpc_id)else:self._client.disable_vpc_classic_link(VpcId=vpc_id)deftag_updater(tags):self._client.create_tags(VpcId=vpc_id,Tags=[{""Key"":k,""Value"":tags[k]}forkintags])self.update_tags_using(tag_updater,user_tags=defn.config[""tags""],check=check)withself.depl._db:self.state=self.UPself.vpc_id=vpc_idself.region=defn.config['region']self.cidr_block=defn.config['cidrBlock']self.instance_tenancy=defn.config['instanceTenancy']",nixops/resources/vpc.py
kennethjeremyau/snippets,8ecbf423acf64f91083d0d3502eb892807a763dc,boto3,boto,"from boto import dynamodb2
from boto.dynamodb2.table import Table",import boto3,update,frombotoimportdynamodb2fromboto.dynamodb2.tableimportTable,importboto3,
kennethjeremyau/snippets,8ecbf423acf64f91083d0d3502eb892807a763dc,boto3,boto,"table = Table('tablename', connection=dynamodb2.connect_to_region('us-east-1'))
for row in table.scan(segment=args.segment, total_segments=args.total_segments):
    print row['id']","dynamodb = boto3.client('dynamodb', region_name='us-east-1')
table = dynamodb.get_paginator('scan')

for page in table.paginate(
    TableName='tablename',
    Segment=args.segment,
    TotalSegments=args.total_segments
):
    for item in page['Items']:
        print str(item['id']['S'])",update,"table=Table('tablename',connection=dynamodb2.connect_to_region('us-east-1'))forrowintable.scan(segment=args.segment,total_segments=args.total_segments):printrow['id']","dynamodb=boto3.client('dynamodb',region_name='us-east-1')table=dynamodb.get_paginator('scan')forpageintable.paginate(TableName='tablename',Segment=args.segment,TotalSegments=args.total_segments):foriteminpage['Items']:printstr(item['id']['S'])",python/aws/dynamodb/scan.py
octavifs/horarispompeu,15518d6bfe63c81e2f31dd40f5f3add2e70cdb26,boto3,boto,"from boto.s3.connection import S3Connection
from boto.s3.key import Key",from boto3.session import Session,update,fromboto.s3.connectionimportS3Connectionfromboto.s3.keyimportKey,fromboto3.sessionimportSession,
octavifs/horarispompeu,15518d6bfe63c81e2f31dd40f5f3add2e70cdb26,boto3,boto,"if settings.S3_BACKUP:
            try:
                conn = S3Connection(settings.AWS_ACCESS_KEY,
                                    settings.AWS_SECRET_KEY)
                bucket = conn.get_bucket(settings.S3_BUCKET)
                database = Key(bucket)
                database.key = '/{}/horaris.sqlite'.format(now)
                database.set_contents_from_filename(
                    path.join(settings.BASE_DIR, 'resources/horaris.sqlite'))




                message += 'Uploaded DB to S3\n'
                private_settings = Key(bucket)
                private_settings.key = '/{}/settings_private.py'.format(now)
                private_settings.set_contents_from_filename(
                    path.join(settings.BASE_DIR,
                              'horarispompeu/settings_private.py'))

                message += 'Uploaded settings_private.py to S3\n'
                supervisord_config = Key(bucket)
                supervisord_config.key = '/{}/supervisord_horarispompeu.conf'.\
                    format(now)
                supervisord_config.set_contents_from_filename(
                    settings.SUPERVISORD_CONFIG)

                message += 'Uploaded supervisor conf to S3\n'
                nginx_config = Key(bucket)
                nginx_config.key = '/{}/nginx_horarispompeu'.format(now)
                nginx_config.set_contents_from_filename(settings.NGINX_CONFIG)","if settings.S3_BACKUP:
            try:
                s3 = Session(
                    aws_access_key_id=settings.AWS_ACCESS_KEY,
                    aws_secret_access_key=settings.AWS_SECRET_KEY,
                    region_name='eu-west-1'
                ).client('s3')
                # Upload the database
                s3.upload_file(
                    path.join(settings.BASE_DIR, 'resources/horaris.sqlite'),
                    settings.S3_BUCKET,
                    '/{}/horaris.sqlite'.format(now)
                )
                message += 'Uploaded DB to S3\n'
                # Upload private settings
                s3.upload_file(
                    path.join(settings.BASE_DIR, 'horarispompeu/settings_private.py'),
                    settings.S3_BUCKET,
                    '/{}/settings_private.py'.format(now)
                )
                message += 'Uploaded settings_private.py to S3\n'
                # Upload supervisord config
                s3.upload_file(
                    settings.SUPERVISORD_CONFIG,
                    settings.S3_BUCKET,
                    '/{}/supervisord_horarispompeu.conf'.format(now)
                )
                message += 'Uploaded supervisor conf to S3\n'
                # Upload nginx config
                s3.upload_file(
                    settings.NGINX_CONFIG,
                    settings.S3_BUCKET,
                    '/{}/nginx_horarispompeu'.format(now)
                )",update,"ifsettings.S3_BACKUP:try:conn=S3Connection(settings.AWS_ACCESS_KEY,settings.AWS_SECRET_KEY)bucket=conn.get_bucket(settings.S3_BUCKET)database=Key(bucket)database.key='/{}/horaris.sqlite'.format(now)database.set_contents_from_filename(path.join(settings.BASE_DIR,'resources/horaris.sqlite'))message+='UploadedDBtoS3\n'private_settings=Key(bucket)private_settings.key='/{}/settings_private.py'.format(now)private_settings.set_contents_from_filename(path.join(settings.BASE_DIR,'horarispompeu/settings_private.py'))message+='Uploadedsettings_private.pytoS3\n'supervisord_config=Key(bucket)supervisord_config.key='/{}/supervisord_horarispompeu.conf'.\format(now)supervisord_config.set_contents_from_filename(settings.SUPERVISORD_CONFIG)message+='UploadedsupervisorconftoS3\n'nginx_config=Key(bucket)nginx_config.key='/{}/nginx_horarispompeu'.format(now)nginx_config.set_contents_from_filename(settings.NGINX_CONFIG)","ifsettings.S3_BACKUP:try:s3=Session(aws_access_key_id=settings.AWS_ACCESS_KEY,aws_secret_access_key=settings.AWS_SECRET_KEY,region_name='eu-west-1').client('s3')#Uploadthedatabases3.upload_file(path.join(settings.BASE_DIR,'resources/horaris.sqlite'),settings.S3_BUCKET,'/{}/horaris.sqlite'.format(now))message+='UploadedDBtoS3\n'#Uploadprivatesettingss3.upload_file(path.join(settings.BASE_DIR,'horarispompeu/settings_private.py'),settings.S3_BUCKET,'/{}/settings_private.py'.format(now))message+='Uploadedsettings_private.pytoS3\n'#Uploadsupervisordconfigs3.upload_file(settings.SUPERVISORD_CONFIG,settings.S3_BUCKET,'/{}/supervisord_horarispompeu.conf'.format(now))message+='UploadedsupervisorconftoS3\n'#Uploadnginxconfigs3.upload_file(settings.NGINX_CONFIG,settings.S3_BUCKET,'/{}/nginx_horarispompeu'.format(now))",timetable/management/commands/backup.py
jpazdyga/tgdeployer,0bfd633bffc0e55b68b477444f4cab026ec7f5ba,boto3,boto,"import os
import boto.ec2.elb
from boto.ec2.elb import ELBConnection
from boto.ec2.elb import HealthCheck
from pprint import pprint",import boto3,update,importosimportboto.ec2.elbfromboto.ec2.elbimportELBConnectionfromboto.ec2.elbimportHealthCheckfrompprintimportpprint,importboto3,
jpazdyga/tgdeployer,0bfd633bffc0e55b68b477444f4cab026ec7f5ba,boto3,boto,conn = ELBConnection(),"conn = boto3.client('elb')
lbname = 'test1-loadbalancer';",update,conn=ELBConnection(),conn=boto3.client('elb')lbname='test1-loadbalancer';,create_load_balancer.py
jpazdyga/tgdeployer,0bfd633bffc0e55b68b477444f4cab026ec7f5ba,boto3,boto,"def create():






 zones = ['eu-west-1a','eu-west-1b','eu-west-1c']
 ports = [(80,80,'http')]
 security_groups = ['sg-de04e3ba']
 lb = conn.create_load_balancer('test1-loadbalancer', zones, ports, security_groups=security_groups)","def create():
 listeners = [
  {'Protocol': 'http',
   'LoadBalancerPort': 80,
   'InstanceProtocol': 'http',
   'InstancePort': 80,
  }];
 zones = ['eu-west-1a','eu-west-1b','eu-west-1c']

 security_groups = ['sg-de04e3ba']
 lb = conn.create_load_balancer(
  LoadBalancerName=lbname, 
  Listeners=listeners, 
  AvailabilityZones=zones,
  SecurityGroups=security_groups)",update,"defcreate():zones=['eu-west-1a','eu-west-1b','eu-west-1c']ports=[(80,80,'http')]security_groups=['sg-de04e3ba']lb=conn.create_load_balancer('test1-loadbalancer',zones,ports,security_groups=security_groups)","defcreate():listeners=[{'Protocol':'http','LoadBalancerPort':80,'InstanceProtocol':'http','InstancePort':80,}];zones=['eu-west-1a','eu-west-1b','eu-west-1c']security_groups=['sg-de04e3ba']lb=conn.create_load_balancer(LoadBalancerName=lbname,Listeners=listeners,AvailabilityZones=zones,SecurityGroups=security_groups)",create_load_balancer.py
geoadmin/mf-chsdi3,57ed173fe03ebdc1b3e734adb1481d026c001663,boto3,boto,"import boto.s3
import boto.dynamodb2
from boto.s3.connection import OrdinaryCallingFormat
from boto.dynamodb2.table import Table",import boto3,update,importboto.s3importboto.dynamodb2fromboto.s3.connectionimportOrdinaryCallingFormatfromboto.dynamodb2.tableimportTable,importboto3,chsdi/models/clientdata_dynamodb.py
geoadmin/mf-chsdi3,57ed173fe03ebdc1b3e734adb1481d026c001663,boto3,boto,"def get(self):
        if self.conn is None:
            try:
                self.conn = boto.dynamodb2.connect_to_region(self.region)
            except Exception as e:  # pragma: no cover
                raise exc.HTTPBadRequest(
                    'DynamoDB: Error during connection init %s' % e)
        return self.conn","def get(self):
        if self.conn is None:
            try:
                self.conn = boto3.resource('dynamodb', region_name=self.region)
            except Exception as e:  # pragma: no cover
                raise exc.HTTPBadRequest(
                    'DynamoDB: Error during connection init %s' % e)
        return self.conn",update,defget(self):ifself.connisNone:try:self.conn=boto.dynamodb2.connect_to_region(self.region)exceptExceptionase:#pragma:nocoverraiseexc.HTTPBadRequest('DynamoDB:Errorduringconnectioninit%s'%e)returnself.conn,"defget(self):ifself.connisNone:try:self.conn=boto3.resource('dynamodb',region_name=self.region)exceptExceptionase:#pragma:nocoverraiseexc.HTTPBadRequest('DynamoDB:Errorduringconnectioninit%s'%e)returnself.conn",chsdi/models/clientdata_dynamodb.py
geoadmin/mf-chsdi3,57ed173fe03ebdc1b3e734adb1481d026c001663,boto3,boto,"self.conn = boto.s3.connect_to_region(
                    self.region,
                    calling_format=OrdinaryCallingFormat())","self.conn = boto3.client('s3', region_name=self.region)",update,"self.conn=boto.s3.connect_to_region(self.region,calling_format=OrdinaryCallingFormat())","self.conn=boto3.client('s3',region_name=self.region)",chsdi/models/clientdata_dynamodb.py
geoadmin/mf-chsdi3,57ed173fe03ebdc1b3e734adb1481d026c001663,boto3,boto,,s3_connection = S3Connect(),insert,,s3_connection=S3Connect(),chsdi/models/clientdata_dynamodb.py
geoadmin/mf-chsdi3,57ed173fe03ebdc1b3e734adb1481d026c001663,boto3,boto,"def get_dynamodb_table(table_name='shorturl'):
    conn = dynamodb_connection.get()
    try:
        table = Table(table_name, connection=conn)
    except Exception as e:  # pragma: no cover
        raise exc.HTTPInternalServerError(
            'DynamoDB: Error during connection to the table %s\n%s' % (
                table_name, e))
    return table","def get_dynamodb_table(table_name='shorturl'):
    conn = dynamodb_connection.get()
    try:
        table = conn.Table(table_name)
    except Exception as e:  # pragma: no cover
        raise exc.HTTPInternalServerError(
            'DynamoDB: Error during connection to the table %s\n%s' % (
                table_name, e))
    return table",update,"defget_dynamodb_table(table_name='shorturl'):conn=dynamodb_connection.get()try:table=Table(table_name,connection=conn)exceptExceptionase:#pragma:nocoverraiseexc.HTTPInternalServerError('DynamoDB:Errorduringconnectiontothetable%s\n%s'%(table_name,e))returntable","defget_dynamodb_table(table_name='shorturl'):conn=dynamodb_connection.get()try:table=conn.Table(table_name)exceptExceptionase:#pragma:nocoverraiseexc.HTTPInternalServerError('DynamoDB:Errorduringconnectiontothetable%s\n%s'%(table_name,e))returntable",chsdi/models/clientdata_dynamodb.py
geoadmin/mf-chsdi3,57ed173fe03ebdc1b3e734adb1481d026c001663,boto3,boto,"def get_bucket(bucket_name):
    s3_connection = S3Connect()
    conn = s3_connection.get()
    try:
        bucket = conn.get_bucket(bucket_name)
    except Exception as e:  # pragma: no cover
        raise exc.HTTPInternalServerError(
            'S3 Error during connection to the bucket %s\n%s' % (
                bucket_name, e))
    return bucket","def get_file_from_bucket(bucket_name, file_name):

    conn = s3_connection.get()
    try:
        response = conn.get_object(Bucket=bucket_name,
                                   Key=file_name)
    except Exception as e:
        raise exc.HTTPInternalServerError(""bucket (%s) or file (%s) not valids. \n%s"" % (bucket_name, file_name, e))
    return response",update,"defget_bucket(bucket_name):s3_connection=S3Connect()conn=s3_connection.get()try:bucket=conn.get_bucket(bucket_name)exceptExceptionase:#pragma:nocoverraiseexc.HTTPInternalServerError('S3Errorduringconnectiontothebucket%s\n%s'%(bucket_name,e))returnbucket","defget_file_from_bucket(bucket_name,file_name):conn=s3_connection.get()try:response=conn.get_object(Bucket=bucket_name,Key=file_name)exceptExceptionase:raiseexc.HTTPInternalServerError(""bucket(%s)orfile(%s)notvalids.\n%s""%(bucket_name,file_name,e))returnresponse",chsdi/models/clientdata_dynamodb.py
geoadmin/mf-chsdi3,57ed173fe03ebdc1b3e734adb1481d026c001663,boto3,boto,,"def delete_file_in_bucket(bucket_name, file_name):
    return s3_connection.get().delete_object(Bucket=bucket_name,
                                  Key=file_name)


def upload_object_to_bucket(bucket_name, file_id, mime, content_encoding, data, cache_control, replace=False):
    if not replace:
        try:
            get_file_from_bucket(bucket_name, file_id)
            return None
        except Exception:
            pass
    return s3_connection.get().put_object(
        Body=data,
        Key=file_id,
        ContentType=mime,
        ContentEncoding=content_encoding,
        CacheControl=cache_control,
        Bucket=bucket_name
    )",insert,,"defdelete_file_in_bucket(bucket_name,file_name):returns3_connection.get().delete_object(Bucket=bucket_name,Key=file_name)defupload_object_to_bucket(bucket_name,file_id,mime,content_encoding,data,cache_control,replace=False):ifnotreplace:try:get_file_from_bucket(bucket_name,file_id)returnNoneexceptException:passreturns3_connection.get().put_object(Body=data,Key=file_id,ContentType=mime,ContentEncoding=content_encoding,CacheControl=cache_control,Bucket=bucket_name)",chsdi/models/clientdata_dynamodb.py
beanbaginc/cloudpuff,77e857ca1d1420412a651c35241051d4f319693d,boto3,boto,import boto.ec2,"import boto3

if TYPE_CHECKING:
    from mypy_boto3_ec2.client import EC2Client
    from mypy_boto3_ec2.literals import ImageStateType
    from mypy_boto3_ec2.type_defs import ImageTypeDef",update,importboto.ec2,importboto3ifTYPE_CHECKING:frommypy_boto3_ec2.clientimportEC2Clientfrommypy_boto3_ec2.literalsimportImageStateTypefrommypy_boto3_ec2.type_defsimportImageTypeDef,
beanbaginc/cloudpuff,77e857ca1d1420412a651c35241051d4f319693d,boto3,boto,self.cnx = boto.ec2.connect_to_region(region),"session = boto3.Session(
            profile_name=os.environ.get('CLOUDPUFF_AWS_PROFILE'))
        self.cnx = session.client('ec2', region_name=region)",update,self.cnx=boto.ec2.connect_to_region(region),"session=boto3.Session(profile_name=os.environ.get('CLOUDPUFF_AWS_PROFILE'))self.cnx=session.client('ec2',region_name=region)",cloudpuff/cloudformation.py
beanbaginc/cloudpuff,77e857ca1d1420412a651c35241051d4f319693d,boto3,boto," ami_id = self.cnx.create_image(instance_id, ami_name, ami_description)


        pending_ami = PendingAMI(creator=self,
                                 ami_id=ami_id)
        self.pending_amis.append(pending_ami)

        return pending_ami"," result = self.cnx.create_image(InstanceId=instance_id,
                                       Name=name,
                                       Description=description)
        pending_ami = PendingAMI(creator=self,
                                 ami_id=result['ImageId'])
        self.pending_amis.append(pending_ami)

        return pending_ami",update,"ami_id=self.cnx.create_image(instance_id,ami_name,ami_description)pending_ami=PendingAMI(creator=self,ami_id=ami_id)self.pending_amis.append(pending_ami)returnpending_ami","result=self.cnx.create_image(InstanceId=instance_id,Name=name,Description=description)pending_ami=PendingAMI(creator=self,ami_id=result['ImageId'])self.pending_amis.append(pending_ami)returnpending_ami",cloudpuff/ami.py
elifesciences/builder,337fac52329b8bb804bec2d5a0ab60225abe1b4d,boto3,boto,"def _current_cloudformation_template(stackname):
    ""retrieves a template from the CloudFormation API, using it as the source of truth""
    conn = core.connect_aws_with_stack(stackname, 'cfn')
    return json.loads(conn.get_template(stackname)['GetTemplateResponse']['GetTemplateResult']['TemplateBody'])","def _current_cloudformation_template(stackname):
    ""retrieves a template from the CloudFormation API, using it as the source of truth""
    cfn = core.boto_conn(stackname, 'cloudformation', client=True)
    return cfn.get_template(StackName=stackname)['TemplateBody']",update,"def_current_cloudformation_template(stackname):""retrievesatemplatefromtheCloudFormationAPI,usingitasthesourceoftruth""conn=core.connect_aws_with_stack(stackname,'cfn')returnjson.loads(conn.get_template(stackname)['GetTemplateResponse']['GetTemplateResult']['TemplateBody'])","def_current_cloudformation_template(stackname):""retrievesatemplatefromtheCloudFormationAPI,usingitasthesourceoftruth""cfn=core.boto_conn(stackname,'cloudformation',client=True)returncfn.get_template(StackName=stackname)['TemplateBody']",src/buildercore/cfngen.py
elifesciences/builder,bb4f0b99134d331430f77c54d007337d703f9ee9,boto3,boto,"import boto
from boto import s3
from boto.s3.key import Key","import boto3
from botocore.exceptions import ClientError
",update,importbotofrombotoimports3fromboto.s3.keyimportKey,importboto3frombotocore.exceptionsimportClientError,
elifesciences/builder,bb4f0b99134d331430f77c54d007337d703f9ee9,boto3,boto,"def connect_s3():
    # we'll need to deal with this assumption
    return s3.connect_to_region('us-east-1')  # Location.USWest2)","def connect_s3(region='us-east-1'):
    return boto3.resource('s3', region_name=region)
",update,defconnect_s3():#we'llneedtodealwiththisassumptionreturns3.connect_to_region('us-east-1')#Location.USWest2),"defconnect_s3(region='us-east-1'):returnboto3.resource('s3',region_name=region)",src/buildercore/s3.py
elifesciences/builder,bb4f0b99134d331430f77c54d007337d703f9ee9,boto3,boto,"def builder_bucket():

    try:
        nom = config.BUILDER_BUCKET
        return connect_s3().get_bucket(nom)
    except boto.exception.S3ResponseError as err:
        LOG.error(""got an S3 error attempting to get the builder bucket. have you created it yet?"",
                  extra={'bucket': nom, 'error': err.message})



        raise","def builder_bucket():
    ""returns connection to the bucket where builder stores templates and credentials.""
    try:
        nom, region = config.BUILDER_BUCKET
        resource = connect_s3(region)
        bucket = resource.Bucket(nom)
        ensure(bucket in resource.buckets.all(), ""bucket %r in region %r does not exist"" % (nom, region))
        return bucket
    except ClientError as err:
        LOG.error(""unhandled error attempting to find S3 bucket %r in region %r"", nom, region,
                  extra={'bucket': nom, 'region': region, 'error': err.message})
        raise",update,"defbuilder_bucket():try:nom=config.BUILDER_BUCKETreturnconnect_s3().get_bucket(nom)exceptboto.exception.S3ResponseErroraserr:LOG.error(""gotanS3errorattemptingtogetthebuilderbucket.haveyoucreatedityet?"",extra={'bucket':nom,'error':err.message})raise","defbuilder_bucket():""returnsconnectiontothebucketwherebuilderstorestemplatesandcredentials.""try:nom,region=config.BUILDER_BUCKETresource=connect_s3(region)bucket=resource.Bucket(nom)ensure(bucketinresource.buckets.all(),""bucket%rinregion%rdoesnotexist""%(nom,region))returnbucketexceptClientErroraserr:LOG.error(""unhandlederrorattemptingtofindS3bucket%rinregion%r"",nom,region,extra={'bucket':nom,'region':region,'error':err.message})raise",src/buildercore/s3.py
elifesciences/builder,bb4f0b99134d331430f77c54d007337d703f9ee9,boto3,boto,"def exists(key):
    return builder_bucket().get_key(key) is not None","def exists(key):
    ""predicate, returns True if given key in configured bucket+region exists""
    try:
        builder_bucket().Object(key).load()
        return True
    except ClientError as err:
        if err.response['Error']['Code'] == '404':
            return False
        raise",update,defexists(key):returnbuilder_bucket().get_key(key)isnotNone,"defexists(key):""predicate,returnsTrueifgivenkeyinconfiguredbucket+regionexists""try:builder_bucket().Object(key).load()returnTrueexceptClientErroraserr:iferr.response['Error']['Code']=='404':returnFalseraise",src/buildercore/s3.py
xethorn/garcon,e5ba08f4c99f88312b667aa8a68d9d6953759fad,boto3,boto,import boto.swf.layer2 as swf,import boto3,update,importboto.swf.layer2asswf,importboto3,example/custom_decider/run.py
xethorn/garcon,e5ba08f4c99f88312b667aa8a68d9d6953759fad,boto3,boto,"flow = workflow.Workflow('dev', 'custom_decider')
deciderworker = decider.DeciderWorker(flow)","client = boto3.client('swf', region_name='us-east-1')
workflow = workflow.Workflow(client, 'dev', 'custom_decider')
deciderworker = decider.DeciderWorker(workflow)

client.start_workflow_execution(
    domain=workflow.domain,
    workflowId='unique-workflow-identifier',
    workflowType=dict(
        name=workflow.name,
        version='1.0'),
    executionStartToCloseTimeout='3600',
    taskStartToCloseTimeout='3600',
    childPolicy='TERMINATE',
    taskList=dict(name=workflow.name))

Thread(target=activity.ActivityWorker(workflow).run).start()",update,"flow=workflow.Workflow('dev','custom_decider')deciderworker=decider.DeciderWorker(flow)","client=boto3.client('swf',region_name='us-east-1')workflow=workflow.Workflow(client,'dev','custom_decider')deciderworker=decider.DeciderWorker(workflow)client.start_workflow_execution(domain=workflow.domain,workflowId='unique-workflow-identifier',workflowType=dict(name=workflow.name,version='1.0'),executionStartToCloseTimeout='3600',taskStartToCloseTimeout='3600',childPolicy='TERMINATE',taskList=dict(name=workflow.name))Thread(target=activity.ActivityWorker(workflow).run).start()",example/custom_decider/run.py
xethorn/garcon,e5ba08f4c99f88312b667aa8a68d9d6953759fad,boto3,boto,"deciderworker = decider.DeciderWorker(test_flow)


swf.WorkflowType(
    name=test_flow.name, domain=test_flow.domain,
    version='1.0', task_list=test_flow.name).start()





Thread(target=activity.ActivityWorker(test_flow).run).start()","client = boto3.client('swf', region_name='us-east-1')
deciderworker = decider.DeciderWorker(client, workflow)

client.start_workflow_execution(
    domain=workflow.domain,
    workflowId='unique-workflow-identifier',
    workflowType=dict(
        name=workflow.name,
        version='1.0'),
    taskList=dict(name=workflow.name))

Thread(target=activity.ActivityWorker(client, workflow).run).start()",update,"deciderworker=decider.DeciderWorker(test_flow)swf.WorkflowType(name=test_flow.name,domain=test_flow.domain,version='1.0',task_list=test_flow.name).start()Thread(target=activity.ActivityWorker(test_flow).run).start()","client=boto3.client('swf',region_name='us-east-1')deciderworker=decider.DeciderWorker(client,workflow)client.start_workflow_execution(domain=workflow.domain,workflowId='unique-workflow-identifier',workflowType=dict(name=workflow.name,version='1.0'),taskList=dict(name=workflow.name))Thread(target=activity.ActivityWorker(client,workflow).run).start()",example/simple/run.py
xethorn/garcon,e5ba08f4c99f88312b667aa8a68d9d6953759fad,boto3,boto,import boto.swf.layer2 as swf,import boto3,update,importboto.swf.layer2asswf,importboto3,README.rst
xethorn/garcon,e5ba08f4c99f88312b667aa8a68d9d6953759fad,boto3,boto,"logger = logging.getLogger(__name__)

domain = 'dev'
name = 'workflow_sample'
create = activity.create(domain, name)","logger = logging.getLogger(__name__)
client = boto3.client('swf', region_name='us-east-1')
domain = 'dev'
name = 'workflow_sample'
create = activity.create(client, domain, name)",update,"logger=logging.getLogger(__name__)domain='dev'name='workflow_sample'create=activity.create(domain,name)","logger=logging.getLogger(__name__)client=boto3.client('swf',region_name='us-east-1')domain='dev'name='workflow_sample'create=activity.create(client,domain,name)",example/simple/workflow.py
xethorn/garcon,e5ba08f4c99f88312b667aa8a68d9d6953759fad,boto3,boto,"import backoff
import boto.exception as boto_exception
import boto.swf.layer2 as swf",from botocore import exceptions,update,importbackoffimportboto.exceptionasboto_exceptionimportboto.swf.layer2asswf,frombotocoreimportexceptions,garcon/activity.py
xethorn/garcon,e5ba08f4c99f88312b667aa8a68d9d6953759fad,boto3,boto,"@backoff.on_exception(
        backoff.expo,
        boto_exception.SWFResponseError,
        max_tries=5,
        giveup=utils.non_throttle_error,
        on_backoff=utils.throttle_backoff_handler,","@backoff.on_exception(
        backoff.expo,
        exceptions.ClientError,
        max_tries=5,
        giveup=utils.non_throttle_error,
        on_backoff=utils.throttle_backoff_handler,",update,"@backoff.on_exception(backoff.expo,boto_exception.SWFResponseError,max_tries=5,giveup=utils.non_throttle_error,on_backoff=utils.throttle_backoff_handler,","@backoff.on_exception(backoff.expo,exceptions.ClientError,max_tries=5,giveup=utils.non_throttle_error,on_backoff=utils.throttle_backoff_handler,",garcon/activity.py
xethorn/garcon,e5ba08f4c99f88312b667aa8a68d9d6953759fad,boto3,boto,,"
    from unittest.mock import MagicMock
except:
    from mock import MagicMock

import pytest
import boto3


@pytest.fixture
def boto_client(monkeypatch):
    """"""Create a fake boto client.""""""
    return MagicMock(spec=boto3.client('swf', region_name='us-east-1'))",insert,,"fromunittest.mockimportMagicMockexcept:frommockimportMagicMockimportpytestimportboto3@pytest.fixturedefboto_client(monkeypatch):""""""Createafakebotoclient.""""""returnMagicMock(spec=boto3.client('swf',region_name='us-east-1'))",tests/conftest.py
xethorn/garcon,e5ba08f4c99f88312b667aa8a68d9d6953759fad,boto3,boto,"from garcon import activity
from garcon import runner




domain = 'dev'
name = 'workflow_name'
create = activity.create(domain, name)","from garcon import activity
from garcon import runner

import boto3

client = boto3.client('swf', region_name='us-east-1')
domain = 'dev'
name = 'workflow_name'
create = activity.create(client, domain, name)",update,"fromgarconimportactivityfromgarconimportrunnerdomain='dev'name='workflow_name'create=activity.create(domain,name)","fromgarconimportactivityfromgarconimportrunnerimportboto3client=boto3.client('swf',region_name='us-east-1')domain='dev'name='workflow_name'create=activity.create(client,domain,name)",tests/fixtures/flows/example.py
thinhpham/aws-tools,24e9cd06f56f4b825a9e22b1974e573b34edb8b9,boto3,boto,"import botoprint 'Security group information:\n'ec2 = boto.connect_ec2()sgs = ec2.get_all_security_groups()for sg in sgs:    instances = sg.instances()    print 'id: %s, name: %s, count: %s' % (sg.id, sg.name, len(instances))    for inst in instances:        tag_name = 'UNKNOWN'        if inst.tags is not None and 'Name' in inst.tags:            tag_name = inst.tags['Name']        print '\tid: %s, name: %s' % (inst.id, tag_name)","import boto3

print 'Security group information:\n'

ec2 = boto3.resource('ec2')
sgs = ec2.security_groups.all()

for sg in sgs:
    tag_name = sg.group_name
    if sg.tags is not None:
        for tag in sg.tags:
            if tag['Key'] == 'Name' and tag['Value'] != '':
                tag_name = tag['Value']

    print 'id: %s, name: %s, vpc_id: %s' % (sg.id, tag_name, sg.vpc_id)",update,"importbotoprint'Securitygroupinformation:\n'ec2=boto.connect_ec2()sgs=ec2.get_all_security_groups()forsginsgs:instances=sg.instances()print'id:%s,name:%s,count:%s'%(sg.id,sg.name,len(instances))forinstininstances:tag_name='UNKNOWN'ifinst.tagsisnotNoneand'Name'ininst.tags:tag_name=inst.tags['Name']print'\tid:%s,name:%s'%(inst.id,tag_name)","importboto3print'Securitygroupinformation:\n'ec2=boto3.resource('ec2')sgs=ec2.security_groups.all()forsginsgs:tag_name=sg.group_nameifsg.tagsisnotNone:fortaginsg.tags:iftag['Key']=='Name'andtag['Value']!='':tag_name=tag['Value']print'id:%s,name:%s,vpc_id:%s'%(sg.id,tag_name,sg.vpc_id)",ec2_security_group_list.py
zalando-zmon/zmon-aws-agent,2e2fb2ee2c51528ce95e09ce78e2ba2fa078c5e2,boto3,boto,"import boto.iam
import boto.ec2
import boto.ec2.autoscale
import boto.ec2.elb
import boto.ec2.instance
import boto.cloudformation
import boto.utils
import boto.rds2",import boto3,update,importboto.iamimportboto.ec2importboto.ec2.autoscaleimportboto.ec2.elbimportboto.ec2.instanceimportboto.cloudformationimportboto.utilsimportboto.rds2,importboto3,zmon-agent.py
zalando-zmon/zmon-aws-agent,2e2fb2ee2c51528ce95e09ce78e2ba2fa078c5e2,boto3,boto,"def get_running_apps(region):
    aws = boto.ec2.connect_to_region(region)
    rs = aws.get_all_reservations()
    result = []

    for r in rs:

        owner = r.owner_id

        instances = r.instances

        for i in instances:

            if str(i._state) != 'running(16)':
                continue

            try:
                user_data = base64.b64decode(str(i.get_attribute('userData')[""userData""]))
                user_data = yaml.load(user_data)


            except:
                pass","def get_running_apps(region):
    aws_client = boto3.client('ec2')
    rs = aws_client.describe_instances()['Reservations']
    result = []

    for r in rs:

        owner = r['OwnerId']

        instances = r['Instances']

        for i in instances:

            if str(i['State']['Name']) != 'running':
                continue

            try:
                user_data_response = aws_client.describe_instance_attribute(InstanceId=i['InstanceId'],
                                                                            Attribute='userData')
                user_data = base64.b64decode(user_data_response['UserData']['Value'])
                user_data = yaml.safe_load(user_data)
            except:
                pass

            tags = {}
            if i['Tags']:
                for t in i['Tags']:
                    tags[t['Key']] = t['Value']

            ins = {
                'type': 'instance',
                'created_by': 'agent',
                'region': region,
                'ip': i['PrivateIpAddress'],
                'host': i['PrivateIpAddress'],
                'instance_type': i['InstanceType'],
                'aws_id': i['InstanceId'],
                'infrastructure_account': 'aws:{}'.format(owner),
            }",update,"defget_running_apps(region):aws=boto.ec2.connect_to_region(region)rs=aws.get_all_reservations()result=[]forrinrs:owner=r.owner_idinstances=r.instancesforiininstances:ifstr(i._state)!='running(16)':continuetry:user_data=base64.b64decode(str(i.get_attribute('userData')[""userData""]))user_data=yaml.load(user_data)except:pass","defget_running_apps(region):aws_client=boto3.client('ec2')rs=aws_client.describe_instances()['Reservations']result=[]forrinrs:owner=r['OwnerId']instances=r['Instances']foriininstances:ifstr(i['State']['Name'])!='running':continuetry:user_data_response=aws_client.describe_instance_attribute(InstanceId=i['InstanceId'],Attribute='userData')user_data=base64.b64decode(user_data_response['UserData']['Value'])user_data=yaml.safe_load(user_data)except:passtags={}ifi['Tags']:fortini['Tags']:tags[t['Key']]=t['Value']ins={'type':'instance','created_by':'agent','region':region,'ip':i['PrivateIpAddress'],'host':i['PrivateIpAddress'],'instance_type':i['InstanceType'],'aws_id':i['InstanceId'],'infrastructure_account':'aws:{}'.format(owner),}",zmon-agent.py
zalando-zmon/zmon-aws-agent,2e2fb2ee2c51528ce95e09ce78e2ba2fa078c5e2,boto3,boto,"def get_auto_scaling_groups(region, acc):
    groups = []

    ec2 = boto.ec2.connect_to_region(region)
    autoscale = boto.ec2.autoscale.connect_to_region(region)
    for g in autoscale.get_all_groups():
        sg = {'type':'asg', 'infrastructure_account':acc, 'region': region, 'created_by':'agent'}
        sg['id'] = 'asg-{}[{}:{}]'.format(g.name, acc, region)
        sg['name'] = g.name
        sg['availability_zones'] = g.availability_zones
        sg['desired_capacity'] = g.desired_capacity
        sg['max_size'] = g.max_size
        sg['min_size'] = g.min_size

        stack_name_tag = [t for t in g.tags if t.key=='StackName']
        if stack_name_tag:
            sg['stack_name_tag'] = stack_name_tag[0].value

        instance_ids = [i.instance_id for i in g.instances]
        instances = ec2.get_only_instances(instance_ids)
        sg['instances'] = [{'aws_id': i.id, 'ip': i.private_ip_address} for i in instances]","def get_auto_scaling_groups(region, acc):
    groups = []
    as_client = boto3.client('autoscaling')
    ec2_client = boto3.client('ec2')
    asgs = as_client.describe_auto_scaling_groups()['AutoScalingGroups']
    for g in asgs:
        sg = {'type': 'asg', 'infrastructure_account': acc, 'region': region, 'created_by': 'agent'}
        sg['id'] = 'asg-{}[{}:{}]'.format(g['AutoScalingGroupName'], acc, region)
        sg['name'] = g['AutoScalingGroupName']
        sg['availability_zones'] = g['AvailabilityZones']
        sg['desired_capacity'] = g['DesiredCapacity']
        sg['max_size'] = g['MaxSize']
        sg['min_size'] = g['MinSize']

        stack_name_tag = [t for t in g['Tags'] if t['Key'] == 'StackName']
        if stack_name_tag:
            sg['stack_name_tag'] = stack_name_tag[0]['Value']

        instance_ids = [i['InstanceId'] for i in g['Instances']]
        reservations = ec2_client.describe_instances(InstanceIds=instance_ids)['Reservations']
        sg['instances'] = []
        for r in reservations:
            for i in r['Instances']:
                sg['instances'].append({
                    'aws_id': i['InstanceId'],
                    'ip': i['PrivateIpAddress'],
                })
        groups.append(sg)

    return groups",update,"defget_auto_scaling_groups(region,acc):groups=[]ec2=boto.ec2.connect_to_region(region)autoscale=boto.ec2.autoscale.connect_to_region(region)forginautoscale.get_all_groups():sg={'type':'asg','infrastructure_account':acc,'region':region,'created_by':'agent'}sg['id']='asg-{}[{}:{}]'.format(g.name,acc,region)sg['name']=g.namesg['availability_zones']=g.availability_zonessg['desired_capacity']=g.desired_capacitysg['max_size']=g.max_sizesg['min_size']=g.min_sizestack_name_tag=[tforting.tagsift.key=='StackName']ifstack_name_tag:sg['stack_name_tag']=stack_name_tag[0].valueinstance_ids=[i.instance_idforiing.instances]instances=ec2.get_only_instances(instance_ids)sg['instances']=[{'aws_id':i.id,'ip':i.private_ip_address}foriininstances]","defget_auto_scaling_groups(region,acc):groups=[]as_client=boto3.client('autoscaling')ec2_client=boto3.client('ec2')asgs=as_client.describe_auto_scaling_groups()['AutoScalingGroups']forginasgs:sg={'type':'asg','infrastructure_account':acc,'region':region,'created_by':'agent'}sg['id']='asg-{}[{}:{}]'.format(g['AutoScalingGroupName'],acc,region)sg['name']=g['AutoScalingGroupName']sg['availability_zones']=g['AvailabilityZones']sg['desired_capacity']=g['DesiredCapacity']sg['max_size']=g['MaxSize']sg['min_size']=g['MinSize']stack_name_tag=[tforting['Tags']ift['Key']=='StackName']ifstack_name_tag:sg['stack_name_tag']=stack_name_tag[0]['Value']instance_ids=[i['InstanceId']foriing['Instances']]reservations=ec2_client.describe_instances(InstanceIds=instance_ids)['Reservations']sg['instances']=[]forrinreservations:foriinr['Instances']:sg['instances'].append({'aws_id':i['InstanceId'],'ip':i['PrivateIpAddress'],})groups.append(sg)returngroups",zmon-agent.py
zalando-zmon/zmon-aws-agent,2e2fb2ee2c51528ce95e09ce78e2ba2fa078c5e2,boto3,boto,"def get_account_alias(region):
    try:
        c = boto.iam.connect_to_region(region)
        resp = c.get_account_alias()
        return resp['list_account_aliases_response']['list_account_aliases_result']['account_aliases'][0]
    except:
        return None","def get_account_alias(region):
    try:
        iam_client = boto3.client('iam')
        resp = iam_client.list_account_aliases()
        return resp['AccountAliases'][0]
    except:
        return None",update,defget_account_alias(region):try:c=boto.iam.connect_to_region(region)resp=c.get_account_alias()returnresp['list_account_aliases_response']['list_account_aliases_result']['account_aliases'][0]except:returnNone,defget_account_alias(region):try:iam_client=boto3.client('iam')resp=iam_client.list_account_aliases()returnresp['AccountAliases'][0]except:returnNone,zmon-agent.py
zalando-zmon/zmon-aws-agent,2e2fb2ee2c51528ce95e09ce78e2ba2fa078c5e2,boto3,boto,"try:
        aws = boto.rds2.connect_to_region(region)
        instances = aws.describe_db_instances()
        for i in instances[""DescribeDBInstancesResponse""][""DescribeDBInstancesResult""][""DBInstances""]:","try:
        rds_client = boto3.client('rds')
        instances = rds_client.describe_db_instances()

        for i in instances[""DBInstances""]:",update,"try:aws=boto.rds2.connect_to_region(region)instances=aws.describe_db_instances()foriininstances[""DescribeDBInstancesResponse""][""DescribeDBInstancesResult""][""DBInstances""]:","try:rds_client=boto3.client('rds')instances=rds_client.describe_db_instances()foriininstances[""DBInstances""]:",zmon-agent.py
zalando-zmon/zmon-aws-agent,2e2fb2ee2c51528ce95e09ce78e2ba2fa078c5e2,boto3,boto,"if not args.region:
        logging.info(""Trying to figure out region..."")
        region = boto.utils.get_instance_metadata()['placement']['availability-zone'][:-1]","if not args.region:
        logging.info(""Trying to figure out region..."")
        try:
            response = requests.get('http://169.254.169.254/latest/meta-data/placement/availability-zone', timeout=2)
        except:
            logging.error(""Region was not specified as a parameter and can not be fetched from instance meta-data!"")
            raise
        region = response.text[:-1]",update,"ifnotargs.region:logging.info(""Tryingtofigureoutregion..."")region=boto.utils.get_instance_metadata()['placement']['availability-zone'][:-1]","ifnotargs.region:logging.info(""Tryingtofigureoutregion..."")try:response=requests.get('http://169.254.169.254/latest/meta-data/placement/availability-zone',timeout=2)except:logging.error(""Regionwasnotspecifiedasaparameterandcannotbefetchedfrominstancemeta-data!"")raiseregion=response.text[:-1]",zmon-agent.py
cloudtools/stacker,3604de9e4e67ebd572941ce9ccc3b2e25cb28908,boto3,boto,"from boto.regioninfo import get_regions
from boto.ec2containerservice.layer1 import EC2ContainerServiceConnection",import boto3,update,fromboto.regioninfoimportget_regionsfromboto.ec2containerservice.layer1importEC2ContainerServiceConnection,importboto3,stacker/tests/hooks/test_ecs.py
cloudtools/stacker,3604de9e4e67ebd572941ce9ccc3b2e25cb28908,boto3,boto,"def regions():
    return get_regions(""ec2containerservice"",
                       connection_cls=EC2ContainerServiceConnection)


def connect_to_region(region_name, **kw_params):
    for region in regions():
        if region.name == region_name:
            return region.connect(**kw_params)
    return None",,delete,"defregions():returnget_regions(""ec2containerservice"",connection_cls=EC2ContainerServiceConnection)defconnect_to_region(region_name,**kw_params):forregioninregions():ifregion.name==region_name:returnregion.connect(**kw_params)returnNone",,
cloudtools/stacker,3604de9e4e67ebd572941ce9ccc3b2e25cb28908,boto3,boto,"def create_clusters(region, namespace, mappings, parameters, **kwargs):
    """"""Creates ECS clusters.

    Expects a ""clusters"" argument, which should contain a list of cluster
    names to create.

    """"""
    conn = connect_to_region(region)

    try:
        clusters = kwargs[""clusters""]
    except KeyError:
        logger.error(""setup_clusters hook missing \""clusters\"" argument"")
        return False

    if isinstance(clusters, basestring):
        clusters = [clusters]

    for cluster in clusters:
        logger.debug(""Creating ECS cluster: %s"", cluster)
        conn.create_cluster(cluster)
    return True","def create_clusters(region, namespace, mappings, parameters, **kwargs):
    """"""Creates ECS clusters.

    Expects a ""clusters"" argument, which should contain a list of cluster
    names to create.

    """"""
    conn = boto3.client(""ecs"", region_name=region)

    try:
        clusters = kwargs[""clusters""]
    except KeyError:
        logger.error(""setup_clusters hook missing \""clusters\"" argument"")
        return False

    if isinstance(clusters, basestring):
        clusters = [clusters]

    for cluster in clusters:
        logger.debug(""Creating ECS cluster: %s"", cluster)
        conn.create_cluster(clusterName=cluster)
    return True",update,"defcreate_clusters(region,namespace,mappings,parameters,**kwargs):""""""CreatesECSclusters.Expectsa""clusters""argument,whichshouldcontainalistofclusternamestocreate.""""""conn=connect_to_region(region)try:clusters=kwargs[""clusters""]exceptKeyError:logger.error(""setup_clustershookmissing\""clusters\""argument"")returnFalseifisinstance(clusters,basestring):clusters=[clusters]forclusterinclusters:logger.debug(""CreatingECScluster:%s"",cluster)conn.create_cluster(cluster)returnTrue","defcreate_clusters(region,namespace,mappings,parameters,**kwargs):""""""CreatesECSclusters.Expectsa""clusters""argument,whichshouldcontainalistofclusternamestocreate.""""""conn=boto3.client(""ecs"",region_name=region)try:clusters=kwargs[""clusters""]exceptKeyError:logger.error(""setup_clustershookmissing\""clusters\""argument"")returnFalseifisinstance(clusters,basestring):clusters=[clusters]forclusterinclusters:logger.debug(""CreatingECScluster:%s"",cluster)conn.create_cluster(clusterName=cluster)returnTrue",stacker/hooks/ecs.py
cloudtools/stacker,3604de9e4e67ebd572941ce9ccc3b2e25cb28908,boto3,boto,"from boto.exception import BotoServerError
from boto.iam import connect_to_region","import boto3
from botocore.exceptions import ClientError",update,fromboto.exceptionimportBotoServerErrorfromboto.iamimportconnect_to_region,importboto3frombotocore.exceptionsimportClientError,stacker/hooks/iam.py
cloudtools/stacker,3604de9e4e67ebd572941ce9ccc3b2e25cb28908,boto3,boto,conn = connect_to_region(region),"role_name = kwargs.get(""role_name"", ""ecsServiceRole"")
    client = boto3.client(""iam"", region_name=region)

    try:
        client.create_role(
            RoleName=role_name,
            AssumeRolePolicyDocument=get_ecs_assumerole_policy().to_json()
        )
    except ClientError as e:
        if ""already exists"" in e.message:
            pass
        else:
            raise",update,conn=connect_to_region(region),"role_name=kwargs.get(""role_name"",""ecsServiceRole"")client=boto3.client(""iam"",region_name=region)try:client.create_role(RoleName=role_name,AssumeRolePolicyDocument=get_ecs_assumerole_policy().to_json())exceptClientErrorase:if""alreadyexists""ine.message:passelse:raise",stacker/hooks/iam.py
cloudtools/stacker,3604de9e4e67ebd572941ce9ccc3b2e25cb28908,boto3,boto,"policy = Policy(
        Statement=[
            Statement(
                Effect=Allow,
                Resource=[""*""],
                Action=[ecs.CreateCluster, ecs.DeregisterContainerInstance,
                        ecs.DiscoverPollEndpoint, ecs.Poll,
                        ecs.ECSAction(""Submit*"")]
            )
        ])
    conn.put_role_policy(""ecsServiceRole"", ""AmazonEC2ContainerServiceRole"",
                         policy.to_json())","policy = Policy(
        Statement=[
            Statement(
                Effect=Allow,
                Resource=[""*""],
                Action=[ecs.CreateCluster, ecs.DeregisterContainerInstance,
                        ecs.DiscoverPollEndpoint, ecs.Poll,
                        ecs.Action(""Submit*"")]
            )
        ])
    client.put_role_policy(
        RoleName=role_name,
        PolicyName=""AmazonEC2ContainerServiceRolePolicy"",
        PolicyDocument=policy.to_json()
    )",update,"policy=Policy(Statement=[Statement(Effect=Allow,Resource=[""*""],Action=[ecs.CreateCluster,ecs.DeregisterContainerInstance,ecs.DiscoverPollEndpoint,ecs.Poll,ecs.ECSAction(""Submit*"")])])conn.put_role_policy(""ecsServiceRole"",""AmazonEC2ContainerServiceRole"",policy.to_json())","policy=Policy(Statement=[Statement(Effect=Allow,Resource=[""*""],Action=[ecs.CreateCluster,ecs.DeregisterContainerInstance,ecs.DiscoverPollEndpoint,ecs.Poll,ecs.Action(""Submit*"")])])client.put_role_policy(RoleName=role_name,PolicyName=""AmazonEC2ContainerServiceRolePolicy"",PolicyDocument=policy.to_json())",stacker/hooks/iam.py
cloudtools/stacker,3604de9e4e67ebd572941ce9ccc3b2e25cb28908,boto3,boto,"def ensure_server_cert_exists(region, namespace, mappings, parameters,
                              **kwargs):
    conn = connect_to_region(region)
    cert_name = kwargs[""cert_name""]
    try:
        response = conn.get_server_certificate(cert_name)
        cert_arn = _get_cert_arn_from_response(
            ""get_server_certificate"",
            response,
        )

        logger.info(""certificate exists: %s (%s)"", cert_name, cert_arn)
    except BotoServerError:
        upload = raw_input(
            ""Certificate '%s' wasn't found. Upload it now? (yes/no) "" % (
                cert_name,


            )
        )
        if upload != ""yes"":
            return False

        paths = {
            ""certificate"": kwargs.get(""path_to_certificate""),
            ""private_key"": kwargs.get(""path_to_private_key""),
            ""chain"": kwargs.get(""path_to_chain""),
        }

        for key, value in paths.iteritems():
            if value is not None:
                continue

            path = raw_input(""Path to %s (skip): "" % (key,))
            if path == ""skip"" or not path.strip():
                continue

            full_path = utils.full_path(path)
            if not os.path.exists(full_path):
                logger.error(""%s path '%s' does not exist"", key, full_path)
                return False
            paths[key] = full_path

        parameters = {
            ""cert_name"": cert_name,
        }
        for key, path in paths.iteritems():
            if not path:
                continue

            with open(path) as read_file:
                contents = read_file.read()

            if key == ""certificate"":
                parameters[""cert_body""] = contents
            elif key == ""private_key"":
                parameters[""private_key""] = contents
            elif key == ""chain"":
                parameters[""cert_chain""] = contents

        response = conn.upload_server_cert(**parameters)
        cert_arn = _get_cert_arn_from_response(
            ""upload_server_certificate"",
            response,
        )","def ensure_server_cert_exists(region, namespace, mappings, parameters,
                              **kwargs):
    client = boto3.client(""iam"", region_name=region)
    cert_name = kwargs[""cert_name""]
    try:
        response = client.get_server_certificate(
            ServerCertificateName=cert_name


        )
        cert_arn = _get_cert_arn_from_response(response)
        logger.info(""certificate exists: %s (%s)"", cert_name, cert_arn)
    except ClientError:
        if kwargs.get(""prompt"", True):
            upload = raw_input(
                ""Certificate '%s' wasn't found. Upload it now? (yes/no) "" % (
                    cert_name,
                )
            )
            if upload != ""yes"":




















                return False


        parameters = get_cert_contents(kwargs)
        if not parameters:
            return False
        response = client.upload_server_certificate(**parameters)
        cert_arn = _get_cert_arn_from_response(response)",update,"defensure_server_cert_exists(region,namespace,mappings,parameters,**kwargs):conn=connect_to_region(region)cert_name=kwargs[""cert_name""]try:response=conn.get_server_certificate(cert_name)cert_arn=_get_cert_arn_from_response(""get_server_certificate"",response,)logger.info(""certificateexists:%s(%s)"",cert_name,cert_arn)exceptBotoServerError:upload=raw_input(""Certificate'%s'wasn'tfound.Uploaditnow?(yes/no)""%(cert_name,))ifupload!=""yes"":returnFalsepaths={""certificate"":kwargs.get(""path_to_certificate""),""private_key"":kwargs.get(""path_to_private_key""),""chain"":kwargs.get(""path_to_chain""),}forkey,valueinpaths.iteritems():ifvalueisnotNone:continuepath=raw_input(""Pathto%s(skip):""%(key,))ifpath==""skip""ornotpath.strip():continuefull_path=utils.full_path(path)ifnotos.path.exists(full_path):logger.error(""%spath'%s'doesnotexist"",key,full_path)returnFalsepaths[key]=full_pathparameters={""cert_name"":cert_name,}forkey,pathinpaths.iteritems():ifnotpath:continuewithopen(path)asread_file:contents=read_file.read()ifkey==""certificate"":parameters[""cert_body""]=contentselifkey==""private_key"":parameters[""private_key""]=contentselifkey==""chain"":parameters[""cert_chain""]=contentsresponse=conn.upload_server_cert(**parameters)cert_arn=_get_cert_arn_from_response(""upload_server_certificate"",response,)","defensure_server_cert_exists(region,namespace,mappings,parameters,**kwargs):client=boto3.client(""iam"",region_name=region)cert_name=kwargs[""cert_name""]try:response=client.get_server_certificate(ServerCertificateName=cert_name)cert_arn=_get_cert_arn_from_response(response)logger.info(""certificateexists:%s(%s)"",cert_name,cert_arn)exceptClientError:ifkwargs.get(""prompt"",True):upload=raw_input(""Certificate'%s'wasn'tfound.Uploaditnow?(yes/no)""%(cert_name,))ifupload!=""yes"":returnFalseparameters=get_cert_contents(kwargs)ifnotparameters:returnFalseresponse=client.upload_server_certificate(**parameters)cert_arn=_get_cert_arn_from_response(response)",stacker/hooks/iam.py
cloudtools/stacker,3604de9e4e67ebd572941ce9ccc3b2e25cb28908,boto3,boto,from boto.route53 import connect_to_region,"import boto3
from botocore.exceptions import ClientError",update,fromboto.route53importconnect_to_region,importboto3frombotocore.exceptionsimportClientError,stacker/hooks/iam.py
cloudtools/stacker,3604de9e4e67ebd572941ce9ccc3b2e25cb28908,boto3,boto,"def create_domain(region, namespace, mappings, parameters, **kwargs):
    conn = connect_to_region(region)
    domain = kwargs.get('domain', parameters.get('BaseDomain'))
    if not domain:
        logger.error(""domain argument or BaseDomain parameter not provided."")
        return False
    create_route53_zone(conn, domain)
    return True","def create_domain(region, namespace, mappings, parameters, **kwargs):
    client = boto3.client(""route53"", region_name=region)
    domain = kwargs.get('domain', parameters.get('BaseDomain'))
    if not domain:
        logger.error(""domain argument or BaseDomain parameter not provided."")
        return False
    create_route53_zone(client, domain)
    return True",update,"defcreate_domain(region,namespace,mappings,parameters,**kwargs):conn=connect_to_region(region)domain=kwargs.get('domain',parameters.get('BaseDomain'))ifnotdomain:logger.error(""domainargumentorBaseDomainparameternotprovided."")returnFalsecreate_route53_zone(conn,domain)returnTrue","defcreate_domain(region,namespace,mappings,parameters,**kwargs):client=boto3.client(""route53"",region_name=region)domain=kwargs.get('domain',parameters.get('BaseDomain'))ifnotdomain:logger.error(""domainargumentorBaseDomainparameternotprovided."")returnFalsecreate_route53_zone(client,domain)returnTrue",stacker/hooks/route53.py
cloudtools/stacker,3604de9e4e67ebd572941ce9ccc3b2e25cb28908,boto3,boto,,"import unittest

import boto3
from moto import mock_ecs
from testfixtures import LogCapture

from stacker.hooks.ecs import create_clusters

REGION = ""us-east-1""


class TestECSHooks(unittest.TestCase):
    def test_create_single_cluster(self):
        with mock_ecs():
            cluster = ""test-cluster""
            logger = ""stacker.hooks.ecs""
            client = boto3.client(""ecs"", region_name=REGION)
            response = client.list_clusters()

            self.assertEqual(len(response[""clusterArns""]), 0)
            with LogCapture(logger) as logs:
                self.assertTrue(
                    create_clusters(
                        region=REGION,
                        namespace=""fake"",
                        mappings={},
                        parameters={},
                        clusters=cluster
                    )
                )

                logs.check(
                    (
                        logger,
                        ""DEBUG"",
                        ""Creating ECS cluster: %s"" % cluster
                    )
                )

            response = client.list_clusters()
            self.assertEqual(len(response[""clusterArns""]), 1)

    def test_create_multiple_clusters(self):
        with mock_ecs():
            clusters = (""test-cluster0"", ""test-cluster1"")
            logger = ""stacker.hooks.ecs""
            client = boto3.client(""ecs"", region_name=REGION)
            response = client.list_clusters()

            self.assertEqual(len(response[""clusterArns""]), 0)
            for cluster in clusters:
                with LogCapture(logger) as logs:
                    self.assertTrue(
                        create_clusters(
                            region=REGION,
                            namespace=""fake"",
                            mappings={},
                            parameters={},
                            clusters=cluster
                        )
                    )

                    logs.check(
                        (
                            logger,
                            ""DEBUG"",
                            ""Creating ECS cluster: %s"" % cluster
                        )
                    )

            response = client.list_clusters()
            self.assertEqual(len(response[""clusterArns""]), 2)",insert,,"importunittestimportboto3frommotoimportmock_ecsfromtestfixturesimportLogCapturefromstacker.hooks.ecsimportcreate_clustersREGION=""us-east-1""classTestECSHooks(unittest.TestCase):deftest_create_single_cluster(self):withmock_ecs():cluster=""test-cluster""logger=""stacker.hooks.ecs""client=boto3.client(""ecs"",region_name=REGION)response=client.list_clusters()self.assertEqual(len(response[""clusterArns""]),0)withLogCapture(logger)aslogs:self.assertTrue(create_clusters(region=REGION,namespace=""fake"",mappings={},parameters={},clusters=cluster))logs.check((logger,""DEBUG"",""CreatingECScluster:%s""%cluster))response=client.list_clusters()self.assertEqual(len(response[""clusterArns""]),1)deftest_create_multiple_clusters(self):withmock_ecs():clusters=(""test-cluster0"",""test-cluster1"")logger=""stacker.hooks.ecs""client=boto3.client(""ecs"",region_name=REGION)response=client.list_clusters()self.assertEqual(len(response[""clusterArns""]),0)forclusterinclusters:withLogCapture(logger)aslogs:self.assertTrue(create_clusters(region=REGION,namespace=""fake"",mappings={},parameters={},clusters=cluster))logs.check((logger,""DEBUG"",""CreatingECScluster:%s""%cluster))response=client.list_clusters()self.assertEqual(len(response[""clusterArns""]),2)",stacker/tests/hooks/test_ecs.py
cloudtools/stacker,3604de9e4e67ebd572941ce9ccc3b2e25cb28908,boto3,boto,,"import unittest

import boto3
from botocore.exceptions import ClientError

from moto import mock_iam

from stacker.hooks.iam import (
    create_ecs_service_role,
    _get_cert_arn_from_response,
)


REGION = ""us-east-1""

# No test for stacker.hooks.iam.ensure_server_cert_exists until
# this PR is accepted in moto:
# https://github.com/spulec/moto/pull/679


class TestIAMHooks(unittest.TestCase):
    def test_get_cert_arn_from_response(self):
        arn = ""fake-arn""
        # Creation response
        response = {
            ""ServerCertificateMetadata"": {
                ""Arn"": arn
            }
        }

        self.assertEqual(_get_cert_arn_from_response(response), arn)

        # Existing cert response
        response = {""ServerCertificate"": response}
        self.assertEqual(_get_cert_arn_from_response(response), arn)

    def test_create_service_role(self):
        role_name = ""ecsServiceRole""
        policy_name = ""AmazonEC2ContainerServiceRolePolicy""
        with mock_iam():
            client = boto3.client(""iam"", region_name=REGION)

            with self.assertRaises(ClientError):
                client.get_role(RoleName=role_name)

            self.assertTrue(
                create_ecs_service_role(
                    region=REGION,
                    namespace=""fake"",
                    mappings={},
                    parameters={}
                )
            )

            role = client.get_role(RoleName=role_name)

            self.assertIn(""Role"", role)
            self.assertEqual(role_name, role[""Role""][""RoleName""])
            client.get_role_policy(
                RoleName=role_name,
                PolicyName=policy_name
            )",insert,,"importunittestimportboto3frombotocore.exceptionsimportClientErrorfrommotoimportmock_iamfromstacker.hooks.iamimport(create_ecs_service_role,_get_cert_arn_from_response,)REGION=""us-east-1""#Notestforstacker.hooks.iam.ensure_server_cert_existsuntil#thisPRisacceptedinmoto:#https://github.com/spulec/moto/pull/679classTestIAMHooks(unittest.TestCase):deftest_get_cert_arn_from_response(self):arn=""fake-arn""#Creationresponseresponse={""ServerCertificateMetadata"":{""Arn"":arn}}self.assertEqual(_get_cert_arn_from_response(response),arn)#Existingcertresponseresponse={""ServerCertificate"":response}self.assertEqual(_get_cert_arn_from_response(response),arn)deftest_create_service_role(self):role_name=""ecsServiceRole""policy_name=""AmazonEC2ContainerServiceRolePolicy""withmock_iam():client=boto3.client(""iam"",region_name=REGION)withself.assertRaises(ClientError):client.get_role(RoleName=role_name)self.assertTrue(create_ecs_service_role(region=REGION,namespace=""fake"",mappings={},parameters={}))role=client.get_role(RoleName=role_name)self.assertIn(""Role"",role)self.assertEqual(role_name,role[""Role""][""RoleName""])client.get_role_policy(RoleName=role_name,PolicyName=policy_name)",stacker/tests/hooks/test_iam.py
Seagate/cortx-management-portal,c364f0e27a100c3aa19fb5e4381fb2a6a92b8974,boto3,boto,,"import boto3
from functools import partial",insert,,importboto3fromfunctoolsimportpartial,src/eos/plugins/s3.py
Seagate/cortx-management-portal,c364f0e27a100c3aa19fb5e4381fb2a6a92b8974,boto3,boto,"def _create_boto_connection_object(self, **kwargs):
        return S3Connection(**kwargs, calling_format=OrdinaryCallingFormat())","    def _create_boto_connection_object(self, **kwargs):
        """"""Creates S3 server connection""""""

        is_secure = kwargs.get('is_secure', False)
        proto = 'https' if is_secure else 'http'
        url = f""{proto}://{kwargs.get('host', 'localhost')}:{kwargs.get('port', '80')}""
        s3 = boto3.resource(service_name='s3', endpoint_url=url,
                            aws_access_key_id=kwargs['aws_access_key_id'],
                            aws_secret_access_key=kwargs['aws_secret_access_key'])
        return s3",update,"def_create_boto_connection_object(self,**kwargs):returnS3Connection(**kwargs,calling_format=OrdinaryCallingFormat())","def_create_boto_connection_object(self,**kwargs):""""""CreatesS3serverconnection""""""is_secure=kwargs.get('is_secure',False)proto='https'ifis_secureelse'http'url=f""{proto}://{kwargs.get('host','localhost')}:{kwargs.get('port','80')}""s3=boto3.resource(service_name='s3',endpoint_url=url,aws_access_key_id=kwargs['aws_access_key_id'],aws_secret_access_key=kwargs['aws_secret_access_key'])returns3",src/eos/plugins/s3.py
Bennguyenru/HexaHub,5c484b0d82e7dd7939c1dcf48c83e9ccc1e2fa1f,boto3,boto,"  from boto.s3.connection import S3Connection
    from boto.s3.connection import OrdinaryCallingFormat
    from boto.s3.key import Key",import boto3.session,update,fromboto.s3.connectionimportS3Connectionfromboto.s3.connectionimportOrdinaryCallingFormatfromboto.s3.keyimportKey,importboto3.session,build_tools/s3.py
Bennguyenru/HexaHub,5c484b0d82e7dd7939c1dcf48c83e9ccc1e2fa1f,boto3,boto,"conn = S3Connection(key, secret, host='s3-eu-west-1.amazonaws.com', calling_format=OrdinaryCallingFormat())
    bucket = conn.get_bucket(bucket_name)
    s3buckets[bucket_name] = bucket
    return bucket","session = boto3.session.Session(aws_access_key_id=key, aws_secret_access_key=secret, region_name=""eu-west-1"")
    s3_client = session.resource('s3')    
    bucket = s3_client.Bucket(bucket_name)",update,"conn=S3Connection(key,secret,host='s3-eu-west-1.amazonaws.com',calling_format=OrdinaryCallingFormat())bucket=conn.get_bucket(bucket_name)s3buckets[bucket_name]=bucketreturnbucket","session=boto3.session.Session(aws_access_key_id=key,aws_secret_access_key=secret,region_name=""eu-west-1"")s3_client=session.resource('s3')bucket=s3_client.Bucket(bucket_name)",build_tools/s3.py